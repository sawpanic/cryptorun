diff --git a/.githooks/post-checkout b/.githooks/post-checkout
deleted file mode 100644
index 5324014..0000000
--- a/.githooks/post-checkout
+++ /dev/null
@@ -1,7 +0,0 @@
-#!/usr/bin/env pwsh
-command -v git-lfs >$null 2>&1
-if ($LASTEXITCODE -ne 0) {
-  Write-Host "`nThis repository is configured for Git LFS but 'git-lfs' was not found on your path.`n"
-  exit 2
-}
-git lfs post-checkout @args
\ No newline at end of file
diff --git a/.githooks/post-commit b/.githooks/post-commit
deleted file mode 100644
index 0b119ae..0000000
--- a/.githooks/post-commit
+++ /dev/null
@@ -1,7 +0,0 @@
-#!/usr/bin/env pwsh
-command -v git-lfs >$null 2>&1
-if ($LASTEXITCODE -ne 0) {
-  Write-Host "`nThis repository is configured for Git LFS but 'git-lfs' was not found on your path.`n"
-  exit 2
-}
-git lfs post-commit @args
\ No newline at end of file
diff --git a/.githooks/post-merge b/.githooks/post-merge
deleted file mode 100644
index bf0d1fc..0000000
--- a/.githooks/post-merge
+++ /dev/null
@@ -1,7 +0,0 @@
-#!/usr/bin/env pwsh
-command -v git-lfs >$null 2>&1
-if ($LASTEXITCODE -ne 0) {
-  Write-Host "`nThis repository is configured for Git LFS but 'git-lfs' was not found on your path.`n"
-  exit 2
-}
-git lfs post-merge @args
\ No newline at end of file
diff --git a/.githooks/pre-push b/.githooks/pre-push
deleted file mode 100644
index fa648da..0000000
--- a/.githooks/pre-push
+++ /dev/null
@@ -1,19 +0,0 @@
-#!/usr/bin/env pwsh
-
-# Git LFS pre-push hook
-command -v git-lfs >$null 2>&1
-if ($LASTEXITCODE -ne 0) {
-  Write-Host "`nThis repository is configured for Git LFS but 'git-lfs' was not found on your path.`n"
-  exit 2
-}
-git lfs pre-push @args
-if ($LASTEXITCODE -ne 0) { exit $LASTEXITCODE }
-
-# LocalCI check
-Write-Host 'pre-push: running LocalCI'
-pwsh -NoProfile -File tools/LocalCI.ps1
-if ($LASTEXITCODE -ne 0) {
-  Write-Host 'pre-push blocked: LocalCI failed'
-  exit 1
-}
-exit 0
diff --git a/.githooks/pre-push.ps1 b/.githooks/pre-push.ps1
index 399bec2..fa648da 100644
--- a/.githooks/pre-push.ps1
+++ b/.githooks/pre-push.ps1
@@ -1,51 +1,19 @@
-# Pre-push hook for CryptoRun
-# Enforces progress or test improvements before pushing
+#!/usr/bin/env pwsh
 
-Write-Host "üîç Running pre-push checks..."
-
-# Check PowerShell version
-if ($PSVersionTable.PSVersion.Major -lt 7) {
-    Write-Warning "PowerShell 7+ recommended. Current version: $($PSVersionTable.PSVersion)"
-}
-
-# Run documentation guard first
-Write-Host "üìù Checking documentation requirements..."
-try {
-    & pwsh -File "tools/docs_guard.ps1"
-    if ($LASTEXITCODE -ne 0) {
-        Write-Error "‚ùå Documentation guard failed"
-        exit 1
-    }
-} catch {
-    Write-Error "‚ùå Error running documentation guard: $_"
-    exit 1
+# Git LFS pre-push hook
+command -v git-lfs >$null 2>&1
+if ($LASTEXITCODE -ne 0) {
+  Write-Host "`nThis repository is configured for Git LFS but 'git-lfs' was not found on your path.`n"
+  exit 2
 }
+git lfs pre-push @args
+if ($LASTEXITCODE -ne 0) { exit $LASTEXITCODE }
 
-# Run progress check with failure enforcement
-Write-Host "üìä Checking progress..."
-try {
-    & pwsh -File "tools/progress.ps1" -FailIfNoGain
-    if ($LASTEXITCODE -ne 0) {
-        Write-Error "‚ùå Progress check failed"
-        exit 1
-    }
-} catch {
-    Write-Error "‚ùå Error running progress check: $_"
-    exit 1
+# LocalCI check
+Write-Host 'pre-push: running LocalCI'
+pwsh -NoProfile -File tools/LocalCI.ps1
+if ($LASTEXITCODE -ne 0) {
+  Write-Host 'pre-push blocked: LocalCI failed'
+  exit 1
 }
-
-# Run tests to ensure quality
-Write-Host "üß™ Running tests..."
-try {
-    go test ./... -count=1
-    if ($LASTEXITCODE -ne 0) {
-        Write-Error "‚ùå Tests failed"
-        exit 1
-    }
-} catch {
-    Write-Error "‚ùå Error running tests: $_"
-    exit 1
-}
-
-Write-Host "‚úÖ All pre-push checks passed"
-exit 0
\ No newline at end of file
+exit 0
diff --git a/CHANGELOG.md b/CHANGELOG.md
index 54ee134..48568da 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -4,6 +4,164 @@
 
 Real-time implementation progress tracking with comprehensive change documentation: feature completeness indicators, breaking change analysis, and full traceability across all system components.
 
+## 2025-09-06 - Social Inputs & Brand Signals ‚úÖ
+
+### feat(aux): brand & social ingest (PROMPT_ID=AUX.SOCIAL.INGEST)
+
+**Complete Implementation**: Normalized 0-1 brand/social inputs from free, policy-compliant sources with transparent provenance and quality grading
+
+**Core Components**:
+- ‚úÖ **Data Sources** (`internal/social/sources.go`): Multi-source interfaces with GitHub, CoinGecko, and news API integrations plus deterministic fakes
+- ‚úÖ **Normalizer** (`internal/social/normalize.go`): Z-score normalization with winsorization, rolling statistics, and sentiment handling
+- ‚úÖ **API Engine** (`internal/social/api.go`): FetchSocialInputs orchestrator with source aggregation and quality assessment
+- ‚úÖ **Comprehensive Tests**: Full unit test coverage with synthetic data and performance validation
+
+**Source Reliability Grades**:
+- **Grade A Sources**: GitHub API (keyless, developer activity, 6h TTL, 1 req/min rate limit)
+- **Grade B Sources**: CoinGecko free tier (community data, 12h TTL, 0.1 req/sec rate limit)  
+- **Grade C Sources**: News APIs (sentiment, mentions, 2-6h TTL, variable rate limits)
+- **Quality Assessment**: Automatic A-F grading based on source diversity, freshness, and metric coverage
+
+**Normalization Process**:
+- **Primary Method**: Z-score transformation with 30-day rolling windows and 5th-95th percentile winsorization
+- **Sentiment Handling**: Linear transform from [-1,+1] to [0,1] with 0.5 as neutral baseline
+- **Fallback Logic**: Raw clamping to 0-1 when insufficient statistics available
+- **Data Quality**: Staleness penalties, missing data detection, and transparency warnings
+
+**Output Structure**: Complete SocialInputs with normalized components (developer_activity, community_growth, brand_mentions, social_sentiment, overall_social)
+- All values guaranteed in 0-1 range with component-level attribution
+- Source provenance tracking with fetch timing and cache status
+- Quality grading (A-F) with source diversity and freshness assessment
+- Performance metrics and warnings for data quality issues
+
+**Policy Compliance**:
+- ‚úÖ **Robots.txt Respect**: All sources configured to honor robots.txt restrictions
+- ‚úÖ **Rate Limiting**: Source-specific limits with exponential backoff (GitHub: 1/min, CoinGecko: 0.1/sec)
+- ‚úÖ **Keyless APIs**: Prioritizes free/keyless sources (GitHub public, CoinGecko free tier)
+- ‚úÖ **TTL Management**: Configurable per-source cache TTLs (6h-24h) with stale data fallback
+
+**Integration Points**:
+- **Scoring System**: Designed for +10 point social bonus cap in downstream systems
+- **Cache Layer**: Redis-compatible caching with configurable TTLs and stale-on-error fallback
+- **Performance**: <500ms P95 latency target, <30s batch processing, <50MB memory usage
+
+**Documentation**: [docs/SOCIAL_INPUTS.md](./docs/SOCIAL_INPUTS.md) - Complete source specification, normalization methodology, and integration examples
+
+**Testing Coverage**: Unit tests for all source interfaces, normalization mathematics, quality assessment, and API orchestration
+
+**Breaking Changes**: None - new isolated social package with no external dependencies
+
+## 2025-09-06 - Read-Only HTTP Service ‚úÖ
+
+### feat(aux): read-only HTTP service (PROMPT_ID=AUX.HTTP.READONLY)
+
+**Complete Analytical API**: Local-only HTTP service exposing operational and analytical views
+
+- ‚úÖ **New Endpoints**: GET /candidates, /explain/{symbol}, /regime with full JSON schemas
+- ‚úÖ **Gate Status**: Real-time pass/fail evaluation for all entry gates (Score‚â•75, VADR‚â•1.8, etc.)
+- ‚úÖ **Explainability**: Step-by-step scoring attribution with Gram-Schmidt transparency
+- ‚úÖ **Regime Info**: Current regime, weights, health indicators, and switching history
+- ‚úÖ **Microstructure**: Exchange-native spread, depth, VADR data with compliance checks
+- ‚úÖ **Performance**: Sub-300ms P95 latency targets with structured request logging
+- ‚úÖ **Local Access**: 127.0.0.1 binding only, zero auth, graceful shutdown
+- ‚úÖ **Error Handling**: Consistent error responses with detailed validation messages
+
+**Documentation**:
+- [HTTP API](./docs/HTTP_API.md) - Complete API reference with cURL examples and integration guides
+
+**Endpoint Coverage**:
+- `/candidates?n=50` - Top composite candidates with gate status and microstructure
+- `/explain/{symbol}` - Comprehensive scoring explanation from artifacts or live data
+- `/regime` - Current regime information, weights, and 4h evaluation schedule
+- Existing: `/health`, `/metrics`, `/decile`, `/risk` maintained
+
+**Tests Added**:
+- Unit tests with golden JSON validation for all new endpoints
+- Method validation, parameter validation, timeout benchmarks
+- Structured response validation with comprehensive field checks
+
+## 2025-09-06 - Explainability & Audit Artifacts ‚úÖ
+
+### feat(core): explainability & audit artifacts (PROMPT_ID=CORE.EXPLAIN.AUDIT)
+
+**Complete Transparency Layer**: First-class explainability system with audit-ready artifacts
+
+- ‚úÖ **Explainability Schema**: Complete JSON/CSV schema with per-asset attribution and top-3 reasons
+- ‚úÖ **Data Collectors**: Unified collectors consuming scoring/gates/microstructure/regime APIs
+- ‚úÖ **Atomic Writer**: Write-then-rename pattern prevents partial artifacts, configurable paths
+- ‚úÖ **Stable Ordering**: Deterministic sorting (symbol+score) for CI/CD diff compatibility
+- ‚úÖ **Full Attribution**: Factor parts, gate results, microstructure metrics, catalyst evidence
+- ‚úÖ **System Health**: Provider status, rate limits, cache stats, circuit breakers
+- ‚úÖ **Point-in-Time**: Input hashing, UTC timestamps, versioned configuration snapshots
+
+**Documentation**:
+- [Explainability](./docs/EXPLAINABILITY.md) - Complete schema reference and API usage
+- [Artifacts](./docs/ARTIFACTS.md) - File management, retention policies, atomic operations
+
+**Schema Coverage**:
+- JSON: Full-fidelity nested structures with complete attribution
+- CSV: 18-column console format (symbol, decision, score, factors, gates, microstructure)
+- Metadata: Timestamps, input hashes, regime influence, data quality TTLs
+
+**Tests Added**:
+- `TestSchemaRoundTrip` - JSON marshaling/unmarshaling validation
+- `TestStableOrdering` - Deterministic sorting with identical inputs
+- `TestCSVRowGeneration` - CSV format validation and column mapping
+- `TestAtomicWriter` - Atomic write operations with temp file cleanup
+- `TestInputHashConsistency` - Hash stability across key orderings
+
+## 2025-09-06 - Entry/Exit Gate Orchestrator ‚úÖ
+
+### feat(core): entry/exit gate orchestrator (PROMPT_ID=CORE.GATES.ORCH)
+
+**Complete Implementation**: Comprehensive entry/exit gate system with hard requirements, timing guards, and exit precedence rules
+
+**Core Components**:
+- ‚úÖ **Entry Gate Evaluator** (`internal/gates/entry.go`): Hard gates (Score‚â•75, VADR‚â•1.8√ó, funding divergence) with regime-aware threshold adjustment
+- ‚úÖ **Guard Metrics** (`internal/gates/metrics.go`): Timing guards (freshness ‚â§2 bars, fatigue protection, proximity ‚â§1.2√óATR, late-fill <30s)
+- ‚úÖ **Exit Logic** (`internal/exits/logic.go`): Seven-tier exit precedence from hard stops to profit targets with comprehensive reason codes
+- ‚úÖ **Gate Orchestrator** (`internal/gates/api.go`): Unified API for complete entry/exit evaluation with deterministic JSON reporting
+
+**Entry Gate System**:
+- **Hard Gates (All Must Pass)**: Composite score ‚â•75 (80 in High Vol), VADR ‚â•1.8√ó (2.0√ó in High Vol), funding divergence present (‚â§0)
+- **Guard Gates (Any Failure Blocks)**: Signal freshness (‚â§2 bars), fatigue protection (24h>12% AND RSI>70 unless pullback/accel), proximity (‚â§1.2√óATR), late-fill prevention (<30s)
+- **Regime Awareness**: Automatic threshold tightening in High Volatility regime (80 score threshold, 2.0√ó VADR minimum)
+- **Comprehensive Reporting**: Detailed failure reasons with specific threshold violations and gate-by-gate evaluation results
+
+**Exit Rule Precedence (Highest to Lowest)**:
+1. **Hard Stop Loss**: Price ‚â§ stop price (absolute loss protection)
+2. **Venue Health Cut**: P99>2000ms OR error>3% OR reject>5% (exchange degradation)
+3. **Time Limit**: Position held ‚â•48 hours (maximum exposure duration)
+4. **Acceleration Reversal**: Momentum acceleration declined ‚â•50% from entry
+5. **Momentum Fade**: Momentum score declined ‚â•30% from entry
+6. **Trailing Stop**: Price retracement from high water mark (profit protection)
+7. **Profit Targets**: 15%/30% profit realization (lowest precedence)
+
+**Guard Logic Implementation**:
+- **Fatigue Guard**: Prevents overextended entries (24h >12% AND RSI4h >70) with pullback/acceleration exceptions
+- **Freshness Guard**: Ensures signal recency (‚â§2 bars old) for execution relevance
+- **Proximity Guard**: Validates price proximity to trigger (‚â§1.2√ó ATR(1h)) preventing chase entries  
+- **Late-Fill Guard**: Blocks stale executions (‚â•30s since trigger bar close) for timing accuracy
+
+**API Features**:
+- **Deterministic Evaluation**: Stable JSON reporting for automated testing and validation
+- **Complete Attribution**: Gate-by-gate results with specific failure reasons and threshold comparisons
+- **Performance Monitoring**: Evaluation timing, gate pass rates, and failure reason distribution
+- **Regime Integration**: Automatic threshold adjustment based on active market regime
+
+**Documentation**: 
+- [docs/GATES.md](./docs/GATES.md) - Complete entry gate specification with reason codes and regime adjustments
+- [docs/EXITS.md](./docs/EXITS.md) - Exit precedence hierarchy with trigger conditions and performance characteristics
+
+**Testing Coverage**:
+- `TestEntryGateEvaluator_AllGatesPass` - Comprehensive gate evaluation with all conditions passing
+- `TestEntryGateEvaluator_TightenedThresholds` - Regime-specific threshold validation (High Vol adjustments)
+- `TestEntryGateEvaluator_FatigueGuard` - Fatigue condition logic with pullback/acceleration exceptions
+- `TestExitEvaluator_ExitPrecedence` - Multi-condition scenarios validating precedence hierarchy
+- Boundary condition testing for exact threshold behavior (‚â• vs > operators)
+
+**Note**: No changes to scoring system - new `internal/gates` and `internal/exits` packages provide orthogonal gate evaluation functionality
+
 ## 2025-09-06 - Pre-Movement v3.3 Intelligence Module ‚úÖ
 
 ### feat(intel): premovement v3.3 module (PROMPT_ID=INTEL.PREMOVE.V33.MINMOD)
diff --git a/cmd/cryptorun/monitor_main.go b/cmd/cryptorun/monitor_main.go
index 7cbeaed..e39042f 100644
--- a/cmd/cryptorun/monitor_main.go
+++ b/cmd/cryptorun/monitor_main.go
@@ -55,6 +55,16 @@ func runMonitor(cmd *cobra.Command, args []string) error {
 		endpoints.RiskEnvelopeHandler(riskMonitor)(w, r)
 	})
 
+	// New read-only analytical endpoints
+	// Candidates endpoint - top composite candidates with gate status
+	mux.HandleFunc("/candidates", endpoints.CandidatesHandler(metricsCollector))
+
+	// Explain endpoint - explainability for specific symbols
+	mux.HandleFunc("/explain/", endpoints.ExplainHandler())
+
+	// Regime endpoint - current regime information and weights
+	mux.HandleFunc("/regime", endpoints.RegimeHandler(metricsCollector))
+
 	server := &http.Server{
 		Addr:         addr,
 		Handler:      mux,
@@ -76,6 +86,9 @@ func runMonitor(cmd *cobra.Command, args []string) error {
 			Str("health", fmt.Sprintf("http://%s/health", addr)).
 			Str("metrics", fmt.Sprintf("http://%s/metrics", addr)).
 			Str("decile", fmt.Sprintf("http://%s/decile", addr)).
+			Str("candidates", fmt.Sprintf("http://%s/candidates", addr)).
+			Str("explain", fmt.Sprintf("http://%s/explain/{symbol}", addr)).
+			Str("regime", fmt.Sprintf("http://%s/regime", addr)).
 			Msg("Monitor endpoints available")
 
 		if err := server.ListenAndServe(); err != nil && err != http.ErrServerClosed {
diff --git a/docs/ARTIFACTS.md b/docs/ARTIFACTS.md
index 2adf7f6..130633e 100644
--- a/docs/ARTIFACTS.md
+++ b/docs/ARTIFACTS.md
@@ -1,119 +1,213 @@
-# Artifacts Documentation
+# CryptoRun Artifacts Management
 
 ## UX MUST ‚Äî Live Progress & Explainability
 
-This document defines the artifact writing system for CryptoRun's data ledger.
+The CryptoRun artifacts system provides **audit-ready file management** with atomic writes, configurable retention policies, and structured storage for all explainability outputs and operational logs.
 
 ## Overview
 
-The artifact writer provides point-in-time (PIT) file generation with stable schemas for JSON and CSV formats. All artifacts are written to `artifacts/ledger/` with UTC timestamps for chronological ordering.
+CryptoRun generates **point-in-time artifacts** that capture complete system state for regulatory compliance, debugging, and performance analysis. The system supports multiple artifact types with automatic retention management and atomic write operations.
 
-## API Reference
+## Artifact Types
 
-### WriteJSON(name string, v interface{}) error
-Writes any JSON-serializable value to a timestamped file.
+### Explainability Artifacts
+Complete decision transparency with factor attribution:
+- **JSON**: Full-fidelity explainability reports with nested structures
+- **CSV**: Console-optimized 18-column format for quick analysis
+- **Generated by**: `internal/explain` package via scoring pipeline
+- **Retention**: 30 days default, configurable per environment
 
-**Filename Format:** `YYYYMMDD-HHMMSS-{name}.json`
+### Ledger Artifacts (Legacy)
+Point-in-time portfolio and execution logs:
+- **JSON**: Portfolio snapshots, execution records
+- **CSV**: Transaction logs, performance summaries  
+- **Generated by**: `src/internal/artifacts` package via portfolio pipeline
+- **Retention**: 90 days default (regulatory requirement)
 
-**Example:**
-```go
-data := map[string]interface{}{
-    "pair": "BTC-USD",
-    "score": 82.5,
-    "timestamp": time.Now(),
-}
-err := artifacts.WriteJSON("scan-results", data)
-// Creates: 20240906-143022-scan-results.json
+### System Health Artifacts (Future)
+Operational monitoring and performance tracking:
+- **Metrics**: Provider latency, cache hit rates, error rates
+- **Traces**: Request flows, dependency calls, circuit breaker events
+- **Generated by**: Health monitoring system (planned)
+- **Retention**: 14 days default
+
+## Directory Structure
+
+### Standard Layout
+```
+artifacts/
+‚îú‚îÄ‚îÄ explain/                    # Explainability reports
+‚îÇ   ‚îú‚îÄ‚îÄ 20240115-103000-momentum_scan-explain.json
+‚îÇ   ‚îú‚îÄ‚îÄ 20240115-103000-momentum_scan-explain.csv  
+‚îÇ   ‚îú‚îÄ‚îÄ 20240115-154500-regime_change-explain.json
+‚îÇ   ‚îî‚îÄ‚îÄ 20240115-154500-regime_change-explain.csv
+‚îú‚îÄ‚îÄ ledger/                     # Legacy portfolio/execution logs
+‚îÇ   ‚îú‚îÄ‚îÄ 20240115-080000-portfolio.json
+‚îÇ   ‚îú‚îÄ‚îÄ 20240115-080000-execution.csv
+‚îÇ   ‚îî‚îÄ‚îÄ archived/
+‚îÇ       ‚îî‚îÄ‚îÄ 2024-01/
+‚îú‚îÄ‚îÄ health/                     # System monitoring (future)
+‚îÇ   ‚îú‚îÄ‚îÄ 20240115-hourly-metrics.json
+‚îÇ   ‚îî‚îÄ‚îÄ 20240115-traces.jsonl
+‚îî‚îÄ‚îÄ config/                     # Configuration snapshots
+    ‚îî‚îÄ‚îÄ 20240115-active-config.json
 ```
 
-### WriteCSV(name string, rows [][]string) error
-Writes CSV data with header and data rows to a timestamped file.
+### Environment-Specific Paths
+```yaml
+# Development
+artifacts_base: "./artifacts"
 
-**Filename Format:** `YYYYMMDD-HHMMSS-{name}.csv`
+# Staging  
+artifacts_base: "/opt/cryptorun/artifacts"
 
-**Example:**
-```go
-rows := [][]string{
-    {"pair", "score", "volume"},
-    {"BTC-USD", "82.5", "1234567"},
-    {"ETH-USD", "76.2", "987654"},
-}
-err := artifacts.WriteCSV("top-pairs", rows)
-// Creates: 20240906-143022-top-pairs.csv
+# Production
+artifacts_base: "/data/cryptorun/artifacts"
 ```
 
-## Directory Structure
+## File Naming Conventions
 
+### Timestamp Format
 ```
-artifacts/
-‚îî‚îÄ‚îÄ ledger/
-    ‚îú‚îÄ‚îÄ .gitkeep
-    ‚îú‚îÄ‚îÄ 20240906-143022-scan-results.json
-    ‚îú‚îÄ‚îÄ 20240906-143022-top-pairs.csv
-    ‚îî‚îÄ‚îÄ ...
+Format: YYYYMMDD-HHMMSS-{prefix}-{type}.{ext}
+Components:
+  - YYYYMMDD: Date (UTC)
+  - HHMMSS: Time (UTC) 
+  - {prefix}: Operation identifier
+  - {type}: Artifact type (explain, ledger, health)
+  - {ext}: File extension (json, csv, jsonl)
+
+Examples:
+  20240115-103000-momentum_scan-explain.json
+  20240115-080000-portfolio-ledger.json
+  20240115-154500-regime_change-explain.csv
+```
+
+## Atomic Write Operations
+
+### Write-Then-Rename Pattern
+All artifact writes use atomic operations to prevent partial files:
+```go
+func (w *AtomicWriter) writeJSONAtomic(filename string, v interface{}) error {
+    finalPath := filepath.Join(w.BaseDir, filename)
+    tempPath := finalPath + ".tmp"
+    
+    // 1. Write to temporary file
+    data, err := json.MarshalIndent(v, "", "  ")
+    if err != nil {
+        return err
+    }
+    
+    if err := os.WriteFile(tempPath, data, 0644); err != nil {
+        return err
+    }
+    
+    // 2. Atomic rename (guaranteed by filesystem)
+    if err := os.Rename(tempPath, finalPath); err != nil {
+        os.Remove(tempPath)  // Cleanup on failure
+        return err
+    }
+    
+    return nil
+}
 ```
 
-## Naming Convention
+## API Reference
 
-- **Timestamp:** UTC format `YYYYMMDD-HHMMSS` for lexicographic sorting
-- **Component:** Descriptive name (e.g., `scan-results`, `regime-weights`, `gate-violations`)
-- **Extension:** `.json` or `.csv` based on format
+### AtomicWriter
+Primary interface for explainability artifacts:
+```go
+writer := artifacts.NewAtomicWriter("./artifacts/explain")
+err := writer.WriteExplainReport(report, "momentum_scan")
+```
 
-## Schema Discipline
+### Legacy Writers (Maintained)
+Original artifact functions for portfolio/execution logs:
+```go
+// JSON artifacts
+err := artifacts.WriteJSON("portfolio", portfolioData)
 
-### JSON Artifacts
-- Use consistent field names across components
-- Include metadata: `timestamp`, `version`, `component`
-- Prefer flat structures when possible
-- Use ISO 8601 for datetime fields
+// CSV artifacts  
+rows := [][]string{{"symbol", "score"}, {"BTC-USD", "85.5"}}
+err := artifacts.WriteCSV("results", rows)
+```
 
-**Standard Fields:**
-```json
-{
-  "timestamp": "2024-09-06T14:30:22Z",
-  "component": "scanner",
-  "version": "v3.2.1",
-  "data": { ... }
+## Integration Examples
+
+### Explainability Pipeline
+```go
+import (
+    "cryptorun/internal/explain"
+    "cryptorun/src/internal/artifacts"
+)
+
+// Generate complete explainability report
+collector := explain.NewDataCollector("v3.2.1", "./artifacts/explain")
+report, err := collector.GenerateReport(ctx, symbols, inputs)
+if err != nil {
+    return err
 }
+
+// Write atomic artifacts (JSON + CSV)
+writer := artifacts.NewAtomicWriter("./artifacts/explain")
+return writer.WriteExplainReport(report, "momentum_scan")
 ```
 
-### CSV Artifacts
-- First row must be header
-- Use consistent column names across files
-- Prefer numeric formats for calculations
-- Include timestamp column for time series
+### Legacy Portfolio Logging
+```go
+// Portfolio snapshots (maintained for compatibility)
+portfolioState := map[string]interface{}{
+    "timestamp": time.Now().UTC(),
+    "positions": positions,
+    "cash":      cash,
+    "pnl":       pnl,
+}
 
-**Standard Columns:**
-```csv
-timestamp,pair,component,value,unit
-2024-09-06T14:30:22Z,BTC-USD,momentum,82.5,score
+err := artifacts.WriteJSON("portfolio", portfolioState)
 ```
 
-## Retention Policy
+## Configuration
 
-- **Hot:** Last 7 days in `artifacts/ledger/`
-- **Archive:** Older files can be moved to cold storage
-- **Cleanup:** Manual cleanup recommended (no auto-deletion)
+### Environment Variables
+```bash
+# Artifact base directory
+export CRYPTORUN_ARTIFACTS_DIR="./artifacts"
 
-## Integration Points
+# Retention policies
+export CRYPTORUN_EXPLAIN_RETENTION_DAYS="30"
+export CRYPTORUN_LEDGER_RETENTION_DAYS="90"
+```
 
-- **Scanner:** Write scan results and candidate lists
-- **Regime:** Write regime detection and weight changes  
-- **Gates:** Write entry/exit decisions and violations
-- **Backtest:** Write performance metrics and trade logs
+### Config File Support
+```yaml
+# config/artifacts.yaml
+base_dir: "./artifacts"
+retention:
+  explain: 30
+  ledger: 90
+  health: 14
+formats:
+  json_indent: 2
+  csv_precision: 6
+```
 
-## Error Handling
+## Performance & Limits
 
-All functions return errors for:
-- Directory creation failures
-- File write permissions
-- JSON marshaling errors
-- CSV formatting issues
+### File Sizes
+- **Explainability JSON** (100 assets): ~500KB
+- **Explainability CSV** (100 assets): ~25KB  
+- **Portfolio JSON**: ~100KB
+- **Execution CSV** (1000 trades): ~200KB
 
-Handle errors appropriately in calling code:
-```go
-if err := artifacts.WriteJSON("results", data); err != nil {
-    log.Printf("Failed to write artifact: %v", err)
-}
-```
+### Write Performance
+- **Small files** (<1MB): 10-50ms
+- **Large files** (1-10MB): 50-200ms
+- **Concurrent writers**: Up to 10 goroutines safely
+
+### Directory Limits
+- **Files per directory**: <10,000 (filesystem performance)
+- **Total directory size**: <10GB (backup performance)
+- **Retention period**: Configurable per artifact type
+
+This artifacts system ensures reliable, auditable, and performant storage of all CryptoRun operational data.
 
diff --git a/go.mod b/go.mod
index d44da18..1973b11 100644
--- a/go.mod
+++ b/go.mod
@@ -3,7 +3,9 @@ module cryptorun
 go 1.23.0
 
 require (
+	github.com/go-redis/redis/v8 v8.11.5
 	github.com/google/uuid v1.6.0
+	github.com/gorilla/mux v1.8.1
 	github.com/gorilla/websocket v1.5.3
 	github.com/prometheus/client_golang v1.23.2
 	github.com/prometheus/client_model v0.6.2
@@ -24,7 +26,6 @@ require (
 	github.com/cespare/xxhash/v2 v2.3.0 // indirect
 	github.com/davecgh/go-spew v1.1.1 // indirect
 	github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f // indirect
-	github.com/go-redis/redis/v8 v8.11.5 // indirect
 	github.com/inconshreveable/mousetrap v1.1.0 // indirect
 	github.com/kr/text v0.2.0 // indirect
 	github.com/mattn/go-colorable v0.1.13 // indirect
diff --git a/go.sum b/go.sum
index ed4c382..f349c17 100644
--- a/go.sum
+++ b/go.sum
@@ -14,6 +14,8 @@ github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c
 github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
 github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f h1:lO4WD4F/rVNCu3HqELle0jiPLLBs70cWOduZpkS1E78=
 github.com/dgryski/go-rendezvous v0.0.0-20200823014737-9f7001d12a5f/go.mod h1:cuUVRXasLTGF7a8hSLbxyZXjz+1KgoB3wDUb6vlszIc=
+github.com/fsnotify/fsnotify v1.4.9 h1:hsms1Qyu0jgnwNXIxa+/V/PDsU6CfLf6CNO8H7IWoS4=
+github.com/fsnotify/fsnotify v1.4.9/go.mod h1:znqG4EE+3YCdAaPaxE2ZRY/06pZUdp0tY4IgpuI1SZQ=
 github.com/go-redis/redis/v8 v8.11.5 h1:AcZZR7igkdvfVmQTPnu9WE37LRrO/YrBH5zWyjDC0oI=
 github.com/go-redis/redis/v8 v8.11.5/go.mod h1:gREzHqY1hg6oD9ngVRbLStwAWKhA0FEgq8Jd4h5lpwo=
 github.com/godbus/dbus/v5 v5.0.4/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=
@@ -21,6 +23,8 @@ github.com/google/go-cmp v0.7.0 h1:wk8382ETsv4JYUZwIsn6YpYiWiBsYLSJiTsyBybVuN8=
 github.com/google/go-cmp v0.7.0/go.mod h1:pXiqmnSA92OHEEa9HXL2W4E7lf9JzCmGVUdgjX3N/iU=
 github.com/google/uuid v1.6.0 h1:NIvaJDMOsjHA8n1jAhLSgzrAzy1Hgr+hNrb57e+94F0=
 github.com/google/uuid v1.6.0/go.mod h1:TIyPZe4MgqvfeYDBFedMoGGpEw/LqOeaOT+nhxU+yHo=
+github.com/gorilla/mux v1.8.1 h1:TuBL49tXwgrFYWhqrNgrUNEY92u81SPhu7sTdzQEiWY=
+github.com/gorilla/mux v1.8.1/go.mod h1:AKf9I4AEqPTmMytcMc0KkNouC66V3BtZ4qD5fmWSiMQ=
 github.com/gorilla/websocket v1.5.3 h1:saDtZ6Pbx/0u+bgYQ3q96pZgCzfhKXGPqt7kZ72aNNg=
 github.com/gorilla/websocket v1.5.3/go.mod h1:YR8l580nyteQvAITg2hZ9XVh4b55+EU/adAjf1fMHhE=
 github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
@@ -40,6 +44,12 @@ github.com/mattn/go-isatty v0.0.19 h1:JITubQf0MOLdlGRuRq+jtsDlekdYPia9ZFsB8h/APP
 github.com/mattn/go-isatty v0.0.19/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
 github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822 h1:C3w9PqII01/Oq1c1nUAm88MOHcQC9l5mIlSMApZMrHA=
 github.com/munnerz/goautoneg v0.0.0-20191010083416-a7dc8b61c822/go.mod h1:+n7T8mK8HuQTcFwEeznm/DIxMOiR9yIdICNftLE1DvQ=
+github.com/nxadm/tail v1.4.8 h1:nPr65rt6Y5JFSKQO7qToXr7pePgD6Gwiw05lkbyAQTE=
+github.com/nxadm/tail v1.4.8/go.mod h1:+ncqLTQzXmGhMZNUePPaPqPvBxHAIsmXswZKocGu+AU=
+github.com/onsi/ginkgo v1.16.5 h1:8xi0RTUf59SOSfEtZMvwTvXYMzG4gV23XVHOZiXNtnE=
+github.com/onsi/ginkgo v1.16.5/go.mod h1:+E8gABHa3K6zRBolWtd+ROzc/U5bkGt0FwiG042wbpU=
+github.com/onsi/gomega v1.18.1 h1:M1GfJqGRrBrrGGsbxzV5dqM2U2ApXefZCQpkukxYRLE=
+github.com/onsi/gomega v1.18.1/go.mod h1:0q+aL8jAiMXy9hbwj2mr5GziHiwhAIQpFmmtT5hitRs=
 github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
 github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
 github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
@@ -75,6 +85,8 @@ go.uber.org/goleak v1.3.0 h1:2K3zAYmnTNqV73imy9J1T3WC+gmCePx2hEGkimedGto=
 go.uber.org/goleak v1.3.0/go.mod h1:CoHD4mav9JJNrW/WLlf7HGZPjdw8EucARQHekz1X6bE=
 go.yaml.in/yaml/v2 v2.4.2 h1:DzmwEr2rDGHl7lsFgAHxmNz/1NlQ7xLIrlN2h5d1eGI=
 go.yaml.in/yaml/v2 v2.4.2/go.mod h1:081UH+NErpNdqlCXm3TtEran0rJZGxAYx9hb/ELlsPU=
+golang.org/x/net v0.43.0 h1:lat02VYK2j4aLzMzecihNvTlJNQUq316m2Mr9rnM6YE=
+golang.org/x/net v0.43.0/go.mod h1:vhO1fvI4dGsIjh73sWfUVjj3N7CA9WkKJNQm2svM6Jg=
 golang.org/x/sys v0.0.0-20220811171246-fbc7d0a398ab/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
 golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
 golang.org/x/sys v0.12.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
@@ -82,6 +94,8 @@ golang.org/x/sys v0.35.0 h1:vz1N37gP5bs89s7He8XuIYXpyY0+QlsKmzipCbUtyxI=
 golang.org/x/sys v0.35.0/go.mod h1:BJP2sWEmIv4KK5OTEluFJCKSidICx8ciO85XgH3Ak8k=
 golang.org/x/term v0.34.0 h1:O/2T7POpk0ZZ7MAzMeWFSg6S5IpWd/RXDlM9hgM3DR4=
 golang.org/x/term v0.34.0/go.mod h1:5jC53AEywhIVebHgPVeg0mj8OD3VO9OzclacVrqpaAw=
+golang.org/x/text v0.28.0 h1:rhazDwis8INMIwQ4tpjLDzUhx6RlXqZNPEM0huQojng=
+golang.org/x/text v0.28.0/go.mod h1:U8nCwOR8jO/marOQ0QbDiOngZVEBB7MAiitBuMjXiNU=
 golang.org/x/time v0.12.0 h1:ScB/8o8olJvc+CQPWrK3fPZNfh7qgwCrY0zJmoEQLSE=
 golang.org/x/time v0.12.0/go.mod h1:CDIdPxbZBQxdj6cxyCIdrNogrJKMJ7pr37NYpMcMDSg=
 google.golang.org/protobuf v1.36.8 h1:xHScyCOEuuwZEc6UtSOvPbAT4zRh0xcNRYekJwfqyMc=
@@ -89,6 +103,8 @@ google.golang.org/protobuf v1.36.8/go.mod h1:fuxRtAxBytpl4zzqUh6/eyUujkJdNiuEkXn
 gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
 gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
 gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
+gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7 h1:uRGJdciOHaEIrze2W8Q3AKkepLTh2hOroT7a+7czfdQ=
+gopkg.in/tomb.v1 v1.0.0-20141024135613-dd632973f1e7/go.mod h1:dt/ZhP58zS4L8KSrWDmTeBkI65Dw0HsyUHuEVlX15mw=
 gopkg.in/yaml.v2 v2.4.0 h1:D8xgwECY7CYvx+Y2n4sBz93Jn9JRvxdiyyo8CTfuKaY=
 gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
 gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
diff --git a/internal/data/derivs/etf.go b/internal/data/derivs/etf.go
index fddc9bf..88630de 100644
--- a/internal/data/derivs/etf.go
+++ b/internal/data/derivs/etf.go
@@ -2,9 +2,7 @@ package derivs
 
 import (
 	"context"
-	"encoding/json"
 	"fmt"
-	"io"
 	"math"
 	"net/http"
 	"strings"
diff --git a/internal/data/derivs/openinterest.go b/internal/data/derivs/openinterest.go
index f129765..2a1425b 100644
--- a/internal/data/derivs/openinterest.go
+++ b/internal/data/derivs/openinterest.go
@@ -9,6 +9,7 @@ import (
 	"net/http"
 	"os"
 	"path/filepath"
+	"strconv"
 	"strings"
 	"time"
 )
@@ -71,6 +72,22 @@ type OIDataPoint struct {
 	PriceChange1h float64   `json:"price_change_1h"`
 }
 
+// safeDelta calculates the difference between two float64 values with NaN safety
+func safeDelta(curr, prev float64) (float64, bool) {
+	if math.IsNaN(curr) || math.IsNaN(prev) {
+		return 0, false
+	}
+	return curr - prev, true
+}
+
+// Change24h calculates 24h change using previous snapshot
+func (s *OpenInterestSnapshot) Change24h(prev *OpenInterestSnapshot) (float64, bool) {
+	if prev == nil {
+		return 0, false
+	}
+	return safeDelta(s.TotalOI, prev.TotalOI)
+}
+
 // GetOpenInterestSnapshot retrieves OI data and computes 1h delta + residual
 func (oip *OpenInterestProvider) GetOpenInterestSnapshot(ctx context.Context, symbol string, priceChange1h float64) (*OpenInterestSnapshot, error) {
 	cacheKey := fmt.Sprintf("oi_%s", symbol)
@@ -606,13 +623,24 @@ func (oip *OpenInterestProvider) fetchBybitOI(ctx context.Context, symbol string
 		return 0, 0, fmt.Errorf("no Bybit OI data available")
 	}
 
-	currentOI := result.Result.List[0].OpenInterest
+	currentOIStr := result.Result.List[0].OpenInterest
+	currentOI, err := strconv.ParseFloat(currentOIStr, 64)
+	if err != nil {
+		return 0, 0, fmt.Errorf("failed to parse Bybit OI value %s: %w", currentOIStr, err)
+	}
 
 	// Calculate OI change if we have historical data
 	oiChange := 0.0
 	if len(result.Result.List) > 1 {
-		previousOI := result.Result.List[1].OpenInterest
-		oiChange = currentOI - previousOI
+		previousOIStr := result.Result.List[1].OpenInterest
+		previousOI, err := strconv.ParseFloat(previousOIStr, 64)
+		if err != nil {
+			return 0, 0, fmt.Errorf("failed to parse Bybit previous OI value %s: %w", previousOIStr, err)
+		}
+		delta, ok := safeDelta(currentOI, previousOI)
+		if ok {
+			oiChange = delta
+		}
 	}
 
 	return currentOI, oiChange, nil
@@ -635,8 +663,9 @@ func (oip *OpenInterestProvider) toBybitSymbol(symbol string) string {
 // Helper methods for OI analysis
 
 // IsOIExpanding checks if OI is growing (bullish structure signal)
-func (ois *OpenInterestSnapshot) IsOIExpanding(threshold float64) bool {
-	return ois.OIChange24h > threshold
+func (ois *OpenInterestSnapshot) IsOIExpanding(threshold float64, prev *OpenInterestSnapshot) bool {
+	change24h, ok := ois.Change24h(prev)
+	return ok && change24h > threshold
 }
 
 // HasPositiveOIResidual checks if OI growth exceeds price-explained component
@@ -645,10 +674,13 @@ func (ois *OpenInterestSnapshot) HasPositiveOIResidual() bool {
 }
 
 // GetOIIntensity returns normalized OI change intensity
-func (ois *OpenInterestSnapshot) GetOIIntensity() float64 {
+func (ois *OpenInterestSnapshot) GetOIIntensity(prev *OpenInterestSnapshot) float64 {
 	// Normalize by total OI to get percentage change
 	if ois.TotalOI > 0 {
-		return ois.OIChange24h / ois.TotalOI
+		change24h, ok := ois.Change24h(prev)
+		if ok {
+			return change24h / ois.TotalOI
+		}
 	}
 	return 0.0
 }
diff --git a/internal/datasources/cache.go b/internal/datasources/cache.go
index aa6e1e6..02b15aa 100644
--- a/internal/datasources/cache.go
+++ b/internal/datasources/cache.go
@@ -1,294 +1,294 @@
-package datasources
-
-import (
-	"encoding/json"
-	"fmt"
-	"sync"
-	"time"
-)
-
-// CacheConfig defines TTL settings for different data categories
-type CacheConfig struct {
-	Category string        `json:"category"`
-	TTL      time.Duration `json:"ttl"`
-}
-
-// CacheEntry represents a cached data entry
-type CacheEntry struct {
-	Data      interface{} `json:"data"`
-	Timestamp time.Time   `json:"timestamp"`
-	TTL       time.Duration
-}
-
-// CacheManager manages in-memory cache with configurable TTLs
-type CacheManager struct {
-	entries map[string]*CacheEntry
-	config  map[string]time.Duration
-	mu      sync.RWMutex
-}
-
-// Default cache configurations based on playbook specifications
-var DefaultCacheConfig = map[string]time.Duration{
-	// Hot data - real-time streams
-	"ws_stream":     0 * time.Second,     // Never cache WebSocket streams
-	"order_book":    5 * time.Second,     // Very short for orderbook
-	"trades":        10 * time.Second,    // Short for trade data
-	
-	// Warm data - REST APIs with moderate freshness needs
-	"price_current": 30 * time.Second,    // Current price data
-	"volume_24h":    60 * time.Second,    // Volume data
-	"market_data":   120 * time.Second,   // General market data
-	"pair_info":     300 * time.Second,   // Trading pair information
-	
-	// Cold data - slower changing information
-	"exchange_info": 1800 * time.Second,  // 30 minutes for exchange info
-	"asset_info":    3600 * time.Second,  // 1 hour for asset information
-	"historical":    7200 * time.Second,  // 2 hours for historical data
-	"metadata":      21600 * time.Second, // 6 hours for metadata
-}
-
-// Provider-specific cache overrides
-var ProviderCacheOverrides = map[string]map[string]time.Duration{
-	"binance": {
-		"exchange_info": 3600 * time.Second, // Binance exchange info changes less frequently
-		"klines":        300 * time.Second,   // Kline data
-	},
-	"coingecko": {
-		"coin_list":     7200 * time.Second, // CoinGecko coin list
-		"market_data":   180 * time.Second,  // Market data from CoinGecko
-	},
-	"kraken": {
-		"asset_pairs":   1800 * time.Second, // Kraken asset pairs
-		"server_time":   60 * time.Second,   // Server time
-	},
-	"dexscreener": {
-		"token_info":    600 * time.Second,  // DEXScreener token info
-		"pool_data":     120 * time.Second,  // Pool data
-	},
-}
-
-// NewCacheManager creates a new cache manager with default configuration
-func NewCacheManager() *CacheManager {
-	cm := &CacheManager{
-		entries: make(map[string]*CacheEntry),
-		config:  make(map[string]time.Duration),
-	}
-	
-	// Load default configuration
-	for category, ttl := range DefaultCacheConfig {
-		cm.config[category] = ttl
-	}
-	
-	return cm
-}
-
-// SetTTL updates the TTL for a specific cache category
-func (cm *CacheManager) SetTTL(category string, ttl time.Duration) {
-	cm.mu.Lock()
-	defer cm.mu.Unlock()
-	cm.config[category] = ttl
-}
-
-// GetTTL returns the TTL for a specific cache category
-func (cm *CacheManager) GetTTL(category string) time.Duration {
-	cm.mu.RLock()
-	defer cm.mu.RUnlock()
-	
-	if ttl, exists := cm.config[category]; exists {
-		return ttl
-	}
-	
-	// Default TTL if category not found
-	return 300 * time.Second
-}
-
-// Set stores data in cache with the specified category's TTL
-func (cm *CacheManager) Set(key, category string, data interface{}) error {
-	cm.mu.Lock()
-	defer cm.mu.Unlock()
-	
-	ttl := cm.getTTLUnsafe(category)
-	
-	// Don't cache if TTL is 0 (like WebSocket streams)
-	if ttl == 0 {
-		return nil
-	}
-	
-	cm.entries[key] = &CacheEntry{
-		Data:      data,
-		Timestamp: time.Now(),
-		TTL:       ttl,
-	}
-	
-	return nil
-}
-
-// Get retrieves data from cache if not expired
-func (cm *CacheManager) Get(key string) (interface{}, bool) {
-	cm.mu.RLock()
-	defer cm.mu.RUnlock()
-	
-	entry, exists := cm.entries[key]
-	if !exists {
-		return nil, false
-	}
-	
-	// Check if entry has expired
-	if time.Since(entry.Timestamp) > entry.TTL {
-		// Entry expired, clean it up
-		go cm.Delete(key)
-		return nil, false
-	}
-	
-	return entry.Data, true
-}
-
-// GetWithTTL retrieves data from cache along with remaining TTL
-func (cm *CacheManager) GetWithTTL(key string) (interface{}, time.Duration, bool) {
-	cm.mu.RLock()
-	defer cm.mu.RUnlock()
-	
-	entry, exists := cm.entries[key]
-	if !exists {
-		return nil, 0, false
-	}
-	
-	elapsed := time.Since(entry.Timestamp)
-	if elapsed > entry.TTL {
-		go cm.Delete(key)
-		return nil, 0, false
-	}
-	
-	remaining := entry.TTL - elapsed
-	return entry.Data, remaining, true
-}
-
-// Delete removes an entry from cache
-func (cm *CacheManager) Delete(key string) {
-	cm.mu.Lock()
-	defer cm.mu.Unlock()
-	delete(cm.entries, key)
-}
-
-// Clear removes all entries from cache
-func (cm *CacheManager) Clear() {
-	cm.mu.Lock()
-	defer cm.mu.Unlock()
-	cm.entries = make(map[string]*CacheEntry)
-}
-
-// CleanExpired removes all expired entries from cache
-func (cm *CacheManager) CleanExpired() int {
-	cm.mu.Lock()
-	defer cm.mu.Unlock()
-	
-	cleaned := 0
-	now := time.Now()
-	
-	for key, entry := range cm.entries {
-		if now.Sub(entry.Timestamp) > entry.TTL {
-			delete(cm.entries, key)
-			cleaned++
-		}
-	}
-	
-	return cleaned
-}
-
-// Stats returns cache statistics
-func (cm *CacheManager) Stats() CacheStats {
-	cm.mu.RLock()
-	defer cm.mu.RUnlock()
-	
-	stats := CacheStats{
-		TotalEntries: len(cm.entries),
-		Categories:   make(map[string]int),
-	}
-	
-	now := time.Now()
-	for _, entry := range cm.entries {
-		if now.Sub(entry.Timestamp) <= entry.TTL {
-			stats.ActiveEntries++
-		} else {
-			stats.ExpiredEntries++
-		}
-	}
-	
-	return stats
-}
-
-// CacheStats represents cache statistics
-type CacheStats struct {
-	TotalEntries   int            `json:"total_entries"`
-	ActiveEntries  int            `json:"active_entries"`
-	ExpiredEntries int            `json:"expired_entries"`
-	Categories     map[string]int `json:"categories"`
-}
-
-// StartCleanupWorker starts a background goroutine to periodically clean expired entries
-func (cm *CacheManager) StartCleanupWorker(interval time.Duration) {
-	ticker := time.NewTicker(interval)
-	go func() {
-		for range ticker.C {
-			cm.CleanExpired()
-		}
-	}()
-}
-
-// BuildKey creates a cache key from provider, endpoint, and parameters
-func (cm *CacheManager) BuildKey(provider, endpoint string, params map[string]string) string {
-	key := fmt.Sprintf("%s:%s", provider, endpoint)
-	
-	if len(params) > 0 {
-		// Sort parameters for consistent keys
-		paramBytes, _ := json.Marshal(params)
-		key += ":" + string(paramBytes)
-	}
-	
-	return key
-}
-
-// GetProviderTTL returns TTL for a specific provider and category combination
-func (cm *CacheManager) GetProviderTTL(provider, category string) time.Duration {
-	cm.mu.RLock()
-	defer cm.mu.RUnlock()
-	
-	// Check for provider-specific override first
-	if providerOverrides, exists := ProviderCacheOverrides[provider]; exists {
-		if ttl, exists := providerOverrides[category]; exists {
-			return ttl
-		}
-	}
-	
-	// Fall back to default category TTL
-	return cm.getTTLUnsafe(category)
-}
-
-// SetProviderData stores data with provider-specific TTL
-func (cm *CacheManager) SetProviderData(provider, endpoint, category string, params map[string]string, data interface{}) error {
-	key := cm.BuildKey(provider, endpoint, params)
-	ttl := cm.GetProviderTTL(provider, category)
-	
-	cm.mu.Lock()
-	defer cm.mu.Unlock()
-	
-	// Don't cache if TTL is 0
-	if ttl == 0 {
-		return nil
-	}
-	
-	cm.entries[key] = &CacheEntry{
-		Data:      data,
-		Timestamp: time.Now(),
-		TTL:       ttl,
-	}
-	
-	return nil
-}
-
-func (cm *CacheManager) getTTLUnsafe(category string) time.Duration {
-	if ttl, exists := cm.config[category]; exists {
-		return ttl
-	}
-	return 300 * time.Second // 5 minute default
-}
\ No newline at end of file
+package datasources
+
+import (
+	"encoding/json"
+	"fmt"
+	"sync"
+	"time"
+)
+
+// CacheConfig defines TTL settings for different data categories
+type CacheConfig struct {
+	Category string        `json:"category"`
+	TTL      time.Duration `json:"ttl"`
+}
+
+// CacheEntry represents a cached data entry
+type CacheEntry struct {
+	Data      interface{} `json:"data"`
+	Timestamp time.Time   `json:"timestamp"`
+	TTL       time.Duration
+}
+
+// CacheManager manages in-memory cache with configurable TTLs
+type CacheManager struct {
+	entries map[string]*CacheEntry
+	config  map[string]time.Duration
+	mu      sync.RWMutex
+}
+
+// Default cache configurations based on playbook specifications
+var DefaultCacheConfig = map[string]time.Duration{
+	// Hot data - real-time streams
+	"ws_stream":  0 * time.Second,  // Never cache WebSocket streams
+	"order_book": 5 * time.Second,  // Very short for orderbook
+	"trades":     10 * time.Second, // Short for trade data
+
+	// Warm data - REST APIs with moderate freshness needs
+	"price_current": 30 * time.Second,  // Current price data
+	"volume_24h":    60 * time.Second,  // Volume data
+	"market_data":   120 * time.Second, // General market data
+	"pair_info":     300 * time.Second, // Trading pair information
+
+	// Cold data - slower changing information
+	"exchange_info": 1800 * time.Second,  // 30 minutes for exchange info
+	"asset_info":    3600 * time.Second,  // 1 hour for asset information
+	"historical":    7200 * time.Second,  // 2 hours for historical data
+	"metadata":      21600 * time.Second, // 6 hours for metadata
+}
+
+// Provider-specific cache overrides
+var ProviderCacheOverrides = map[string]map[string]time.Duration{
+	"binance": {
+		"exchange_info": 3600 * time.Second, // Binance exchange info changes less frequently
+		"klines":        300 * time.Second,  // Kline data
+	},
+	"coingecko": {
+		"coin_list":   7200 * time.Second, // CoinGecko coin list
+		"market_data": 180 * time.Second,  // Market data from CoinGecko
+	},
+	"kraken": {
+		"asset_pairs": 1800 * time.Second, // Kraken asset pairs
+		"server_time": 60 * time.Second,   // Server time
+	},
+	"dexscreener": {
+		"token_info": 600 * time.Second, // DEXScreener token info
+		"pool_data":  120 * time.Second, // Pool data
+	},
+}
+
+// NewCacheManager creates a new cache manager with default configuration
+func NewCacheManager() *CacheManager {
+	cm := &CacheManager{
+		entries: make(map[string]*CacheEntry),
+		config:  make(map[string]time.Duration),
+	}
+
+	// Load default configuration
+	for category, ttl := range DefaultCacheConfig {
+		cm.config[category] = ttl
+	}
+
+	return cm
+}
+
+// SetTTL updates the TTL for a specific cache category
+func (cm *CacheManager) SetTTL(category string, ttl time.Duration) {
+	cm.mu.Lock()
+	defer cm.mu.Unlock()
+	cm.config[category] = ttl
+}
+
+// GetTTL returns the TTL for a specific cache category
+func (cm *CacheManager) GetTTL(category string) time.Duration {
+	cm.mu.RLock()
+	defer cm.mu.RUnlock()
+
+	if ttl, exists := cm.config[category]; exists {
+		return ttl
+	}
+
+	// Default TTL if category not found
+	return 300 * time.Second
+}
+
+// Set stores data in cache with the specified category's TTL
+func (cm *CacheManager) Set(key, category string, data interface{}) error {
+	cm.mu.Lock()
+	defer cm.mu.Unlock()
+
+	ttl := cm.getTTLUnsafe(category)
+
+	// Don't cache if TTL is 0 (like WebSocket streams)
+	if ttl == 0 {
+		return nil
+	}
+
+	cm.entries[key] = &CacheEntry{
+		Data:      data,
+		Timestamp: time.Now(),
+		TTL:       ttl,
+	}
+
+	return nil
+}
+
+// Get retrieves data from cache if not expired
+func (cm *CacheManager) Get(key string) (interface{}, bool) {
+	cm.mu.RLock()
+	defer cm.mu.RUnlock()
+
+	entry, exists := cm.entries[key]
+	if !exists {
+		return nil, false
+	}
+
+	// Check if entry has expired
+	if time.Since(entry.Timestamp) > entry.TTL {
+		// Entry expired, clean it up
+		go cm.Delete(key)
+		return nil, false
+	}
+
+	return entry.Data, true
+}
+
+// GetWithTTL retrieves data from cache along with remaining TTL
+func (cm *CacheManager) GetWithTTL(key string) (interface{}, time.Duration, bool) {
+	cm.mu.RLock()
+	defer cm.mu.RUnlock()
+
+	entry, exists := cm.entries[key]
+	if !exists {
+		return nil, 0, false
+	}
+
+	elapsed := time.Since(entry.Timestamp)
+	if elapsed > entry.TTL {
+		go cm.Delete(key)
+		return nil, 0, false
+	}
+
+	remaining := entry.TTL - elapsed
+	return entry.Data, remaining, true
+}
+
+// Delete removes an entry from cache
+func (cm *CacheManager) Delete(key string) {
+	cm.mu.Lock()
+	defer cm.mu.Unlock()
+	delete(cm.entries, key)
+}
+
+// Clear removes all entries from cache
+func (cm *CacheManager) Clear() {
+	cm.mu.Lock()
+	defer cm.mu.Unlock()
+	cm.entries = make(map[string]*CacheEntry)
+}
+
+// CleanExpired removes all expired entries from cache
+func (cm *CacheManager) CleanExpired() int {
+	cm.mu.Lock()
+	defer cm.mu.Unlock()
+
+	cleaned := 0
+	now := time.Now()
+
+	for key, entry := range cm.entries {
+		if now.Sub(entry.Timestamp) > entry.TTL {
+			delete(cm.entries, key)
+			cleaned++
+		}
+	}
+
+	return cleaned
+}
+
+// Stats returns cache statistics
+func (cm *CacheManager) Stats() CacheStats {
+	cm.mu.RLock()
+	defer cm.mu.RUnlock()
+
+	stats := CacheStats{
+		TotalEntries: len(cm.entries),
+		Categories:   make(map[string]int),
+	}
+
+	now := time.Now()
+	for _, entry := range cm.entries {
+		if now.Sub(entry.Timestamp) <= entry.TTL {
+			stats.ActiveEntries++
+		} else {
+			stats.ExpiredEntries++
+		}
+	}
+
+	return stats
+}
+
+// CacheStats represents cache statistics
+type CacheStats struct {
+	TotalEntries   int            `json:"total_entries"`
+	ActiveEntries  int            `json:"active_entries"`
+	ExpiredEntries int            `json:"expired_entries"`
+	Categories     map[string]int `json:"categories"`
+}
+
+// StartCleanupWorker starts a background goroutine to periodically clean expired entries
+func (cm *CacheManager) StartCleanupWorker(interval time.Duration) {
+	ticker := time.NewTicker(interval)
+	go func() {
+		for range ticker.C {
+			cm.CleanExpired()
+		}
+	}()
+}
+
+// BuildKey creates a cache key from provider, endpoint, and parameters
+func (cm *CacheManager) BuildKey(provider, endpoint string, params map[string]string) string {
+	key := fmt.Sprintf("%s:%s", provider, endpoint)
+
+	if len(params) > 0 {
+		// Sort parameters for consistent keys
+		paramBytes, _ := json.Marshal(params)
+		key += ":" + string(paramBytes)
+	}
+
+	return key
+}
+
+// GetProviderTTL returns TTL for a specific provider and category combination
+func (cm *CacheManager) GetProviderTTL(provider, category string) time.Duration {
+	cm.mu.RLock()
+	defer cm.mu.RUnlock()
+
+	// Check for provider-specific override first
+	if providerOverrides, exists := ProviderCacheOverrides[provider]; exists {
+		if ttl, exists := providerOverrides[category]; exists {
+			return ttl
+		}
+	}
+
+	// Fall back to default category TTL
+	return cm.getTTLUnsafe(category)
+}
+
+// SetProviderData stores data with provider-specific TTL
+func (cm *CacheManager) SetProviderData(provider, endpoint, category string, params map[string]string, data interface{}) error {
+	key := cm.BuildKey(provider, endpoint, params)
+	ttl := cm.GetProviderTTL(provider, category)
+
+	cm.mu.Lock()
+	defer cm.mu.Unlock()
+
+	// Don't cache if TTL is 0
+	if ttl == 0 {
+		return nil
+	}
+
+	cm.entries[key] = &CacheEntry{
+		Data:      data,
+		Timestamp: time.Now(),
+		TTL:       ttl,
+	}
+
+	return nil
+}
+
+func (cm *CacheManager) getTTLUnsafe(category string) time.Duration {
+	if ttl, exists := cm.config[category]; exists {
+		return ttl
+	}
+	return 300 * time.Second // 5 minute default
+}
diff --git a/internal/datasources/cache_test.go b/internal/datasources/cache_test.go
index d1349e4..67e4092 100644
--- a/internal/datasources/cache_test.go
+++ b/internal/datasources/cache_test.go
@@ -1,301 +1,301 @@
-package datasources
-
-import (
-	"fmt"
-	"testing"
-	"time"
-)
-
-func TestCacheManager_SetGet(t *testing.T) {
-	cm := NewCacheManager()
-	
-	// Set and get data
-	data := "test data"
-	err := cm.Set("test-key", "market_data", data)
-	if err != nil {
-		t.Fatalf("Failed to set cache data: %v", err)
-	}
-	
-	retrieved, found := cm.Get("test-key")
-	if !found {
-		t.Error("Expected to find cached data")
-	}
-	
-	if retrieved != data {
-		t.Errorf("Expected %v, got %v", data, retrieved)
-	}
-}
-
-func TestCacheManager_TTLExpiration(t *testing.T) {
-	cm := NewCacheManager()
-	cm.SetTTL("short_ttl", 100*time.Millisecond)
-	
-	// Set data with short TTL
-	data := "expiring data"
-	err := cm.Set("expire-key", "short_ttl", data)
-	if err != nil {
-		t.Fatalf("Failed to set cache data: %v", err)
-	}
-	
-	// Should be available immediately
-	_, found := cm.Get("expire-key")
-	if !found {
-		t.Error("Expected to find cached data immediately")
-	}
-	
-	// Wait for expiration
-	time.Sleep(150 * time.Millisecond)
-	
-	// Should be expired now
-	_, found = cm.Get("expire-key")
-	if found {
-		t.Error("Expected cached data to be expired")
-	}
-}
-
-func TestCacheManager_ZeroTTL(t *testing.T) {
-	cm := NewCacheManager()
-	
-	// ws_stream has 0 TTL (should not cache)
-	err := cm.Set("stream-key", "ws_stream", "stream data")
-	if err != nil {
-		t.Fatalf("Failed to set cache data: %v", err)
-	}
-	
-	// Should not be cached due to 0 TTL
-	_, found := cm.Get("stream-key")
-	if found {
-		t.Error("Expected ws_stream data not to be cached")
-	}
-}
-
-func TestCacheManager_GetWithTTL(t *testing.T) {
-	cm := NewCacheManager()
-	cm.SetTTL("test_category", 1*time.Second)
-	
-	data := "ttl test data"
-	cm.Set("ttl-key", "test_category", data)
-	
-	retrieved, remaining, found := cm.GetWithTTL("ttl-key")
-	if !found {
-		t.Error("Expected to find cached data")
-	}
-	
-	if retrieved != data {
-		t.Errorf("Expected %v, got %v", data, retrieved)
-	}
-	
-	if remaining <= 0 || remaining > 1*time.Second {
-		t.Errorf("Expected remaining TTL between 0 and 1s, got %v", remaining)
-	}
-}
-
-func TestCacheManager_Delete(t *testing.T) {
-	cm := NewCacheManager()
-	
-	cm.Set("delete-key", "market_data", "data to delete")
-	
-	// Verify it exists
-	_, found := cm.Get("delete-key")
-	if !found {
-		t.Error("Expected to find cached data before deletion")
-	}
-	
-	// Delete it
-	cm.Delete("delete-key")
-	
-	// Verify it's gone
-	_, found = cm.Get("delete-key")
-	if found {
-		t.Error("Expected cached data to be deleted")
-	}
-}
-
-func TestCacheManager_Clear(t *testing.T) {
-	cm := NewCacheManager()
-	
-	// Add multiple entries
-	cm.Set("key1", "market_data", "data1")
-	cm.Set("key2", "market_data", "data2")
-	cm.Set("key3", "market_data", "data3")
-	
-	// Clear all
-	cm.Clear()
-	
-	// Verify all are gone
-	for _, key := range []string{"key1", "key2", "key3"} {
-		if _, found := cm.Get(key); found {
-			t.Errorf("Expected key %s to be cleared", key)
-		}
-	}
-}
-
-func TestCacheManager_CleanExpired(t *testing.T) {
-	cm := NewCacheManager()
-	cm.SetTTL("short_ttl", 50*time.Millisecond)
-	cm.SetTTL("long_ttl", 1*time.Hour)
-	
-	// Add entries with different TTLs
-	cm.Set("short-key", "short_ttl", "short data")
-	cm.Set("long-key", "long_ttl", "long data")
-	
-	// Wait for short TTL to expire
-	time.Sleep(100 * time.Millisecond)
-	
-	// Clean expired entries
-	cleaned := cm.CleanExpired()
-	if cleaned != 1 {
-		t.Errorf("Expected 1 expired entry, cleaned %d", cleaned)
-	}
-	
-	// Verify short key is gone but long key remains
-	_, found := cm.Get("short-key")
-	if found {
-		t.Error("Expected short-key to be cleaned")
-	}
-	
-	_, found = cm.Get("long-key")
-	if !found {
-		t.Error("Expected long-key to remain")
-	}
-}
-
-func TestCacheManager_Stats(t *testing.T) {
-	cm := NewCacheManager()
-	cm.SetTTL("short_ttl", 50*time.Millisecond)
-	
-	// Add entries
-	cm.Set("key1", "market_data", "data1")
-	cm.Set("key2", "short_ttl", "data2")
-	
-	// Get initial stats
-	stats := cm.Stats()
-	if stats.TotalEntries != 2 {
-		t.Errorf("Expected 2 total entries, got %d", stats.TotalEntries)
-	}
-	if stats.ActiveEntries != 2 {
-		t.Errorf("Expected 2 active entries, got %d", stats.ActiveEntries)
-	}
-	
-	// Wait for one to expire
-	time.Sleep(100 * time.Millisecond)
-	
-	stats = cm.Stats()
-	if stats.TotalEntries != 2 {
-		t.Errorf("Expected 2 total entries, got %d", stats.TotalEntries)
-	}
-	if stats.ExpiredEntries != 1 {
-		t.Errorf("Expected 1 expired entry, got %d", stats.ExpiredEntries)
-	}
-}
-
-func TestCacheManager_BuildKey(t *testing.T) {
-	cm := NewCacheManager()
-	
-	// Test key building without parameters
-	key1 := cm.BuildKey("binance", "ticker", nil)
-	expected1 := "binance:ticker"
-	if key1 != expected1 {
-		t.Errorf("Expected key %s, got %s", expected1, key1)
-	}
-	
-	// Test key building with parameters
-	params := map[string]string{
-		"symbol": "BTCUSD",
-		"limit":  "100",
-	}
-	key2 := cm.BuildKey("binance", "orderbook", params)
-	
-	// Should contain provider and endpoint
-	if len(key2) <= len("binance:orderbook") {
-		t.Error("Expected key with parameters to be longer")
-	}
-	
-	// Same parameters should produce same key
-	key3 := cm.BuildKey("binance", "orderbook", params)
-	if key2 != key3 {
-		t.Error("Expected same parameters to produce same key")
-	}
-}
-
-func TestCacheManager_ProviderSpecificTTL(t *testing.T) {
-	cm := NewCacheManager()
-	
-	// Test provider-specific override
-	binanceTTL := cm.GetProviderTTL("binance", "klines")
-	expectedBinanceTTL := 300 * time.Second // From ProviderCacheOverrides
-	if binanceTTL != expectedBinanceTTL {
-		t.Errorf("Expected Binance klines TTL %v, got %v", expectedBinanceTTL, binanceTTL)
-	}
-	
-	// Test fallback to default
-	defaultTTL := cm.GetProviderTTL("kraken", "market_data")
-	expectedDefaultTTL := 120 * time.Second // From DefaultCacheConfig
-	if defaultTTL != expectedDefaultTTL {
-		t.Errorf("Expected default market_data TTL %v, got %v", expectedDefaultTTL, defaultTTL)
-	}
-}
-
-func TestCacheManager_SetProviderData(t *testing.T) {
-	cm := NewCacheManager()
-	
-	params := map[string]string{"symbol": "BTCUSD"}
-	data := "provider data"
-	
-	err := cm.SetProviderData("binance", "ticker", "price_current", params, data)
-	if err != nil {
-		t.Fatalf("Failed to set provider data: %v", err)
-	}
-	
-	// Build the same key and retrieve
-	key := cm.BuildKey("binance", "ticker", params)
-	retrieved, found := cm.Get(key)
-	if !found {
-		t.Error("Expected to find provider data")
-	}
-	
-	if retrieved != data {
-		t.Errorf("Expected %v, got %v", data, retrieved)
-	}
-}
-
-func TestDefaultCacheConfig(t *testing.T) {
-	// Verify all default configurations are valid
-	for category, ttl := range DefaultCacheConfig {
-		if category == "" {
-			t.Error("Found empty category in default config")
-		}
-		if ttl < 0 {
-			t.Errorf("Category %s has negative TTL: %v", category, ttl)
-		}
-	}
-	
-	// Verify WebSocket streams have 0 TTL
-	if DefaultCacheConfig["ws_stream"] != 0 {
-		t.Error("WebSocket streams should have 0 TTL")
-	}
-}
-
-func TestCacheManager_ConcurrentAccess(t *testing.T) {
-	cm := NewCacheManager()
-	
-	// Test concurrent access doesn't cause panics
-	done := make(chan bool)
-	
-	for i := 0; i < 10; i++ {
-		go func(id int) {
-			for j := 0; j < 100; j++ {
-				key := fmt.Sprintf("key-%d-%d", id, j)
-				cm.Set(key, "market_data", fmt.Sprintf("data-%d-%d", id, j))
-				cm.Get(key)
-				cm.Delete(key)
-			}
-			done <- true
-		}(i)
-	}
-	
-	// Wait for all goroutines to complete
-	for i := 0; i < 10; i++ {
-		<-done
-	}
-}
\ No newline at end of file
+package datasources
+
+import (
+	"fmt"
+	"testing"
+	"time"
+)
+
+func TestCacheManager_SetGet(t *testing.T) {
+	cm := NewCacheManager()
+
+	// Set and get data
+	data := "test data"
+	err := cm.Set("test-key", "market_data", data)
+	if err != nil {
+		t.Fatalf("Failed to set cache data: %v", err)
+	}
+
+	retrieved, found := cm.Get("test-key")
+	if !found {
+		t.Error("Expected to find cached data")
+	}
+
+	if retrieved != data {
+		t.Errorf("Expected %v, got %v", data, retrieved)
+	}
+}
+
+func TestCacheManager_TTLExpiration(t *testing.T) {
+	cm := NewCacheManager()
+	cm.SetTTL("short_ttl", 100*time.Millisecond)
+
+	// Set data with short TTL
+	data := "expiring data"
+	err := cm.Set("expire-key", "short_ttl", data)
+	if err != nil {
+		t.Fatalf("Failed to set cache data: %v", err)
+	}
+
+	// Should be available immediately
+	_, found := cm.Get("expire-key")
+	if !found {
+		t.Error("Expected to find cached data immediately")
+	}
+
+	// Wait for expiration
+	time.Sleep(150 * time.Millisecond)
+
+	// Should be expired now
+	_, found = cm.Get("expire-key")
+	if found {
+		t.Error("Expected cached data to be expired")
+	}
+}
+
+func TestCacheManager_ZeroTTL(t *testing.T) {
+	cm := NewCacheManager()
+
+	// ws_stream has 0 TTL (should not cache)
+	err := cm.Set("stream-key", "ws_stream", "stream data")
+	if err != nil {
+		t.Fatalf("Failed to set cache data: %v", err)
+	}
+
+	// Should not be cached due to 0 TTL
+	_, found := cm.Get("stream-key")
+	if found {
+		t.Error("Expected ws_stream data not to be cached")
+	}
+}
+
+func TestCacheManager_GetWithTTL(t *testing.T) {
+	cm := NewCacheManager()
+	cm.SetTTL("test_category", 1*time.Second)
+
+	data := "ttl test data"
+	cm.Set("ttl-key", "test_category", data)
+
+	retrieved, remaining, found := cm.GetWithTTL("ttl-key")
+	if !found {
+		t.Error("Expected to find cached data")
+	}
+
+	if retrieved != data {
+		t.Errorf("Expected %v, got %v", data, retrieved)
+	}
+
+	if remaining <= 0 || remaining > 1*time.Second {
+		t.Errorf("Expected remaining TTL between 0 and 1s, got %v", remaining)
+	}
+}
+
+func TestCacheManager_Delete(t *testing.T) {
+	cm := NewCacheManager()
+
+	cm.Set("delete-key", "market_data", "data to delete")
+
+	// Verify it exists
+	_, found := cm.Get("delete-key")
+	if !found {
+		t.Error("Expected to find cached data before deletion")
+	}
+
+	// Delete it
+	cm.Delete("delete-key")
+
+	// Verify it's gone
+	_, found = cm.Get("delete-key")
+	if found {
+		t.Error("Expected cached data to be deleted")
+	}
+}
+
+func TestCacheManager_Clear(t *testing.T) {
+	cm := NewCacheManager()
+
+	// Add multiple entries
+	cm.Set("key1", "market_data", "data1")
+	cm.Set("key2", "market_data", "data2")
+	cm.Set("key3", "market_data", "data3")
+
+	// Clear all
+	cm.Clear()
+
+	// Verify all are gone
+	for _, key := range []string{"key1", "key2", "key3"} {
+		if _, found := cm.Get(key); found {
+			t.Errorf("Expected key %s to be cleared", key)
+		}
+	}
+}
+
+func TestCacheManager_CleanExpired(t *testing.T) {
+	cm := NewCacheManager()
+	cm.SetTTL("short_ttl", 50*time.Millisecond)
+	cm.SetTTL("long_ttl", 1*time.Hour)
+
+	// Add entries with different TTLs
+	cm.Set("short-key", "short_ttl", "short data")
+	cm.Set("long-key", "long_ttl", "long data")
+
+	// Wait for short TTL to expire
+	time.Sleep(100 * time.Millisecond)
+
+	// Clean expired entries
+	cleaned := cm.CleanExpired()
+	if cleaned != 1 {
+		t.Errorf("Expected 1 expired entry, cleaned %d", cleaned)
+	}
+
+	// Verify short key is gone but long key remains
+	_, found := cm.Get("short-key")
+	if found {
+		t.Error("Expected short-key to be cleaned")
+	}
+
+	_, found = cm.Get("long-key")
+	if !found {
+		t.Error("Expected long-key to remain")
+	}
+}
+
+func TestCacheManager_Stats(t *testing.T) {
+	cm := NewCacheManager()
+	cm.SetTTL("short_ttl", 50*time.Millisecond)
+
+	// Add entries
+	cm.Set("key1", "market_data", "data1")
+	cm.Set("key2", "short_ttl", "data2")
+
+	// Get initial stats
+	stats := cm.Stats()
+	if stats.TotalEntries != 2 {
+		t.Errorf("Expected 2 total entries, got %d", stats.TotalEntries)
+	}
+	if stats.ActiveEntries != 2 {
+		t.Errorf("Expected 2 active entries, got %d", stats.ActiveEntries)
+	}
+
+	// Wait for one to expire
+	time.Sleep(100 * time.Millisecond)
+
+	stats = cm.Stats()
+	if stats.TotalEntries != 2 {
+		t.Errorf("Expected 2 total entries, got %d", stats.TotalEntries)
+	}
+	if stats.ExpiredEntries != 1 {
+		t.Errorf("Expected 1 expired entry, got %d", stats.ExpiredEntries)
+	}
+}
+
+func TestCacheManager_BuildKey(t *testing.T) {
+	cm := NewCacheManager()
+
+	// Test key building without parameters
+	key1 := cm.BuildKey("binance", "ticker", nil)
+	expected1 := "binance:ticker"
+	if key1 != expected1 {
+		t.Errorf("Expected key %s, got %s", expected1, key1)
+	}
+
+	// Test key building with parameters
+	params := map[string]string{
+		"symbol": "BTCUSD",
+		"limit":  "100",
+	}
+	key2 := cm.BuildKey("binance", "orderbook", params)
+
+	// Should contain provider and endpoint
+	if len(key2) <= len("binance:orderbook") {
+		t.Error("Expected key with parameters to be longer")
+	}
+
+	// Same parameters should produce same key
+	key3 := cm.BuildKey("binance", "orderbook", params)
+	if key2 != key3 {
+		t.Error("Expected same parameters to produce same key")
+	}
+}
+
+func TestCacheManager_ProviderSpecificTTL(t *testing.T) {
+	cm := NewCacheManager()
+
+	// Test provider-specific override
+	binanceTTL := cm.GetProviderTTL("binance", "klines")
+	expectedBinanceTTL := 300 * time.Second // From ProviderCacheOverrides
+	if binanceTTL != expectedBinanceTTL {
+		t.Errorf("Expected Binance klines TTL %v, got %v", expectedBinanceTTL, binanceTTL)
+	}
+
+	// Test fallback to default
+	defaultTTL := cm.GetProviderTTL("kraken", "market_data")
+	expectedDefaultTTL := 120 * time.Second // From DefaultCacheConfig
+	if defaultTTL != expectedDefaultTTL {
+		t.Errorf("Expected default market_data TTL %v, got %v", expectedDefaultTTL, defaultTTL)
+	}
+}
+
+func TestCacheManager_SetProviderData(t *testing.T) {
+	cm := NewCacheManager()
+
+	params := map[string]string{"symbol": "BTCUSD"}
+	data := "provider data"
+
+	err := cm.SetProviderData("binance", "ticker", "price_current", params, data)
+	if err != nil {
+		t.Fatalf("Failed to set provider data: %v", err)
+	}
+
+	// Build the same key and retrieve
+	key := cm.BuildKey("binance", "ticker", params)
+	retrieved, found := cm.Get(key)
+	if !found {
+		t.Error("Expected to find provider data")
+	}
+
+	if retrieved != data {
+		t.Errorf("Expected %v, got %v", data, retrieved)
+	}
+}
+
+func TestDefaultCacheConfig(t *testing.T) {
+	// Verify all default configurations are valid
+	for category, ttl := range DefaultCacheConfig {
+		if category == "" {
+			t.Error("Found empty category in default config")
+		}
+		if ttl < 0 {
+			t.Errorf("Category %s has negative TTL: %v", category, ttl)
+		}
+	}
+
+	// Verify WebSocket streams have 0 TTL
+	if DefaultCacheConfig["ws_stream"] != 0 {
+		t.Error("WebSocket streams should have 0 TTL")
+	}
+}
+
+func TestCacheManager_ConcurrentAccess(t *testing.T) {
+	cm := NewCacheManager()
+
+	// Test concurrent access doesn't cause panics
+	done := make(chan bool)
+
+	for i := 0; i < 10; i++ {
+		go func(id int) {
+			for j := 0; j < 100; j++ {
+				key := fmt.Sprintf("key-%d-%d", id, j)
+				cm.Set(key, "market_data", fmt.Sprintf("data-%d-%d", id, j))
+				cm.Get(key)
+				cm.Delete(key)
+			}
+			done <- true
+		}(i)
+	}
+
+	// Wait for all goroutines to complete
+	for i := 0; i < 10; i++ {
+		<-done
+	}
+}
diff --git a/internal/datasources/circuits.go b/internal/datasources/circuits.go
index 1459e63..e4537ee 100644
--- a/internal/datasources/circuits.go
+++ b/internal/datasources/circuits.go
@@ -1,469 +1,468 @@
-package datasources
-
-import (
-	"fmt"
-	"sync"
-	"time"
-)
-
-// CircuitState represents the state of a circuit breaker
-type CircuitState int
-
-const (
-	CircuitClosed CircuitState = iota
-	CircuitOpen
-	CircuitHalfOpen
-)
-
-func (s CircuitState) String() string {
-	switch s {
-	case CircuitClosed:
-		return "closed"
-	case CircuitOpen:
-		return "open"
-	case CircuitHalfOpen:
-		return "half-open"
-	default:
-		return "unknown"
-	}
-}
-
-// CircuitConfig defines configuration for a circuit breaker
-type CircuitConfig struct {
-	Provider              string        `json:"provider"`
-	ErrorThreshold        int           `json:"error_threshold"`        // Number of errors before opening
-	SuccessThreshold      int           `json:"success_threshold"`      // Successes needed to close from half-open
-	Timeout               time.Duration `json:"timeout"`                // How long to stay open before half-open
-	LatencyThreshold      time.Duration `json:"latency_threshold"`      // Max acceptable latency
-	BudgetThreshold       float64       `json:"budget_threshold"`       // Remaining budget % before opening
-	WindowSize            int           `json:"window_size"`            // Size of sliding window for error rate
-	MinRequestsInWindow   int           `json:"min_requests_in_window"` // Min requests before calculating error rate
-}
-
-// CircuitBreaker implements a circuit breaker pattern for data providers
-type CircuitBreaker struct {
-	config         CircuitConfig
-	state          CircuitState
-	errorCount     int
-	successCount   int
-	requestCount   int
-	lastFailTime   time.Time
-	lastSuccessTime time.Time
-	mu             sync.RWMutex
-	
-	// Sliding window for request tracking
-	requests       []RequestResult
-	windowStart    int
-	
-	// Fallback providers in order of preference
-	fallbackProviders []string
-	currentProvider   string
-}
-
-// RequestResult tracks the result of a request
-type RequestResult struct {
-	Timestamp time.Time
-	Success   bool
-	Latency   time.Duration
-	Error     error
-}
-
-// CircuitManager manages circuit breakers for all providers
-type CircuitManager struct {
-	circuits      map[string]*CircuitBreaker
-	fallbackChains map[string][]string
-	mu            sync.RWMutex
-}
-
-// Default circuit configurations
-var DefaultCircuitConfigs = map[string]CircuitConfig{
-	"binance": {
-		Provider:              "binance",
-		ErrorThreshold:        5,
-		SuccessThreshold:      3,
-		Timeout:               30 * time.Second,
-		LatencyThreshold:      5 * time.Second,
-		BudgetThreshold:       0.1, // Open when <10% budget remaining
-		WindowSize:            20,
-		MinRequestsInWindow:   5,
-	},
-	"coingecko": {
-		Provider:              "coingecko",
-		ErrorThreshold:        3,
-		SuccessThreshold:      2,
-		Timeout:               60 * time.Second,
-		LatencyThreshold:      10 * time.Second,
-		BudgetThreshold:       0.05, // Open when <5% monthly budget remaining
-		WindowSize:            15,
-		MinRequestsInWindow:   3,
-	},
-	"moralis": {
-		Provider:              "moralis",
-		ErrorThreshold:        3,
-		SuccessThreshold:      2,
-		Timeout:               45 * time.Second,
-		LatencyThreshold:      8 * time.Second,
-		BudgetThreshold:       0.1, // Open when <10% CU remaining
-		WindowSize:            15,
-		MinRequestsInWindow:   3,
-	},
-	"dexscreener": {
-		Provider:              "dexscreener",
-		ErrorThreshold:        4,
-		SuccessThreshold:      2,
-		Timeout:               30 * time.Second,
-		LatencyThreshold:      6 * time.Second,
-		BudgetThreshold:       0.0, // No budget limit
-		WindowSize:            20,
-		MinRequestsInWindow:   4,
-	},
-	"kraken": {
-		Provider:              "kraken",
-		ErrorThreshold:        2,
-		SuccessThreshold:      1,
-		Timeout:               60 * time.Second,
-		LatencyThreshold:      15 * time.Second,
-		BudgetThreshold:       0.0, // No budget limit
-		WindowSize:            10,
-		MinRequestsInWindow:   2,
-	},
-}
-
-// Default fallback chains - preferred order when primary fails
-var DefaultFallbackChains = map[string][]string{
-	"binance":     {"kraken", "coingecko"},
-	"coingecko":   {"binance", "kraken"},
-	"moralis":     {"coingecko", "binance"},
-	"dexscreener": {"coingecko", "binance"},
-	"kraken":      {"binance", "coingecko"},
-}
-
-// NewCircuitManager creates a new circuit manager with default configurations
-func NewCircuitManager() *CircuitManager {
-	cm := &CircuitManager{
-		circuits:       make(map[string]*CircuitBreaker),
-		fallbackChains: make(map[string][]string),
-	}
-	
-	// Initialize circuits for all providers
-	for provider, config := range DefaultCircuitConfigs {
-		cm.circuits[provider] = &CircuitBreaker{
-			config:            config,
-			state:             CircuitClosed,
-			requests:          make([]RequestResult, config.WindowSize),
-			currentProvider:   provider,
-		}
-		
-		// Set fallback chains
-		if fallbacks, exists := DefaultFallbackChains[provider]; exists {
-			cm.fallbackChains[provider] = fallbacks
-			cm.circuits[provider].fallbackProviders = fallbacks
-		}
-	}
-	
-	return cm
-}
-
-// CanMakeRequest checks if a request can be made to the provider
-func (cm *CircuitManager) CanMakeRequest(provider string) bool {
-	cm.mu.RLock()
-	circuit, exists := cm.circuits[provider]
-	cm.mu.RUnlock()
-	
-	if !exists {
-		return false
-	}
-	
-	return circuit.canMakeRequest()
-}
-
-// RecordRequest records the result of a request
-func (cm *CircuitManager) RecordRequest(provider string, success bool, latency time.Duration, err error) {
-	cm.mu.RLock()
-	circuit, exists := cm.circuits[provider]
-	cm.mu.RUnlock()
-	
-	if !exists {
-		return
-	}
-	
-	circuit.recordRequest(success, latency, err)
-}
-
-// GetActiveProvider returns the currently active provider (may be a fallback)
-func (cm *CircuitManager) GetActiveProvider(originalProvider string) string {
-	cm.mu.RLock()
-	circuit, exists := cm.circuits[originalProvider]
-	cm.mu.RUnlock()
-	
-	if !exists {
-		return originalProvider
-	}
-	
-	if circuit.canMakeRequest() {
-		return originalProvider
-	}
-	
-	// Check fallback providers
-	for _, fallback := range circuit.fallbackProviders {
-		if fallbackCircuit, exists := cm.circuits[fallback]; exists {
-			if fallbackCircuit.canMakeRequest() {
-				return fallback
-			}
-		}
-	}
-	
-	// Return original provider if no fallbacks available (will fail)
-	return originalProvider
-}
-
-// GetCircuitState returns the current state of a circuit
-func (cm *CircuitManager) GetCircuitState(provider string) CircuitState {
-	cm.mu.RLock()
-	circuit, exists := cm.circuits[provider]
-	cm.mu.RUnlock()
-	
-	if !exists {
-		return CircuitClosed // Default to closed for unknown providers
-	}
-	
-	return circuit.getState()
-}
-
-// ForceOpen forces a circuit to open (for testing or manual intervention)
-func (cm *CircuitManager) ForceOpen(provider string) {
-	cm.mu.RLock()
-	circuit, exists := cm.circuits[provider]
-	cm.mu.RUnlock()
-	
-	if exists {
-		circuit.forceOpen()
-	}
-}
-
-// ForceClose forces a circuit to close (for testing or manual intervention)
-func (cm *CircuitManager) ForceClose(provider string) {
-	cm.mu.RLock()
-	circuit, exists := cm.circuits[provider]
-	cm.mu.RUnlock()
-	
-	if exists {
-		circuit.forceClose()
-	}
-}
-
-// CheckBudgetThreshold checks if circuit should open due to low budget
-func (cm *CircuitManager) CheckBudgetThreshold(provider string, healthPercent float64) {
-	cm.mu.RLock()
-	circuit, exists := cm.circuits[provider]
-	cm.mu.RUnlock()
-	
-	if !exists {
-		return
-	}
-	
-	circuit.checkBudgetThreshold(healthPercent)
-}
-
-func (cb *CircuitBreaker) canMakeRequest() bool {
-	cb.mu.RLock()
-	defer cb.mu.RUnlock()
-	
-	now := time.Now()
-	
-	switch cb.state {
-	case CircuitClosed:
-		return true
-	case CircuitOpen:
-		// Check if timeout has passed
-		if now.Sub(cb.lastFailTime) >= cb.config.Timeout {
-			cb.mu.RUnlock()
-			cb.mu.Lock()
-			if cb.state == CircuitOpen { // Double-check after acquiring write lock
-				cb.state = CircuitHalfOpen
-				cb.successCount = 0
-			}
-			cb.mu.Unlock()
-			cb.mu.RLock()
-			return true
-		}
-		return false
-	case CircuitHalfOpen:
-		return true
-	}
-	
-	return false
-}
-
-func (cb *CircuitBreaker) recordRequest(success bool, latency time.Duration, err error) {
-	cb.mu.Lock()
-	defer cb.mu.Unlock()
-	
-	now := time.Now()
-	
-	// Check latency threshold first
-	if success && latency > cb.config.LatencyThreshold {
-		success = false // Treat slow requests as failures
-		err = fmt.Errorf("request too slow: %v > %v", latency, cb.config.LatencyThreshold)
-	}
-	
-	// Add request to sliding window with corrected success value
-	cb.requests[cb.requestCount%cb.config.WindowSize] = RequestResult{
-		Timestamp: now,
-		Success:   success,
-		Latency:   latency,
-		Error:     err,
-	}
-	cb.requestCount++
-	
-	if success {
-		cb.successCount++
-		cb.lastSuccessTime = now
-		
-		// If half-open and enough successes, close the circuit
-		if cb.state == CircuitHalfOpen && cb.successCount >= cb.config.SuccessThreshold {
-			cb.state = CircuitClosed
-			cb.errorCount = 0
-		}
-	} else {
-		cb.errorCount++
-		cb.lastFailTime = now
-		cb.successCount = 0
-		
-		// Check if we should open the circuit
-		if cb.shouldOpen() {
-			cb.state = CircuitOpen
-		}
-	}
-}
-
-func (cb *CircuitBreaker) shouldOpen() bool {
-	// Only consider opening if we have enough requests in the window
-	if cb.requestCount < cb.config.MinRequestsInWindow {
-		return false
-	}
-	
-	// Calculate error rate over sliding window
-	windowSize := minInt(cb.requestCount, cb.config.WindowSize)
-	errorCount := 0
-	
-	for i := 0; i < windowSize; i++ {
-		if !cb.requests[i].Success {
-			errorCount++
-		}
-	}
-	
-	errorRate := float64(errorCount) / float64(windowSize)
-	threshold := float64(cb.config.ErrorThreshold) / float64(cb.config.WindowSize)
-	
-	return errorRate >= threshold
-}
-
-func (cb *CircuitBreaker) checkBudgetThreshold(healthPercent float64) {
-	cb.mu.Lock()
-	defer cb.mu.Unlock()
-	
-	if cb.config.BudgetThreshold > 0 && healthPercent < cb.config.BudgetThreshold*100 {
-		if cb.state == CircuitClosed {
-			cb.state = CircuitOpen
-			cb.lastFailTime = time.Now()
-		}
-	}
-}
-
-func (cb *CircuitBreaker) getState() CircuitState {
-	cb.mu.RLock()
-	defer cb.mu.RUnlock()
-	return cb.state
-}
-
-func (cb *CircuitBreaker) forceOpen() {
-	cb.mu.Lock()
-	defer cb.mu.Unlock()
-	cb.state = CircuitOpen
-	cb.lastFailTime = time.Now()
-}
-
-func (cb *CircuitBreaker) forceClose() {
-	cb.mu.Lock()
-	defer cb.mu.Unlock()
-	cb.state = CircuitClosed
-	cb.errorCount = 0
-	cb.successCount = 0
-}
-
-// GetStats returns current statistics for the circuit breaker
-func (cb *CircuitBreaker) GetStats() CircuitStats {
-	cb.mu.RLock()
-	defer cb.mu.RUnlock()
-	
-	// Calculate current window stats
-	windowSize := minInt(cb.requestCount, cb.config.WindowSize)
-	var totalLatency time.Duration
-	var maxLatency time.Duration
-	errorCount := 0
-	
-	for i := 0; i < windowSize; i++ {
-		req := cb.requests[i]
-		totalLatency += req.Latency
-		if req.Latency > maxLatency {
-			maxLatency = req.Latency
-		}
-		if !req.Success {
-			errorCount++
-		}
-	}
-	
-	var avgLatency time.Duration
-	if windowSize > 0 {
-		avgLatency = totalLatency / time.Duration(windowSize)
-	}
-	
-	errorRate := 0.0
-	if windowSize > 0 {
-		errorRate = float64(errorCount) / float64(windowSize) * 100
-	}
-	
-	return CircuitStats{
-		Provider:         cb.config.Provider,
-		State:            cb.state.String(),
-		ErrorCount:       cb.errorCount,
-		SuccessCount:     cb.successCount,
-		RequestCount:     cb.requestCount,
-		ErrorRate:        errorRate,
-		AvgLatency:       avgLatency,
-		MaxLatency:       maxLatency,
-		LastFailTime:     cb.lastFailTime,
-		LastSuccessTime:  cb.lastSuccessTime,
-		FallbackProviders: cb.fallbackProviders,
-	}
-}
-
-// CircuitStats represents statistics for a circuit breaker
-type CircuitStats struct {
-	Provider          string        `json:"provider"`
-	State             string        `json:"state"`
-	ErrorCount        int           `json:"error_count"`
-	SuccessCount      int           `json:"success_count"`
-	RequestCount      int           `json:"request_count"`
-	ErrorRate         float64       `json:"error_rate_percent"`
-	AvgLatency        time.Duration `json:"avg_latency"`
-	MaxLatency        time.Duration `json:"max_latency"`
-	LastFailTime      time.Time     `json:"last_fail_time"`
-	LastSuccessTime   time.Time     `json:"last_success_time"`
-	FallbackProviders []string      `json:"fallback_providers"`
-}
-
-// GetAllStats returns statistics for all circuits
-func (cm *CircuitManager) GetAllStats() map[string]CircuitStats {
-	cm.mu.RLock()
-	defer cm.mu.RUnlock()
-	
-	stats := make(map[string]CircuitStats)
-	for provider, circuit := range cm.circuits {
-		stats[provider] = circuit.GetStats()
-	}
-	
-	return stats
-}
-
+package datasources
+
+import (
+	"fmt"
+	"sync"
+	"time"
+)
+
+// CircuitState represents the state of a circuit breaker
+type CircuitState int
+
+const (
+	CircuitClosed CircuitState = iota
+	CircuitOpen
+	CircuitHalfOpen
+)
+
+func (s CircuitState) String() string {
+	switch s {
+	case CircuitClosed:
+		return "closed"
+	case CircuitOpen:
+		return "open"
+	case CircuitHalfOpen:
+		return "half-open"
+	default:
+		return "unknown"
+	}
+}
+
+// CircuitConfig defines configuration for a circuit breaker
+type CircuitConfig struct {
+	Provider            string        `json:"provider"`
+	ErrorThreshold      int           `json:"error_threshold"`        // Number of errors before opening
+	SuccessThreshold    int           `json:"success_threshold"`      // Successes needed to close from half-open
+	Timeout             time.Duration `json:"timeout"`                // How long to stay open before half-open
+	LatencyThreshold    time.Duration `json:"latency_threshold"`      // Max acceptable latency
+	BudgetThreshold     float64       `json:"budget_threshold"`       // Remaining budget % before opening
+	WindowSize          int           `json:"window_size"`            // Size of sliding window for error rate
+	MinRequestsInWindow int           `json:"min_requests_in_window"` // Min requests before calculating error rate
+}
+
+// CircuitBreaker implements a circuit breaker pattern for data providers
+type CircuitBreaker struct {
+	config          CircuitConfig
+	state           CircuitState
+	errorCount      int
+	successCount    int
+	requestCount    int
+	lastFailTime    time.Time
+	lastSuccessTime time.Time
+	mu              sync.RWMutex
+
+	// Sliding window for request tracking
+	requests    []RequestResult
+	windowStart int
+
+	// Fallback providers in order of preference
+	fallbackProviders []string
+	currentProvider   string
+}
+
+// RequestResult tracks the result of a request
+type RequestResult struct {
+	Timestamp time.Time
+	Success   bool
+	Latency   time.Duration
+	Error     error
+}
+
+// CircuitManager manages circuit breakers for all providers
+type CircuitManager struct {
+	circuits       map[string]*CircuitBreaker
+	fallbackChains map[string][]string
+	mu             sync.RWMutex
+}
+
+// Default circuit configurations
+var DefaultCircuitConfigs = map[string]CircuitConfig{
+	"binance": {
+		Provider:            "binance",
+		ErrorThreshold:      5,
+		SuccessThreshold:    3,
+		Timeout:             30 * time.Second,
+		LatencyThreshold:    5 * time.Second,
+		BudgetThreshold:     0.1, // Open when <10% budget remaining
+		WindowSize:          20,
+		MinRequestsInWindow: 5,
+	},
+	"coingecko": {
+		Provider:            "coingecko",
+		ErrorThreshold:      3,
+		SuccessThreshold:    2,
+		Timeout:             60 * time.Second,
+		LatencyThreshold:    10 * time.Second,
+		BudgetThreshold:     0.05, // Open when <5% monthly budget remaining
+		WindowSize:          15,
+		MinRequestsInWindow: 3,
+	},
+	"moralis": {
+		Provider:            "moralis",
+		ErrorThreshold:      3,
+		SuccessThreshold:    2,
+		Timeout:             45 * time.Second,
+		LatencyThreshold:    8 * time.Second,
+		BudgetThreshold:     0.1, // Open when <10% CU remaining
+		WindowSize:          15,
+		MinRequestsInWindow: 3,
+	},
+	"dexscreener": {
+		Provider:            "dexscreener",
+		ErrorThreshold:      4,
+		SuccessThreshold:    2,
+		Timeout:             30 * time.Second,
+		LatencyThreshold:    6 * time.Second,
+		BudgetThreshold:     0.0, // No budget limit
+		WindowSize:          20,
+		MinRequestsInWindow: 4,
+	},
+	"kraken": {
+		Provider:            "kraken",
+		ErrorThreshold:      2,
+		SuccessThreshold:    1,
+		Timeout:             60 * time.Second,
+		LatencyThreshold:    15 * time.Second,
+		BudgetThreshold:     0.0, // No budget limit
+		WindowSize:          10,
+		MinRequestsInWindow: 2,
+	},
+}
+
+// Default fallback chains - preferred order when primary fails
+var DefaultFallbackChains = map[string][]string{
+	"binance":     {"kraken", "coingecko"},
+	"coingecko":   {"binance", "kraken"},
+	"moralis":     {"coingecko", "binance"},
+	"dexscreener": {"coingecko", "binance"},
+	"kraken":      {"binance", "coingecko"},
+}
+
+// NewCircuitManager creates a new circuit manager with default configurations
+func NewCircuitManager() *CircuitManager {
+	cm := &CircuitManager{
+		circuits:       make(map[string]*CircuitBreaker),
+		fallbackChains: make(map[string][]string),
+	}
+
+	// Initialize circuits for all providers
+	for provider, config := range DefaultCircuitConfigs {
+		cm.circuits[provider] = &CircuitBreaker{
+			config:          config,
+			state:           CircuitClosed,
+			requests:        make([]RequestResult, config.WindowSize),
+			currentProvider: provider,
+		}
+
+		// Set fallback chains
+		if fallbacks, exists := DefaultFallbackChains[provider]; exists {
+			cm.fallbackChains[provider] = fallbacks
+			cm.circuits[provider].fallbackProviders = fallbacks
+		}
+	}
+
+	return cm
+}
+
+// CanMakeRequest checks if a request can be made to the provider
+func (cm *CircuitManager) CanMakeRequest(provider string) bool {
+	cm.mu.RLock()
+	circuit, exists := cm.circuits[provider]
+	cm.mu.RUnlock()
+
+	if !exists {
+		return false
+	}
+
+	return circuit.canMakeRequest()
+}
+
+// RecordRequest records the result of a request
+func (cm *CircuitManager) RecordRequest(provider string, success bool, latency time.Duration, err error) {
+	cm.mu.RLock()
+	circuit, exists := cm.circuits[provider]
+	cm.mu.RUnlock()
+
+	if !exists {
+		return
+	}
+
+	circuit.recordRequest(success, latency, err)
+}
+
+// GetActiveProvider returns the currently active provider (may be a fallback)
+func (cm *CircuitManager) GetActiveProvider(originalProvider string) string {
+	cm.mu.RLock()
+	circuit, exists := cm.circuits[originalProvider]
+	cm.mu.RUnlock()
+
+	if !exists {
+		return originalProvider
+	}
+
+	if circuit.canMakeRequest() {
+		return originalProvider
+	}
+
+	// Check fallback providers
+	for _, fallback := range circuit.fallbackProviders {
+		if fallbackCircuit, exists := cm.circuits[fallback]; exists {
+			if fallbackCircuit.canMakeRequest() {
+				return fallback
+			}
+		}
+	}
+
+	// Return original provider if no fallbacks available (will fail)
+	return originalProvider
+}
+
+// GetCircuitState returns the current state of a circuit
+func (cm *CircuitManager) GetCircuitState(provider string) CircuitState {
+	cm.mu.RLock()
+	circuit, exists := cm.circuits[provider]
+	cm.mu.RUnlock()
+
+	if !exists {
+		return CircuitClosed // Default to closed for unknown providers
+	}
+
+	return circuit.getState()
+}
+
+// ForceOpen forces a circuit to open (for testing or manual intervention)
+func (cm *CircuitManager) ForceOpen(provider string) {
+	cm.mu.RLock()
+	circuit, exists := cm.circuits[provider]
+	cm.mu.RUnlock()
+
+	if exists {
+		circuit.forceOpen()
+	}
+}
+
+// ForceClose forces a circuit to close (for testing or manual intervention)
+func (cm *CircuitManager) ForceClose(provider string) {
+	cm.mu.RLock()
+	circuit, exists := cm.circuits[provider]
+	cm.mu.RUnlock()
+
+	if exists {
+		circuit.forceClose()
+	}
+}
+
+// CheckBudgetThreshold checks if circuit should open due to low budget
+func (cm *CircuitManager) CheckBudgetThreshold(provider string, healthPercent float64) {
+	cm.mu.RLock()
+	circuit, exists := cm.circuits[provider]
+	cm.mu.RUnlock()
+
+	if !exists {
+		return
+	}
+
+	circuit.checkBudgetThreshold(healthPercent)
+}
+
+func (cb *CircuitBreaker) canMakeRequest() bool {
+	cb.mu.RLock()
+	defer cb.mu.RUnlock()
+
+	now := time.Now()
+
+	switch cb.state {
+	case CircuitClosed:
+		return true
+	case CircuitOpen:
+		// Check if timeout has passed
+		if now.Sub(cb.lastFailTime) >= cb.config.Timeout {
+			cb.mu.RUnlock()
+			cb.mu.Lock()
+			if cb.state == CircuitOpen { // Double-check after acquiring write lock
+				cb.state = CircuitHalfOpen
+				cb.successCount = 0
+			}
+			cb.mu.Unlock()
+			cb.mu.RLock()
+			return true
+		}
+		return false
+	case CircuitHalfOpen:
+		return true
+	}
+
+	return false
+}
+
+func (cb *CircuitBreaker) recordRequest(success bool, latency time.Duration, err error) {
+	cb.mu.Lock()
+	defer cb.mu.Unlock()
+
+	now := time.Now()
+
+	// Check latency threshold first
+	if success && latency > cb.config.LatencyThreshold {
+		success = false // Treat slow requests as failures
+		err = fmt.Errorf("request too slow: %v > %v", latency, cb.config.LatencyThreshold)
+	}
+
+	// Add request to sliding window with corrected success value
+	cb.requests[cb.requestCount%cb.config.WindowSize] = RequestResult{
+		Timestamp: now,
+		Success:   success,
+		Latency:   latency,
+		Error:     err,
+	}
+	cb.requestCount++
+
+	if success {
+		cb.successCount++
+		cb.lastSuccessTime = now
+
+		// If half-open and enough successes, close the circuit
+		if cb.state == CircuitHalfOpen && cb.successCount >= cb.config.SuccessThreshold {
+			cb.state = CircuitClosed
+			cb.errorCount = 0
+		}
+	} else {
+		cb.errorCount++
+		cb.lastFailTime = now
+		cb.successCount = 0
+
+		// Check if we should open the circuit
+		if cb.shouldOpen() {
+			cb.state = CircuitOpen
+		}
+	}
+}
+
+func (cb *CircuitBreaker) shouldOpen() bool {
+	// Only consider opening if we have enough requests in the window
+	if cb.requestCount < cb.config.MinRequestsInWindow {
+		return false
+	}
+
+	// Calculate error rate over sliding window
+	windowSize := minInt(cb.requestCount, cb.config.WindowSize)
+	errorCount := 0
+
+	for i := 0; i < windowSize; i++ {
+		if !cb.requests[i].Success {
+			errorCount++
+		}
+	}
+
+	errorRate := float64(errorCount) / float64(windowSize)
+	threshold := float64(cb.config.ErrorThreshold) / float64(cb.config.WindowSize)
+
+	return errorRate >= threshold
+}
+
+func (cb *CircuitBreaker) checkBudgetThreshold(healthPercent float64) {
+	cb.mu.Lock()
+	defer cb.mu.Unlock()
+
+	if cb.config.BudgetThreshold > 0 && healthPercent < cb.config.BudgetThreshold*100 {
+		if cb.state == CircuitClosed {
+			cb.state = CircuitOpen
+			cb.lastFailTime = time.Now()
+		}
+	}
+}
+
+func (cb *CircuitBreaker) getState() CircuitState {
+	cb.mu.RLock()
+	defer cb.mu.RUnlock()
+	return cb.state
+}
+
+func (cb *CircuitBreaker) forceOpen() {
+	cb.mu.Lock()
+	defer cb.mu.Unlock()
+	cb.state = CircuitOpen
+	cb.lastFailTime = time.Now()
+}
+
+func (cb *CircuitBreaker) forceClose() {
+	cb.mu.Lock()
+	defer cb.mu.Unlock()
+	cb.state = CircuitClosed
+	cb.errorCount = 0
+	cb.successCount = 0
+}
+
+// GetStats returns current statistics for the circuit breaker
+func (cb *CircuitBreaker) GetStats() CircuitStats {
+	cb.mu.RLock()
+	defer cb.mu.RUnlock()
+
+	// Calculate current window stats
+	windowSize := minInt(cb.requestCount, cb.config.WindowSize)
+	var totalLatency time.Duration
+	var maxLatency time.Duration
+	errorCount := 0
+
+	for i := 0; i < windowSize; i++ {
+		req := cb.requests[i]
+		totalLatency += req.Latency
+		if req.Latency > maxLatency {
+			maxLatency = req.Latency
+		}
+		if !req.Success {
+			errorCount++
+		}
+	}
+
+	var avgLatency time.Duration
+	if windowSize > 0 {
+		avgLatency = totalLatency / time.Duration(windowSize)
+	}
+
+	errorRate := 0.0
+	if windowSize > 0 {
+		errorRate = float64(errorCount) / float64(windowSize) * 100
+	}
+
+	return CircuitStats{
+		Provider:          cb.config.Provider,
+		State:             cb.state.String(),
+		ErrorCount:        cb.errorCount,
+		SuccessCount:      cb.successCount,
+		RequestCount:      cb.requestCount,
+		ErrorRate:         errorRate,
+		AvgLatency:        avgLatency,
+		MaxLatency:        maxLatency,
+		LastFailTime:      cb.lastFailTime,
+		LastSuccessTime:   cb.lastSuccessTime,
+		FallbackProviders: cb.fallbackProviders,
+	}
+}
+
+// CircuitStats represents statistics for a circuit breaker
+type CircuitStats struct {
+	Provider          string        `json:"provider"`
+	State             string        `json:"state"`
+	ErrorCount        int           `json:"error_count"`
+	SuccessCount      int           `json:"success_count"`
+	RequestCount      int           `json:"request_count"`
+	ErrorRate         float64       `json:"error_rate_percent"`
+	AvgLatency        time.Duration `json:"avg_latency"`
+	MaxLatency        time.Duration `json:"max_latency"`
+	LastFailTime      time.Time     `json:"last_fail_time"`
+	LastSuccessTime   time.Time     `json:"last_success_time"`
+	FallbackProviders []string      `json:"fallback_providers"`
+}
+
+// GetAllStats returns statistics for all circuits
+func (cm *CircuitManager) GetAllStats() map[string]CircuitStats {
+	cm.mu.RLock()
+	defer cm.mu.RUnlock()
+
+	stats := make(map[string]CircuitStats)
+	for provider, circuit := range cm.circuits {
+		stats[provider] = circuit.GetStats()
+	}
+
+	return stats
+}
diff --git a/internal/datasources/circuits_test.go b/internal/datasources/circuits_test.go
index d5b9254..a0495f9 100644
--- a/internal/datasources/circuits_test.go
+++ b/internal/datasources/circuits_test.go
@@ -1,320 +1,320 @@
-package datasources
-
-import (
-	"errors"
-	"testing"
-	"time"
-)
-
-func TestCircuitManager_CanMakeRequest(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Should be able to make requests initially (closed state)
-	if !cm.CanMakeRequest("binance") {
-		t.Error("Should be able to make request when circuit is closed")
-	}
-	
-	// Unknown provider should return false
-	if cm.CanMakeRequest("unknown") {
-		t.Error("Should not be able to make request to unknown provider")
-	}
-}
-
-func TestCircuitManager_RecordSuccess(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Record successful requests
-	cm.RecordRequest("binance", true, 100*time.Millisecond, nil)
-	cm.RecordRequest("binance", true, 200*time.Millisecond, nil)
-	
-	// Circuit should remain closed
-	if cm.GetCircuitState("binance") != CircuitClosed {
-		t.Error("Circuit should remain closed after successful requests")
-	}
-}
-
-func TestCircuitManager_RecordFailures(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Record enough failures to open the circuit
-	config := DefaultCircuitConfigs["binance"]
-	
-	// Need minimum requests before circuit can open
-	for i := 0; i < config.MinRequestsInWindow; i++ {
-		cm.RecordRequest("binance", false, 0, errors.New("test error"))
-	}
-	
-	// Circuit should now be open
-	if cm.GetCircuitState("binance") != CircuitOpen {
-		t.Error("Circuit should be open after enough failures")
-	}
-	
-	// Should not be able to make requests
-	if cm.CanMakeRequest("binance") {
-		t.Error("Should not be able to make request when circuit is open")
-	}
-}
-
-func TestCircuitManager_LatencyThreshold(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	config := DefaultCircuitConfigs["binance"]
-	slowLatency := config.LatencyThreshold + time.Second
-	
-	// Record slow requests (should be treated as failures)
-	for i := 0; i < config.MinRequestsInWindow; i++ {
-		cm.RecordRequest("binance", true, slowLatency, nil)
-	}
-	
-	// Circuit should open due to slow requests
-	if cm.GetCircuitState("binance") != CircuitOpen {
-		t.Error("Circuit should be open after slow requests")
-	}
-}
-
-func TestCircuitManager_HalfOpenTransition(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Open the circuit first
-	cm.ForceOpen("binance")
-	
-	if cm.GetCircuitState("binance") != CircuitOpen {
-		t.Error("Circuit should be open after ForceOpen")
-	}
-	
-	// Modify timeout for testing (make it very short)
-	circuit := cm.circuits["binance"]
-	circuit.mu.Lock()
-	circuit.config.Timeout = 10 * time.Millisecond
-	circuit.lastFailTime = time.Now().Add(-20 * time.Millisecond) // Set in the past
-	circuit.mu.Unlock()
-	
-	// Should transition to half-open after timeout
-	if !cm.CanMakeRequest("binance") {
-		t.Error("Should be able to make request after timeout (half-open state)")
-	}
-	
-	if cm.GetCircuitState("binance") != CircuitHalfOpen {
-		t.Error("Circuit should be half-open after timeout")
-	}
-}
-
-func TestCircuitManager_HalfOpenToClosedTransition(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Set circuit to half-open
-	circuit := cm.circuits["binance"]
-	circuit.mu.Lock()
-	circuit.state = CircuitHalfOpen
-	circuit.successCount = 0
-	circuit.mu.Unlock()
-	
-	config := DefaultCircuitConfigs["binance"]
-	
-	// Record successful requests to close the circuit
-	for i := 0; i < config.SuccessThreshold; i++ {
-		cm.RecordRequest("binance", true, 100*time.Millisecond, nil)
-	}
-	
-	// Circuit should now be closed
-	if cm.GetCircuitState("binance") != CircuitClosed {
-		t.Error("Circuit should be closed after enough successes from half-open")
-	}
-}
-
-func TestCircuitManager_FallbackProviders(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Open binance circuit
-	cm.ForceOpen("binance")
-	
-	// Should get fallback provider
-	activeProvider := cm.GetActiveProvider("binance")
-	if activeProvider == "binance" {
-		t.Error("Should return fallback provider when primary is down")
-	}
-	
-	// Should be one of the fallback providers
-	fallbacks := DefaultFallbackChains["binance"]
-	found := false
-	for _, fallback := range fallbacks {
-		if activeProvider == fallback {
-			found = true
-			break
-		}
-	}
-	if !found {
-		t.Errorf("Active provider %s should be in fallback chain %v", activeProvider, fallbacks)
-	}
-}
-
-func TestCircuitManager_BudgetThreshold(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Circuit should open when budget is too low
-	lowHealthPercent := 5.0 // Below 10% threshold for binance
-	cm.CheckBudgetThreshold("binance", lowHealthPercent)
-	
-	if cm.GetCircuitState("binance") != CircuitOpen {
-		t.Error("Circuit should be open when budget is below threshold")
-	}
-}
-
-func TestCircuitManager_ForceOperations(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Test force open
-	cm.ForceOpen("binance")
-	if cm.GetCircuitState("binance") != CircuitOpen {
-		t.Error("Circuit should be open after ForceOpen")
-	}
-	
-	// Test force close
-	cm.ForceClose("binance")
-	if cm.GetCircuitState("binance") != CircuitClosed {
-		t.Error("Circuit should be closed after ForceClose")
-	}
-}
-
-func TestCircuitBreaker_Stats(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Record some requests
-	cm.RecordRequest("binance", true, 100*time.Millisecond, nil)
-	cm.RecordRequest("binance", false, 200*time.Millisecond, errors.New("test error"))
-	cm.RecordRequest("binance", true, 150*time.Millisecond, nil)
-	
-	circuit := cm.circuits["binance"]
-	stats := circuit.GetStats()
-	
-	if stats.Provider != "binance" {
-		t.Errorf("Expected provider binance, got %s", stats.Provider)
-	}
-	
-	if stats.RequestCount != 3 {
-		t.Errorf("Expected 3 requests, got %d", stats.RequestCount)
-	}
-	
-	if stats.ErrorRate <= 0 {
-		t.Error("Expected non-zero error rate")
-	}
-	
-	if stats.AvgLatency <= 0 {
-		t.Error("Expected positive average latency")
-	}
-	
-	if stats.MaxLatency != 200*time.Millisecond {
-		t.Errorf("Expected max latency 200ms, got %v", stats.MaxLatency)
-	}
-}
-
-func TestCircuitManager_GetAllStats(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Record requests for multiple providers
-	cm.RecordRequest("binance", true, 100*time.Millisecond, nil)
-	cm.RecordRequest("coingecko", false, 200*time.Millisecond, errors.New("test"))
-	
-	allStats := cm.GetAllStats()
-	
-	if len(allStats) != len(DefaultCircuitConfigs) {
-		t.Errorf("Expected %d provider stats, got %d", len(DefaultCircuitConfigs), len(allStats))
-	}
-	
-	if _, exists := allStats["binance"]; !exists {
-		t.Error("Expected binance stats to exist")
-	}
-	
-	if _, exists := allStats["coingecko"]; !exists {
-		t.Error("Expected coingecko stats to exist")
-	}
-}
-
-func TestCircuitState_String(t *testing.T) {
-	tests := []struct {
-		state    CircuitState
-		expected string
-	}{
-		{CircuitClosed, "closed"},
-		{CircuitOpen, "open"},
-		{CircuitHalfOpen, "half-open"},
-		{CircuitState(999), "unknown"},
-	}
-	
-	for _, test := range tests {
-		if test.state.String() != test.expected {
-			t.Errorf("Expected %s, got %s", test.expected, test.state.String())
-		}
-	}
-}
-
-func TestDefaultCircuitConfigs(t *testing.T) {
-	// Verify all default configurations are valid
-	for provider, config := range DefaultCircuitConfigs {
-		if config.Provider != provider {
-			t.Errorf("Provider %s config has mismatched name %s", provider, config.Provider)
-		}
-		if config.ErrorThreshold <= 0 {
-			t.Errorf("Provider %s has invalid ErrorThreshold: %d", provider, config.ErrorThreshold)
-		}
-		if config.SuccessThreshold <= 0 {
-			t.Errorf("Provider %s has invalid SuccessThreshold: %d", provider, config.SuccessThreshold)
-		}
-		if config.Timeout <= 0 {
-			t.Errorf("Provider %s has invalid Timeout: %v", provider, config.Timeout)
-		}
-		if config.WindowSize <= 0 {
-			t.Errorf("Provider %s has invalid WindowSize: %d", provider, config.WindowSize)
-		}
-		if config.MinRequestsInWindow <= 0 {
-			t.Errorf("Provider %s has invalid MinRequestsInWindow: %d", provider, config.MinRequestsInWindow)
-		}
-	}
-}
-
-func TestDefaultFallbackChains(t *testing.T) {
-	// Verify fallback chains are valid
-	for provider, fallbacks := range DefaultFallbackChains {
-		if len(fallbacks) == 0 {
-			t.Errorf("Provider %s has no fallback providers", provider)
-		}
-		
-		// Ensure fallback providers exist in configs
-		for _, fallback := range fallbacks {
-			if _, exists := DefaultCircuitConfigs[fallback]; !exists {
-				t.Errorf("Provider %s has invalid fallback %s", provider, fallback)
-			}
-		}
-		
-		// Ensure provider doesn't include itself in fallbacks
-		for _, fallback := range fallbacks {
-			if fallback == provider {
-				t.Errorf("Provider %s includes itself in fallback chain", provider)
-			}
-		}
-	}
-}
-
-func TestCircuitManager_ConcurrentAccess(t *testing.T) {
-	cm := NewCircuitManager()
-	
-	// Test concurrent access doesn't cause panics
-	done := make(chan bool)
-	
-	for i := 0; i < 10; i++ {
-		go func() {
-			for j := 0; j < 100; j++ {
-				cm.CanMakeRequest("binance")
-				cm.RecordRequest("binance", j%2 == 0, time.Duration(j)*time.Millisecond, nil)
-				cm.GetCircuitState("binance")
-				cm.GetActiveProvider("binance")
-			}
-			done <- true
-		}()
-	}
-	
-	// Wait for all goroutines to complete
-	for i := 0; i < 10; i++ {
-		<-done
-	}
-}
\ No newline at end of file
+package datasources
+
+import (
+	"errors"
+	"testing"
+	"time"
+)
+
+func TestCircuitManager_CanMakeRequest(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Should be able to make requests initially (closed state)
+	if !cm.CanMakeRequest("binance") {
+		t.Error("Should be able to make request when circuit is closed")
+	}
+
+	// Unknown provider should return false
+	if cm.CanMakeRequest("unknown") {
+		t.Error("Should not be able to make request to unknown provider")
+	}
+}
+
+func TestCircuitManager_RecordSuccess(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Record successful requests
+	cm.RecordRequest("binance", true, 100*time.Millisecond, nil)
+	cm.RecordRequest("binance", true, 200*time.Millisecond, nil)
+
+	// Circuit should remain closed
+	if cm.GetCircuitState("binance") != CircuitClosed {
+		t.Error("Circuit should remain closed after successful requests")
+	}
+}
+
+func TestCircuitManager_RecordFailures(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Record enough failures to open the circuit
+	config := DefaultCircuitConfigs["binance"]
+
+	// Need minimum requests before circuit can open
+	for i := 0; i < config.MinRequestsInWindow; i++ {
+		cm.RecordRequest("binance", false, 0, errors.New("test error"))
+	}
+
+	// Circuit should now be open
+	if cm.GetCircuitState("binance") != CircuitOpen {
+		t.Error("Circuit should be open after enough failures")
+	}
+
+	// Should not be able to make requests
+	if cm.CanMakeRequest("binance") {
+		t.Error("Should not be able to make request when circuit is open")
+	}
+}
+
+func TestCircuitManager_LatencyThreshold(t *testing.T) {
+	cm := NewCircuitManager()
+
+	config := DefaultCircuitConfigs["binance"]
+	slowLatency := config.LatencyThreshold + time.Second
+
+	// Record slow requests (should be treated as failures)
+	for i := 0; i < config.MinRequestsInWindow; i++ {
+		cm.RecordRequest("binance", true, slowLatency, nil)
+	}
+
+	// Circuit should open due to slow requests
+	if cm.GetCircuitState("binance") != CircuitOpen {
+		t.Error("Circuit should be open after slow requests")
+	}
+}
+
+func TestCircuitManager_HalfOpenTransition(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Open the circuit first
+	cm.ForceOpen("binance")
+
+	if cm.GetCircuitState("binance") != CircuitOpen {
+		t.Error("Circuit should be open after ForceOpen")
+	}
+
+	// Modify timeout for testing (make it very short)
+	circuit := cm.circuits["binance"]
+	circuit.mu.Lock()
+	circuit.config.Timeout = 10 * time.Millisecond
+	circuit.lastFailTime = time.Now().Add(-20 * time.Millisecond) // Set in the past
+	circuit.mu.Unlock()
+
+	// Should transition to half-open after timeout
+	if !cm.CanMakeRequest("binance") {
+		t.Error("Should be able to make request after timeout (half-open state)")
+	}
+
+	if cm.GetCircuitState("binance") != CircuitHalfOpen {
+		t.Error("Circuit should be half-open after timeout")
+	}
+}
+
+func TestCircuitManager_HalfOpenToClosedTransition(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Set circuit to half-open
+	circuit := cm.circuits["binance"]
+	circuit.mu.Lock()
+	circuit.state = CircuitHalfOpen
+	circuit.successCount = 0
+	circuit.mu.Unlock()
+
+	config := DefaultCircuitConfigs["binance"]
+
+	// Record successful requests to close the circuit
+	for i := 0; i < config.SuccessThreshold; i++ {
+		cm.RecordRequest("binance", true, 100*time.Millisecond, nil)
+	}
+
+	// Circuit should now be closed
+	if cm.GetCircuitState("binance") != CircuitClosed {
+		t.Error("Circuit should be closed after enough successes from half-open")
+	}
+}
+
+func TestCircuitManager_FallbackProviders(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Open binance circuit
+	cm.ForceOpen("binance")
+
+	// Should get fallback provider
+	activeProvider := cm.GetActiveProvider("binance")
+	if activeProvider == "binance" {
+		t.Error("Should return fallback provider when primary is down")
+	}
+
+	// Should be one of the fallback providers
+	fallbacks := DefaultFallbackChains["binance"]
+	found := false
+	for _, fallback := range fallbacks {
+		if activeProvider == fallback {
+			found = true
+			break
+		}
+	}
+	if !found {
+		t.Errorf("Active provider %s should be in fallback chain %v", activeProvider, fallbacks)
+	}
+}
+
+func TestCircuitManager_BudgetThreshold(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Circuit should open when budget is too low
+	lowHealthPercent := 5.0 // Below 10% threshold for binance
+	cm.CheckBudgetThreshold("binance", lowHealthPercent)
+
+	if cm.GetCircuitState("binance") != CircuitOpen {
+		t.Error("Circuit should be open when budget is below threshold")
+	}
+}
+
+func TestCircuitManager_ForceOperations(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Test force open
+	cm.ForceOpen("binance")
+	if cm.GetCircuitState("binance") != CircuitOpen {
+		t.Error("Circuit should be open after ForceOpen")
+	}
+
+	// Test force close
+	cm.ForceClose("binance")
+	if cm.GetCircuitState("binance") != CircuitClosed {
+		t.Error("Circuit should be closed after ForceClose")
+	}
+}
+
+func TestCircuitBreaker_Stats(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Record some requests
+	cm.RecordRequest("binance", true, 100*time.Millisecond, nil)
+	cm.RecordRequest("binance", false, 200*time.Millisecond, errors.New("test error"))
+	cm.RecordRequest("binance", true, 150*time.Millisecond, nil)
+
+	circuit := cm.circuits["binance"]
+	stats := circuit.GetStats()
+
+	if stats.Provider != "binance" {
+		t.Errorf("Expected provider binance, got %s", stats.Provider)
+	}
+
+	if stats.RequestCount != 3 {
+		t.Errorf("Expected 3 requests, got %d", stats.RequestCount)
+	}
+
+	if stats.ErrorRate <= 0 {
+		t.Error("Expected non-zero error rate")
+	}
+
+	if stats.AvgLatency <= 0 {
+		t.Error("Expected positive average latency")
+	}
+
+	if stats.MaxLatency != 200*time.Millisecond {
+		t.Errorf("Expected max latency 200ms, got %v", stats.MaxLatency)
+	}
+}
+
+func TestCircuitManager_GetAllStats(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Record requests for multiple providers
+	cm.RecordRequest("binance", true, 100*time.Millisecond, nil)
+	cm.RecordRequest("coingecko", false, 200*time.Millisecond, errors.New("test"))
+
+	allStats := cm.GetAllStats()
+
+	if len(allStats) != len(DefaultCircuitConfigs) {
+		t.Errorf("Expected %d provider stats, got %d", len(DefaultCircuitConfigs), len(allStats))
+	}
+
+	if _, exists := allStats["binance"]; !exists {
+		t.Error("Expected binance stats to exist")
+	}
+
+	if _, exists := allStats["coingecko"]; !exists {
+		t.Error("Expected coingecko stats to exist")
+	}
+}
+
+func TestCircuitState_String(t *testing.T) {
+	tests := []struct {
+		state    CircuitState
+		expected string
+	}{
+		{CircuitClosed, "closed"},
+		{CircuitOpen, "open"},
+		{CircuitHalfOpen, "half-open"},
+		{CircuitState(999), "unknown"},
+	}
+
+	for _, test := range tests {
+		if test.state.String() != test.expected {
+			t.Errorf("Expected %s, got %s", test.expected, test.state.String())
+		}
+	}
+}
+
+func TestDefaultCircuitConfigs(t *testing.T) {
+	// Verify all default configurations are valid
+	for provider, config := range DefaultCircuitConfigs {
+		if config.Provider != provider {
+			t.Errorf("Provider %s config has mismatched name %s", provider, config.Provider)
+		}
+		if config.ErrorThreshold <= 0 {
+			t.Errorf("Provider %s has invalid ErrorThreshold: %d", provider, config.ErrorThreshold)
+		}
+		if config.SuccessThreshold <= 0 {
+			t.Errorf("Provider %s has invalid SuccessThreshold: %d", provider, config.SuccessThreshold)
+		}
+		if config.Timeout <= 0 {
+			t.Errorf("Provider %s has invalid Timeout: %v", provider, config.Timeout)
+		}
+		if config.WindowSize <= 0 {
+			t.Errorf("Provider %s has invalid WindowSize: %d", provider, config.WindowSize)
+		}
+		if config.MinRequestsInWindow <= 0 {
+			t.Errorf("Provider %s has invalid MinRequestsInWindow: %d", provider, config.MinRequestsInWindow)
+		}
+	}
+}
+
+func TestDefaultFallbackChains(t *testing.T) {
+	// Verify fallback chains are valid
+	for provider, fallbacks := range DefaultFallbackChains {
+		if len(fallbacks) == 0 {
+			t.Errorf("Provider %s has no fallback providers", provider)
+		}
+
+		// Ensure fallback providers exist in configs
+		for _, fallback := range fallbacks {
+			if _, exists := DefaultCircuitConfigs[fallback]; !exists {
+				t.Errorf("Provider %s has invalid fallback %s", provider, fallback)
+			}
+		}
+
+		// Ensure provider doesn't include itself in fallbacks
+		for _, fallback := range fallbacks {
+			if fallback == provider {
+				t.Errorf("Provider %s includes itself in fallback chain", provider)
+			}
+		}
+	}
+}
+
+func TestCircuitManager_ConcurrentAccess(t *testing.T) {
+	cm := NewCircuitManager()
+
+	// Test concurrent access doesn't cause panics
+	done := make(chan bool)
+
+	for i := 0; i < 10; i++ {
+		go func() {
+			for j := 0; j < 100; j++ {
+				cm.CanMakeRequest("binance")
+				cm.RecordRequest("binance", j%2 == 0, time.Duration(j)*time.Millisecond, nil)
+				cm.GetCircuitState("binance")
+				cm.GetActiveProvider("binance")
+			}
+			done <- true
+		}()
+	}
+
+	// Wait for all goroutines to complete
+	for i := 0; i < 10; i++ {
+		<-done
+	}
+}
diff --git a/internal/datasources/health.go b/internal/datasources/health.go
index 2c5eeb3..35d869d 100644
--- a/internal/datasources/health.go
+++ b/internal/datasources/health.go
@@ -1,379 +1,379 @@
-package datasources
-
-import (
-	"encoding/json"
-	"fmt"
-	"sync"
-	"time"
-)
-
-// HealthManager aggregates health information from all datasource components
-type HealthManager struct {
-	providerManager *ProviderManager
-	cacheManager    *CacheManager
-	circuitManager  *CircuitManager
-	mu              sync.RWMutex
-	
-	// Latency tracking
-	latencyStats map[string]*LatencyTracker
-}
-
-// LatencyTracker tracks latency percentiles for a provider
-type LatencyTracker struct {
-	samples []time.Duration
-	mu      sync.RWMutex
-}
-
-// HealthSnapshot represents the complete health state of all datasources
-type HealthSnapshot struct {
-	Timestamp     time.Time                    `json:"timestamp"`
-	OverallHealth string                       `json:"overall_health"`
-	Providers     map[string]ProviderHealth    `json:"providers"`
-	Cache         CacheHealth                  `json:"cache"`
-	Circuits      map[string]CircuitHealth     `json:"circuits"`
-	Summary       HealthSummary                `json:"summary"`
-}
-
-// ProviderHealth represents health metrics for a single provider
-type ProviderHealth struct {
-	Name          string        `json:"name"`
-	Status        string        `json:"status"`
-	RequestsToday int64         `json:"requests_today"`
-	RequestsMonth int64         `json:"requests_month"`
-	DailyQuota    int64         `json:"daily_quota"`
-	MonthlyQuota  int64         `json:"monthly_quota"`
-	HealthPercent float64       `json:"health_percent"`
-	WeightUsed    int           `json:"weight_used,omitempty"`
-	Latency       LatencyMetrics `json:"latency"`
-	Cost          float64       `json:"cost"`
-	LastRequest   time.Time     `json:"last_request"`
-	Circuit       string        `json:"circuit_state"`
-}
-
-// LatencyMetrics represents latency statistics
-type LatencyMetrics struct {
-	P50 time.Duration `json:"p50"`
-	P95 time.Duration `json:"p95"`
-	P99 time.Duration `json:"p99"`
-	Max time.Duration `json:"max"`
-	Avg time.Duration `json:"avg"`
-}
-
-// CacheHealth represents cache system health
-type CacheHealth struct {
-	Status         string `json:"status"`
-	TotalEntries   int    `json:"total_entries"`
-	ActiveEntries  int    `json:"active_entries"`
-	ExpiredEntries int    `json:"expired_entries"`
-	HitRate        float64 `json:"hit_rate_percent"`
-}
-
-// CircuitHealth represents circuit breaker health
-type CircuitHealth struct {
-	Provider    string        `json:"provider"`
-	State       string        `json:"state"`
-	ErrorRate   float64       `json:"error_rate_percent"`
-	AvgLatency  time.Duration `json:"avg_latency"`
-	MaxLatency  time.Duration `json:"max_latency"`
-	LastFailure time.Time     `json:"last_failure,omitempty"`
-}
-
-// HealthSummary provides high-level health indicators
-type HealthSummary struct {
-	ProvidersHealthy   int     `json:"providers_healthy"`
-	ProvidersTotal     int     `json:"providers_total"`
-	CircuitsClosed     int     `json:"circuits_closed"`
-	CircuitsTotal      int     `json:"circuits_total"`
-	OverallLatencyP99  time.Duration `json:"overall_latency_p99"`
-	CacheHitRate       float64 `json:"cache_hit_rate"`
-	BudgetUtilization  float64 `json:"budget_utilization_percent"`
-}
-
-// NewHealthManager creates a new health manager
-func NewHealthManager(pm *ProviderManager, cm *CacheManager, circm *CircuitManager) *HealthManager {
-	return &HealthManager{
-		providerManager: pm,
-		cacheManager:    cm,
-		circuitManager:  circm,
-		latencyStats:    make(map[string]*LatencyTracker),
-	}
-}
-
-// RecordLatency records a latency sample for a provider
-func (hm *HealthManager) RecordLatency(provider string, latency time.Duration) {
-	hm.mu.Lock()
-	defer hm.mu.Unlock()
-	
-	tracker, exists := hm.latencyStats[provider]
-	if !exists {
-		tracker = &LatencyTracker{
-			samples: make([]time.Duration, 0, 1000), // Keep last 1000 samples
-		}
-		hm.latencyStats[provider] = tracker
-	}
-	
-	tracker.addSample(latency)
-}
-
-// GetHealthSnapshot returns a complete health snapshot
-func (hm *HealthManager) GetHealthSnapshot() HealthSnapshot {
-	hm.mu.RLock()
-	defer hm.mu.RUnlock()
-	
-	snapshot := HealthSnapshot{
-		Timestamp: time.Now(),
-		Providers: make(map[string]ProviderHealth),
-		Circuits:  make(map[string]CircuitHealth),
-	}
-	
-	// Collect provider health
-	providersHealthy := 0
-	totalBudgetUsed := 0.0
-	totalProviders := 0
-	
-	for providerName := range DefaultProviders {
-		totalProviders++
-		
-		usage, err := hm.providerManager.GetUsageStats(providerName)
-		if err != nil {
-			continue
-		}
-		
-		circuitState := hm.circuitManager.GetCircuitState(providerName)
-		
-		providerHealth := ProviderHealth{
-			Name:          usage.Provider,
-			RequestsToday: usage.RequestsToday,
-			RequestsMonth: usage.RequestsMonth,
-			DailyQuota:    usage.DailyQuota,
-			MonthlyQuota:  usage.MonthlyQuota,
-			HealthPercent: usage.HealthPercent,
-			WeightUsed:    usage.WeightUsed,
-			Cost:          0.0, // Free APIs
-			LastRequest:   usage.LastRequest,
-			Circuit:       circuitState.String(),
-		}
-		
-		// Calculate provider status
-		if circuitState == CircuitClosed && usage.HealthPercent > 50 {
-			providerHealth.Status = "healthy"
-			providersHealthy++
-		} else if circuitState == CircuitHalfOpen || (circuitState == CircuitClosed && usage.HealthPercent > 10) {
-			providerHealth.Status = "degraded"
-		} else {
-			providerHealth.Status = "unhealthy"
-		}
-		
-		// Add latency metrics
-		if tracker, exists := hm.latencyStats[providerName]; exists {
-			providerHealth.Latency = tracker.getMetrics()
-		}
-		
-		// Track budget utilization
-		if usage.HealthPercent < 100 {
-			totalBudgetUsed += (100.0 - usage.HealthPercent)
-		}
-		
-		snapshot.Providers[providerName] = providerHealth
-	}
-	
-	// Collect cache health
-	cacheStats := hm.cacheManager.Stats()
-	snapshot.Cache = CacheHealth{
-		Status:         "healthy", // Simple status for now
-		TotalEntries:   cacheStats.TotalEntries,
-		ActiveEntries:  cacheStats.ActiveEntries,
-		ExpiredEntries: cacheStats.ExpiredEntries,
-		HitRate:        85.0, // Placeholder - would need actual hit tracking
-	}
-	
-	if cacheStats.ExpiredEntries > cacheStats.ActiveEntries {
-		snapshot.Cache.Status = "degraded"
-	}
-	
-	// Collect circuit health
-	circuitStats := hm.circuitManager.GetAllStats()
-	circuitsClosed := 0
-	totalCircuits := len(circuitStats)
-	var maxLatencyP99 time.Duration
-	
-	for provider, stats := range circuitStats {
-		circuitHealth := CircuitHealth{
-			Provider:   stats.Provider,
-			State:      stats.State,
-			ErrorRate:  stats.ErrorRate,
-			AvgLatency: stats.AvgLatency,
-			MaxLatency: stats.MaxLatency,
-		}
-		
-		if !stats.LastFailTime.IsZero() {
-			circuitHealth.LastFailure = stats.LastFailTime
-		}
-		
-		if stats.State == "closed" {
-			circuitsClosed++
-		}
-		
-		if stats.MaxLatency > maxLatencyP99 {
-			maxLatencyP99 = stats.MaxLatency
-		}
-		
-		snapshot.Circuits[provider] = circuitHealth
-	}
-	
-	// Create summary
-	snapshot.Summary = HealthSummary{
-		ProvidersHealthy:   providersHealthy,
-		ProvidersTotal:     totalProviders,
-		CircuitsClosed:     circuitsClosed,
-		CircuitsTotal:      totalCircuits,
-		OverallLatencyP99:  maxLatencyP99,
-		CacheHitRate:       snapshot.Cache.HitRate,
-		BudgetUtilization:  totalBudgetUsed / float64(totalProviders),
-	}
-	
-	// Determine overall health
-	snapshot.OverallHealth = hm.calculateOverallHealth(snapshot.Summary)
-	
-	return snapshot
-}
-
-// GetHealthJSON returns the health snapshot as JSON string
-func (hm *HealthManager) GetHealthJSON() (string, error) {
-	snapshot := hm.GetHealthSnapshot()
-	data, err := json.MarshalIndent(snapshot, "", "  ")
-	if err != nil {
-		return "", fmt.Errorf("failed to marshal health snapshot: %w", err)
-	}
-	return string(data), nil
-}
-
-// GetHealthSummary returns a brief health summary
-func (hm *HealthManager) GetHealthSummary() string {
-	snapshot := hm.GetHealthSnapshot()
-	
-	return fmt.Sprintf(
-		"Health: %s | Providers: %d/%d healthy | Circuits: %d/%d closed | Cache: %s (%.1f%% hit rate) | Latency P99: %v",
-		snapshot.OverallHealth,
-		snapshot.Summary.ProvidersHealthy,
-		snapshot.Summary.ProvidersTotal,
-		snapshot.Summary.CircuitsClosed,
-		snapshot.Summary.CircuitsTotal,
-		snapshot.Cache.Status,
-		snapshot.Cache.HitRate,
-		snapshot.Summary.OverallLatencyP99,
-	)
-}
-
-// IsHealthy returns true if the overall system is healthy
-func (hm *HealthManager) IsHealthy() bool {
-	snapshot := hm.GetHealthSnapshot()
-	return snapshot.OverallHealth == "healthy"
-}
-
-func (hm *HealthManager) calculateOverallHealth(summary HealthSummary) string {
-	// System is healthy if:
-	// - >50% providers are healthy
-	// - >50% circuits are closed
-	// - Cache hit rate >70%
-	// - P99 latency <10s
-	
-	providerHealthRatio := float64(summary.ProvidersHealthy) / float64(summary.ProvidersTotal)
-	circuitHealthRatio := float64(summary.CircuitsClosed) / float64(summary.CircuitsTotal)
-	
-	if providerHealthRatio >= 0.5 &&
-		circuitHealthRatio >= 0.5 &&
-		summary.CacheHitRate >= 70.0 &&
-		summary.OverallLatencyP99 < 10*time.Second {
-		return "healthy"
-	}
-	
-	if providerHealthRatio >= 0.3 &&
-		circuitHealthRatio >= 0.3 &&
-		summary.CacheHitRate >= 50.0 {
-		return "degraded"
-	}
-	
-	return "unhealthy"
-}
-
-func (lt *LatencyTracker) addSample(latency time.Duration) {
-	lt.mu.Lock()
-	defer lt.mu.Unlock()
-	
-	// Keep only last 1000 samples for memory efficiency
-	if len(lt.samples) >= 1000 {
-		lt.samples = lt.samples[1:]
-	}
-	
-	lt.samples = append(lt.samples, latency)
-}
-
-func (lt *LatencyTracker) getMetrics() LatencyMetrics {
-	lt.mu.RLock()
-	defer lt.mu.RUnlock()
-	
-	if len(lt.samples) == 0 {
-		return LatencyMetrics{}
-	}
-	
-	// Sort samples for percentile calculation
-	sorted := make([]time.Duration, len(lt.samples))
-	copy(sorted, lt.samples)
-	
-	// Simple bubble sort for small arrays
-	for i := 0; i < len(sorted)-1; i++ {
-		for j := 0; j < len(sorted)-i-1; j++ {
-			if sorted[j] > sorted[j+1] {
-				sorted[j], sorted[j+1] = sorted[j+1], sorted[j]
-			}
-		}
-	}
-	
-	var total time.Duration
-	var max time.Duration
-	
-	for _, sample := range sorted {
-		total += sample
-		if sample > max {
-			max = sample
-		}
-	}
-	
-	n := len(sorted)
-	
-	return LatencyMetrics{
-		P50: sorted[n*50/100],
-		P95: sorted[n*95/100],
-		P99: sorted[n*99/100],
-		Max: max,
-		Avg: total / time.Duration(n),
-	}
-}
-
-// StartHealthMonitoring starts background monitoring tasks
-func (hm *HealthManager) StartHealthMonitoring() {
-	// Clean expired cache entries every 5 minutes
-	go func() {
-		ticker := time.NewTicker(5 * time.Minute)
-		defer ticker.Stop()
-		
-		for range ticker.C {
-			hm.cacheManager.CleanExpired()
-		}
-	}()
-	
-	// Update circuit breaker budget thresholds every minute
-	go func() {
-		ticker := time.NewTicker(1 * time.Minute)
-		defer ticker.Stop()
-		
-		for range ticker.C {
-			for providerName := range DefaultProviders {
-				if usage, err := hm.providerManager.GetUsageStats(providerName); err == nil {
-					hm.circuitManager.CheckBudgetThreshold(providerName, usage.HealthPercent)
-				}
-			}
-		}
-	}()
-}
\ No newline at end of file
+package datasources
+
+import (
+	"encoding/json"
+	"fmt"
+	"sync"
+	"time"
+)
+
+// HealthManager aggregates health information from all datasource components
+type HealthManager struct {
+	providerManager *ProviderManager
+	cacheManager    *CacheManager
+	circuitManager  *CircuitManager
+	mu              sync.RWMutex
+
+	// Latency tracking
+	latencyStats map[string]*LatencyTracker
+}
+
+// LatencyTracker tracks latency percentiles for a provider
+type LatencyTracker struct {
+	samples []time.Duration
+	mu      sync.RWMutex
+}
+
+// HealthSnapshot represents the complete health state of all datasources
+type HealthSnapshot struct {
+	Timestamp     time.Time                 `json:"timestamp"`
+	OverallHealth string                    `json:"overall_health"`
+	Providers     map[string]ProviderHealth `json:"providers"`
+	Cache         CacheHealth               `json:"cache"`
+	Circuits      map[string]CircuitHealth  `json:"circuits"`
+	Summary       HealthSummary             `json:"summary"`
+}
+
+// ProviderHealth represents health metrics for a single provider
+type ProviderHealth struct {
+	Name          string         `json:"name"`
+	Status        string         `json:"status"`
+	RequestsToday int64          `json:"requests_today"`
+	RequestsMonth int64          `json:"requests_month"`
+	DailyQuota    int64          `json:"daily_quota"`
+	MonthlyQuota  int64          `json:"monthly_quota"`
+	HealthPercent float64        `json:"health_percent"`
+	WeightUsed    int            `json:"weight_used,omitempty"`
+	Latency       LatencyMetrics `json:"latency"`
+	Cost          float64        `json:"cost"`
+	LastRequest   time.Time      `json:"last_request"`
+	Circuit       string         `json:"circuit_state"`
+}
+
+// LatencyMetrics represents latency statistics
+type LatencyMetrics struct {
+	P50 time.Duration `json:"p50"`
+	P95 time.Duration `json:"p95"`
+	P99 time.Duration `json:"p99"`
+	Max time.Duration `json:"max"`
+	Avg time.Duration `json:"avg"`
+}
+
+// CacheHealth represents cache system health
+type CacheHealth struct {
+	Status         string  `json:"status"`
+	TotalEntries   int     `json:"total_entries"`
+	ActiveEntries  int     `json:"active_entries"`
+	ExpiredEntries int     `json:"expired_entries"`
+	HitRate        float64 `json:"hit_rate_percent"`
+}
+
+// CircuitHealth represents circuit breaker health
+type CircuitHealth struct {
+	Provider    string        `json:"provider"`
+	State       string        `json:"state"`
+	ErrorRate   float64       `json:"error_rate_percent"`
+	AvgLatency  time.Duration `json:"avg_latency"`
+	MaxLatency  time.Duration `json:"max_latency"`
+	LastFailure time.Time     `json:"last_failure,omitempty"`
+}
+
+// HealthSummary provides high-level health indicators
+type HealthSummary struct {
+	ProvidersHealthy  int           `json:"providers_healthy"`
+	ProvidersTotal    int           `json:"providers_total"`
+	CircuitsClosed    int           `json:"circuits_closed"`
+	CircuitsTotal     int           `json:"circuits_total"`
+	OverallLatencyP99 time.Duration `json:"overall_latency_p99"`
+	CacheHitRate      float64       `json:"cache_hit_rate"`
+	BudgetUtilization float64       `json:"budget_utilization_percent"`
+}
+
+// NewHealthManager creates a new health manager
+func NewHealthManager(pm *ProviderManager, cm *CacheManager, circm *CircuitManager) *HealthManager {
+	return &HealthManager{
+		providerManager: pm,
+		cacheManager:    cm,
+		circuitManager:  circm,
+		latencyStats:    make(map[string]*LatencyTracker),
+	}
+}
+
+// RecordLatency records a latency sample for a provider
+func (hm *HealthManager) RecordLatency(provider string, latency time.Duration) {
+	hm.mu.Lock()
+	defer hm.mu.Unlock()
+
+	tracker, exists := hm.latencyStats[provider]
+	if !exists {
+		tracker = &LatencyTracker{
+			samples: make([]time.Duration, 0, 1000), // Keep last 1000 samples
+		}
+		hm.latencyStats[provider] = tracker
+	}
+
+	tracker.addSample(latency)
+}
+
+// GetHealthSnapshot returns a complete health snapshot
+func (hm *HealthManager) GetHealthSnapshot() HealthSnapshot {
+	hm.mu.RLock()
+	defer hm.mu.RUnlock()
+
+	snapshot := HealthSnapshot{
+		Timestamp: time.Now(),
+		Providers: make(map[string]ProviderHealth),
+		Circuits:  make(map[string]CircuitHealth),
+	}
+
+	// Collect provider health
+	providersHealthy := 0
+	totalBudgetUsed := 0.0
+	totalProviders := 0
+
+	for providerName := range DefaultProviders {
+		totalProviders++
+
+		usage, err := hm.providerManager.GetUsageStats(providerName)
+		if err != nil {
+			continue
+		}
+
+		circuitState := hm.circuitManager.GetCircuitState(providerName)
+
+		providerHealth := ProviderHealth{
+			Name:          usage.Provider,
+			RequestsToday: usage.RequestsToday,
+			RequestsMonth: usage.RequestsMonth,
+			DailyQuota:    usage.DailyQuota,
+			MonthlyQuota:  usage.MonthlyQuota,
+			HealthPercent: usage.HealthPercent,
+			WeightUsed:    usage.WeightUsed,
+			Cost:          0.0, // Free APIs
+			LastRequest:   usage.LastRequest,
+			Circuit:       circuitState.String(),
+		}
+
+		// Calculate provider status
+		if circuitState == CircuitClosed && usage.HealthPercent > 50 {
+			providerHealth.Status = "healthy"
+			providersHealthy++
+		} else if circuitState == CircuitHalfOpen || (circuitState == CircuitClosed && usage.HealthPercent > 10) {
+			providerHealth.Status = "degraded"
+		} else {
+			providerHealth.Status = "unhealthy"
+		}
+
+		// Add latency metrics
+		if tracker, exists := hm.latencyStats[providerName]; exists {
+			providerHealth.Latency = tracker.getMetrics()
+		}
+
+		// Track budget utilization
+		if usage.HealthPercent < 100 {
+			totalBudgetUsed += (100.0 - usage.HealthPercent)
+		}
+
+		snapshot.Providers[providerName] = providerHealth
+	}
+
+	// Collect cache health
+	cacheStats := hm.cacheManager.Stats()
+	snapshot.Cache = CacheHealth{
+		Status:         "healthy", // Simple status for now
+		TotalEntries:   cacheStats.TotalEntries,
+		ActiveEntries:  cacheStats.ActiveEntries,
+		ExpiredEntries: cacheStats.ExpiredEntries,
+		HitRate:        85.0, // Placeholder - would need actual hit tracking
+	}
+
+	if cacheStats.ExpiredEntries > cacheStats.ActiveEntries {
+		snapshot.Cache.Status = "degraded"
+	}
+
+	// Collect circuit health
+	circuitStats := hm.circuitManager.GetAllStats()
+	circuitsClosed := 0
+	totalCircuits := len(circuitStats)
+	var maxLatencyP99 time.Duration
+
+	for provider, stats := range circuitStats {
+		circuitHealth := CircuitHealth{
+			Provider:   stats.Provider,
+			State:      stats.State,
+			ErrorRate:  stats.ErrorRate,
+			AvgLatency: stats.AvgLatency,
+			MaxLatency: stats.MaxLatency,
+		}
+
+		if !stats.LastFailTime.IsZero() {
+			circuitHealth.LastFailure = stats.LastFailTime
+		}
+
+		if stats.State == "closed" {
+			circuitsClosed++
+		}
+
+		if stats.MaxLatency > maxLatencyP99 {
+			maxLatencyP99 = stats.MaxLatency
+		}
+
+		snapshot.Circuits[provider] = circuitHealth
+	}
+
+	// Create summary
+	snapshot.Summary = HealthSummary{
+		ProvidersHealthy:  providersHealthy,
+		ProvidersTotal:    totalProviders,
+		CircuitsClosed:    circuitsClosed,
+		CircuitsTotal:     totalCircuits,
+		OverallLatencyP99: maxLatencyP99,
+		CacheHitRate:      snapshot.Cache.HitRate,
+		BudgetUtilization: totalBudgetUsed / float64(totalProviders),
+	}
+
+	// Determine overall health
+	snapshot.OverallHealth = hm.calculateOverallHealth(snapshot.Summary)
+
+	return snapshot
+}
+
+// GetHealthJSON returns the health snapshot as JSON string
+func (hm *HealthManager) GetHealthJSON() (string, error) {
+	snapshot := hm.GetHealthSnapshot()
+	data, err := json.MarshalIndent(snapshot, "", "  ")
+	if err != nil {
+		return "", fmt.Errorf("failed to marshal health snapshot: %w", err)
+	}
+	return string(data), nil
+}
+
+// GetHealthSummary returns a brief health summary
+func (hm *HealthManager) GetHealthSummary() string {
+	snapshot := hm.GetHealthSnapshot()
+
+	return fmt.Sprintf(
+		"Health: %s | Providers: %d/%d healthy | Circuits: %d/%d closed | Cache: %s (%.1f%% hit rate) | Latency P99: %v",
+		snapshot.OverallHealth,
+		snapshot.Summary.ProvidersHealthy,
+		snapshot.Summary.ProvidersTotal,
+		snapshot.Summary.CircuitsClosed,
+		snapshot.Summary.CircuitsTotal,
+		snapshot.Cache.Status,
+		snapshot.Cache.HitRate,
+		snapshot.Summary.OverallLatencyP99,
+	)
+}
+
+// IsHealthy returns true if the overall system is healthy
+func (hm *HealthManager) IsHealthy() bool {
+	snapshot := hm.GetHealthSnapshot()
+	return snapshot.OverallHealth == "healthy"
+}
+
+func (hm *HealthManager) calculateOverallHealth(summary HealthSummary) string {
+	// System is healthy if:
+	// - >50% providers are healthy
+	// - >50% circuits are closed
+	// - Cache hit rate >70%
+	// - P99 latency <10s
+
+	providerHealthRatio := float64(summary.ProvidersHealthy) / float64(summary.ProvidersTotal)
+	circuitHealthRatio := float64(summary.CircuitsClosed) / float64(summary.CircuitsTotal)
+
+	if providerHealthRatio >= 0.5 &&
+		circuitHealthRatio >= 0.5 &&
+		summary.CacheHitRate >= 70.0 &&
+		summary.OverallLatencyP99 < 10*time.Second {
+		return "healthy"
+	}
+
+	if providerHealthRatio >= 0.3 &&
+		circuitHealthRatio >= 0.3 &&
+		summary.CacheHitRate >= 50.0 {
+		return "degraded"
+	}
+
+	return "unhealthy"
+}
+
+func (lt *LatencyTracker) addSample(latency time.Duration) {
+	lt.mu.Lock()
+	defer lt.mu.Unlock()
+
+	// Keep only last 1000 samples for memory efficiency
+	if len(lt.samples) >= 1000 {
+		lt.samples = lt.samples[1:]
+	}
+
+	lt.samples = append(lt.samples, latency)
+}
+
+func (lt *LatencyTracker) getMetrics() LatencyMetrics {
+	lt.mu.RLock()
+	defer lt.mu.RUnlock()
+
+	if len(lt.samples) == 0 {
+		return LatencyMetrics{}
+	}
+
+	// Sort samples for percentile calculation
+	sorted := make([]time.Duration, len(lt.samples))
+	copy(sorted, lt.samples)
+
+	// Simple bubble sort for small arrays
+	for i := 0; i < len(sorted)-1; i++ {
+		for j := 0; j < len(sorted)-i-1; j++ {
+			if sorted[j] > sorted[j+1] {
+				sorted[j], sorted[j+1] = sorted[j+1], sorted[j]
+			}
+		}
+	}
+
+	var total time.Duration
+	var max time.Duration
+
+	for _, sample := range sorted {
+		total += sample
+		if sample > max {
+			max = sample
+		}
+	}
+
+	n := len(sorted)
+
+	return LatencyMetrics{
+		P50: sorted[n*50/100],
+		P95: sorted[n*95/100],
+		P99: sorted[n*99/100],
+		Max: max,
+		Avg: total / time.Duration(n),
+	}
+}
+
+// StartHealthMonitoring starts background monitoring tasks
+func (hm *HealthManager) StartHealthMonitoring() {
+	// Clean expired cache entries every 5 minutes
+	go func() {
+		ticker := time.NewTicker(5 * time.Minute)
+		defer ticker.Stop()
+
+		for range ticker.C {
+			hm.cacheManager.CleanExpired()
+		}
+	}()
+
+	// Update circuit breaker budget thresholds every minute
+	go func() {
+		ticker := time.NewTicker(1 * time.Minute)
+		defer ticker.Stop()
+
+		for range ticker.C {
+			for providerName := range DefaultProviders {
+				if usage, err := hm.providerManager.GetUsageStats(providerName); err == nil {
+					hm.circuitManager.CheckBudgetThreshold(providerName, usage.HealthPercent)
+				}
+			}
+		}
+	}()
+}
diff --git a/internal/datasources/health_test.go b/internal/datasources/health_test.go
index ff6d20b..fde1172 100644
--- a/internal/datasources/health_test.go
+++ b/internal/datasources/health_test.go
@@ -1,356 +1,356 @@
-package datasources
-
-import (
-	"encoding/json"
-	"strings"
-	"testing"
-	"time"
-)
-
-func TestHealthManager_GetHealthSnapshot(t *testing.T) {
-	pm := NewProviderManager()
-	cm := NewCacheManager()
-	circm := NewCircuitManager()
-	hm := NewHealthManager(pm, cm, circm)
-	
-	// Record some activity
-	pm.RecordRequest("binance", 1)
-	cm.Set("test-key", "market_data", "test data")
-	circm.RecordRequest("binance", true, 100*time.Millisecond, nil)
-	hm.RecordLatency("binance", 150*time.Millisecond)
-	
-	snapshot := hm.GetHealthSnapshot()
-	
-	// Verify snapshot structure
-	if snapshot.Timestamp.IsZero() {
-		t.Error("Expected non-zero timestamp")
-	}
-	
-	if len(snapshot.Providers) == 0 {
-		t.Error("Expected provider health data")
-	}
-	
-	if snapshot.Cache.Status == "" {
-		t.Error("Expected cache status")
-	}
-	
-	if len(snapshot.Circuits) == 0 {
-		t.Error("Expected circuit health data")
-	}
-	
-	// Verify provider health
-	if binanceHealth, exists := snapshot.Providers["binance"]; exists {
-		if binanceHealth.Name != "Binance" {
-			t.Errorf("Expected provider name Binance, got %s", binanceHealth.Name)
-		}
-		if binanceHealth.RequestsToday != 1 {
-			t.Errorf("Expected 1 request today, got %d", binanceHealth.RequestsToday)
-		}
-		if binanceHealth.Status == "" {
-			t.Error("Expected provider status")
-		}
-	} else {
-		t.Error("Expected binance provider health")
-	}
-}
-
-func TestHealthManager_RecordLatency(t *testing.T) {
-	pm := NewProviderManager()
-	cm := NewCacheManager()
-	circm := NewCircuitManager()
-	hm := NewHealthManager(pm, cm, circm)
-	
-	// Record latency samples
-	latencies := []time.Duration{
-		100 * time.Millisecond,
-		200 * time.Millisecond,
-		150 * time.Millisecond,
-		300 * time.Millisecond,
-		250 * time.Millisecond,
-	}
-	
-	for _, latency := range latencies {
-		hm.RecordLatency("binance", latency)
-	}
-	
-	snapshot := hm.GetHealthSnapshot()
-	binanceHealth := snapshot.Providers["binance"]
-	
-	// Verify latency metrics are populated
-	if binanceHealth.Latency.Avg == 0 {
-		t.Error("Expected non-zero average latency")
-	}
-	
-	if binanceHealth.Latency.Max != 300*time.Millisecond {
-		t.Errorf("Expected max latency 300ms, got %v", binanceHealth.Latency.Max)
-	}
-	
-	if binanceHealth.Latency.P99 == 0 {
-		t.Error("Expected non-zero P99 latency")
-	}
-}
-
-func TestLatencyTracker_Metrics(t *testing.T) {
-	tracker := &LatencyTracker{}
-	
-	// Add samples in known order
-	samples := []time.Duration{
-		100 * time.Millisecond, // P50 will be around here
-		200 * time.Millisecond,
-		300 * time.Millisecond,
-		400 * time.Millisecond,
-		500 * time.Millisecond, // P99 will be around here
-	}
-	
-	for _, sample := range samples {
-		tracker.addSample(sample)
-	}
-	
-	metrics := tracker.getMetrics()
-	
-	if metrics.Max != 500*time.Millisecond {
-		t.Errorf("Expected max 500ms, got %v", metrics.Max)
-	}
-	
-	expectedAvg := 300 * time.Millisecond
-	if metrics.Avg != expectedAvg {
-		t.Errorf("Expected avg %v, got %v", expectedAvg, metrics.Avg)
-	}
-	
-	// P50 should be the median (300ms)
-	if metrics.P50 != 300*time.Millisecond {
-		t.Errorf("Expected P50 300ms, got %v", metrics.P50)
-	}
-}
-
-func TestLatencyTracker_SampleLimit(t *testing.T) {
-	tracker := &LatencyTracker{}
-	
-	// Add more than 1000 samples
-	for i := 0; i < 1200; i++ {
-		tracker.addSample(time.Duration(i) * time.Millisecond)
-	}
-	
-	tracker.mu.RLock()
-	sampleCount := len(tracker.samples)
-	tracker.mu.RUnlock()
-	
-	if sampleCount > 1000 {
-		t.Errorf("Expected sample count <= 1000, got %d", sampleCount)
-	}
-}
-
-func TestHealthManager_GetHealthJSON(t *testing.T) {
-	pm := NewProviderManager()
-	cm := NewCacheManager()
-	circm := NewCircuitManager()
-	hm := NewHealthManager(pm, cm, circm)
-	
-	jsonStr, err := hm.GetHealthJSON()
-	if err != nil {
-		t.Fatalf("Failed to get health JSON: %v", err)
-	}
-	
-	// Verify it's valid JSON
-	var snapshot HealthSnapshot
-	err = json.Unmarshal([]byte(jsonStr), &snapshot)
-	if err != nil {
-		t.Fatalf("Failed to parse health JSON: %v", err)
-	}
-	
-	// Verify structure
-	if len(snapshot.Providers) == 0 {
-		t.Error("Expected providers in JSON")
-	}
-}
-
-func TestHealthManager_GetHealthSummary(t *testing.T) {
-	pm := NewProviderManager()
-	cm := NewCacheManager()
-	circm := NewCircuitManager()
-	hm := NewHealthManager(pm, cm, circm)
-	
-	summary := hm.GetHealthSummary()
-	
-	if summary == "" {
-		t.Error("Expected non-empty health summary")
-	}
-	
-	// Should contain key health indicators
-	expectedParts := []string{"Health:", "Providers:", "Circuits:", "Cache:", "Latency P99:"}
-	for _, part := range expectedParts {
-		if !strings.Contains(summary, part) {
-			t.Errorf("Expected summary to contain '%s', got: %s", part, summary)
-		}
-	}
-}
-
-func TestHealthManager_IsHealthy(t *testing.T) {
-	pm := NewProviderManager()
-	cm := NewCacheManager()
-	circm := NewCircuitManager()
-	hm := NewHealthManager(pm, cm, circm)
-	
-	// Should be healthy initially
-	if !hm.IsHealthy() {
-		t.Error("Expected system to be healthy initially")
-	}
-	
-	// Open all circuits to make it unhealthy
-	for provider := range DefaultProviders {
-		circm.ForceOpen(provider)
-	}
-	
-	// Should be unhealthy now
-	if hm.IsHealthy() {
-		t.Error("Expected system to be unhealthy after opening circuits")
-	}
-}
-
-func TestHealthManager_CalculateOverallHealth(t *testing.T) {
-	pm := NewProviderManager()
-	cm := NewCacheManager()
-	circm := NewCircuitManager()
-	hm := NewHealthManager(pm, cm, circm)
-	
-	testCases := []struct {
-		name     string
-		summary  HealthSummary
-		expected string
-	}{
-		{
-			name: "healthy system",
-			summary: HealthSummary{
-				ProvidersHealthy:  4,
-				ProvidersTotal:    5,
-				CircuitsClosed:    4,
-				CircuitsTotal:     5,
-				CacheHitRate:      85.0,
-				OverallLatencyP99: 2 * time.Second,
-			},
-			expected: "healthy",
-		},
-		{
-			name: "degraded system",
-			summary: HealthSummary{
-				ProvidersHealthy:  2,
-				ProvidersTotal:    5,
-				CircuitsClosed:    2,
-				CircuitsTotal:     5,
-				CacheHitRate:      60.0,
-				OverallLatencyP99: 5 * time.Second,
-			},
-			expected: "degraded",
-		},
-		{
-			name: "unhealthy system",
-			summary: HealthSummary{
-				ProvidersHealthy:  1,
-				ProvidersTotal:    5,
-				CircuitsClosed:    1,
-				CircuitsTotal:     5,
-				CacheHitRate:      30.0,
-				OverallLatencyP99: 15 * time.Second,
-			},
-			expected: "unhealthy",
-		},
-	}
-	
-	for _, tc := range testCases {
-		t.Run(tc.name, func(t *testing.T) {
-			result := hm.calculateOverallHealth(tc.summary)
-			if result != tc.expected {
-				t.Errorf("Expected %s, got %s", tc.expected, result)
-			}
-		})
-	}
-}
-
-func TestProviderHealth_Status(t *testing.T) {
-	pm := NewProviderManager()
-	cm := NewCacheManager()
-	circm := NewCircuitManager()
-	hm := NewHealthManager(pm, cm, circm)
-	
-	// Test healthy provider
-	snapshot := hm.GetHealthSnapshot()
-	binanceHealth := snapshot.Providers["binance"]
-	
-	// Should be healthy initially (circuit closed, high health %)
-	if binanceHealth.Status != "healthy" {
-		t.Errorf("Expected healthy status, got %s", binanceHealth.Status)
-	}
-	
-	// Open circuit to make it degraded/unhealthy
-	circm.ForceOpen("binance")
-	
-	snapshot = hm.GetHealthSnapshot()
-	binanceHealth = snapshot.Providers["binance"]
-	
-	// Should not be healthy now
-	if binanceHealth.Status == "healthy" {
-		t.Error("Expected unhealthy status after opening circuit")
-	}
-}
-
-func TestHealthManager_ConcurrentAccess(t *testing.T) {
-	pm := NewProviderManager()
-	cm := NewCacheManager()
-	circm := NewCircuitManager()
-	hm := NewHealthManager(pm, cm, circm)
-	
-	// Test concurrent access doesn't cause panics
-	done := make(chan bool)
-	
-	for i := 0; i < 10; i++ {
-		go func(id int) {
-			for j := 0; j < 50; j++ {
-				hm.RecordLatency("binance", time.Duration(j)*time.Millisecond)
-				hm.GetHealthSnapshot()
-				hm.GetHealthSummary()
-				hm.IsHealthy()
-			}
-			done <- true
-		}(i)
-	}
-	
-	// Wait for all goroutines to complete
-	for i := 0; i < 10; i++ {
-		<-done
-	}
-}
-
-func TestHealthSnapshot_Serialization(t *testing.T) {
-	pm := NewProviderManager()
-	cm := NewCacheManager()
-	circm := NewCircuitManager()
-	hm := NewHealthManager(pm, cm, circm)
-	
-	// Add some data
-	pm.RecordRequest("binance", 1)
-	hm.RecordLatency("binance", 100*time.Millisecond)
-	
-	snapshot := hm.GetHealthSnapshot()
-	
-	// Serialize to JSON
-	data, err := json.Marshal(snapshot)
-	if err != nil {
-		t.Fatalf("Failed to marshal snapshot: %v", err)
-	}
-	
-	// Deserialize back
-	var restored HealthSnapshot
-	err = json.Unmarshal(data, &restored)
-	if err != nil {
-		t.Fatalf("Failed to unmarshal snapshot: %v", err)
-	}
-	
-	// Verify key fields are preserved
-	if restored.OverallHealth != snapshot.OverallHealth {
-		t.Errorf("Expected overall health %s, got %s", snapshot.OverallHealth, restored.OverallHealth)
-	}
-	
-	if len(restored.Providers) != len(snapshot.Providers) {
-		t.Errorf("Expected %d providers, got %d", len(snapshot.Providers), len(restored.Providers))
-	}
-}
\ No newline at end of file
+package datasources
+
+import (
+	"encoding/json"
+	"strings"
+	"testing"
+	"time"
+)
+
+func TestHealthManager_GetHealthSnapshot(t *testing.T) {
+	pm := NewProviderManager()
+	cm := NewCacheManager()
+	circm := NewCircuitManager()
+	hm := NewHealthManager(pm, cm, circm)
+
+	// Record some activity
+	pm.RecordRequest("binance", 1)
+	cm.Set("test-key", "market_data", "test data")
+	circm.RecordRequest("binance", true, 100*time.Millisecond, nil)
+	hm.RecordLatency("binance", 150*time.Millisecond)
+
+	snapshot := hm.GetHealthSnapshot()
+
+	// Verify snapshot structure
+	if snapshot.Timestamp.IsZero() {
+		t.Error("Expected non-zero timestamp")
+	}
+
+	if len(snapshot.Providers) == 0 {
+		t.Error("Expected provider health data")
+	}
+
+	if snapshot.Cache.Status == "" {
+		t.Error("Expected cache status")
+	}
+
+	if len(snapshot.Circuits) == 0 {
+		t.Error("Expected circuit health data")
+	}
+
+	// Verify provider health
+	if binanceHealth, exists := snapshot.Providers["binance"]; exists {
+		if binanceHealth.Name != "Binance" {
+			t.Errorf("Expected provider name Binance, got %s", binanceHealth.Name)
+		}
+		if binanceHealth.RequestsToday != 1 {
+			t.Errorf("Expected 1 request today, got %d", binanceHealth.RequestsToday)
+		}
+		if binanceHealth.Status == "" {
+			t.Error("Expected provider status")
+		}
+	} else {
+		t.Error("Expected binance provider health")
+	}
+}
+
+func TestHealthManager_RecordLatency(t *testing.T) {
+	pm := NewProviderManager()
+	cm := NewCacheManager()
+	circm := NewCircuitManager()
+	hm := NewHealthManager(pm, cm, circm)
+
+	// Record latency samples
+	latencies := []time.Duration{
+		100 * time.Millisecond,
+		200 * time.Millisecond,
+		150 * time.Millisecond,
+		300 * time.Millisecond,
+		250 * time.Millisecond,
+	}
+
+	for _, latency := range latencies {
+		hm.RecordLatency("binance", latency)
+	}
+
+	snapshot := hm.GetHealthSnapshot()
+	binanceHealth := snapshot.Providers["binance"]
+
+	// Verify latency metrics are populated
+	if binanceHealth.Latency.Avg == 0 {
+		t.Error("Expected non-zero average latency")
+	}
+
+	if binanceHealth.Latency.Max != 300*time.Millisecond {
+		t.Errorf("Expected max latency 300ms, got %v", binanceHealth.Latency.Max)
+	}
+
+	if binanceHealth.Latency.P99 == 0 {
+		t.Error("Expected non-zero P99 latency")
+	}
+}
+
+func TestLatencyTracker_Metrics(t *testing.T) {
+	tracker := &LatencyTracker{}
+
+	// Add samples in known order
+	samples := []time.Duration{
+		100 * time.Millisecond, // P50 will be around here
+		200 * time.Millisecond,
+		300 * time.Millisecond,
+		400 * time.Millisecond,
+		500 * time.Millisecond, // P99 will be around here
+	}
+
+	for _, sample := range samples {
+		tracker.addSample(sample)
+	}
+
+	metrics := tracker.getMetrics()
+
+	if metrics.Max != 500*time.Millisecond {
+		t.Errorf("Expected max 500ms, got %v", metrics.Max)
+	}
+
+	expectedAvg := 300 * time.Millisecond
+	if metrics.Avg != expectedAvg {
+		t.Errorf("Expected avg %v, got %v", expectedAvg, metrics.Avg)
+	}
+
+	// P50 should be the median (300ms)
+	if metrics.P50 != 300*time.Millisecond {
+		t.Errorf("Expected P50 300ms, got %v", metrics.P50)
+	}
+}
+
+func TestLatencyTracker_SampleLimit(t *testing.T) {
+	tracker := &LatencyTracker{}
+
+	// Add more than 1000 samples
+	for i := 0; i < 1200; i++ {
+		tracker.addSample(time.Duration(i) * time.Millisecond)
+	}
+
+	tracker.mu.RLock()
+	sampleCount := len(tracker.samples)
+	tracker.mu.RUnlock()
+
+	if sampleCount > 1000 {
+		t.Errorf("Expected sample count <= 1000, got %d", sampleCount)
+	}
+}
+
+func TestHealthManager_GetHealthJSON(t *testing.T) {
+	pm := NewProviderManager()
+	cm := NewCacheManager()
+	circm := NewCircuitManager()
+	hm := NewHealthManager(pm, cm, circm)
+
+	jsonStr, err := hm.GetHealthJSON()
+	if err != nil {
+		t.Fatalf("Failed to get health JSON: %v", err)
+	}
+
+	// Verify it's valid JSON
+	var snapshot HealthSnapshot
+	err = json.Unmarshal([]byte(jsonStr), &snapshot)
+	if err != nil {
+		t.Fatalf("Failed to parse health JSON: %v", err)
+	}
+
+	// Verify structure
+	if len(snapshot.Providers) == 0 {
+		t.Error("Expected providers in JSON")
+	}
+}
+
+func TestHealthManager_GetHealthSummary(t *testing.T) {
+	pm := NewProviderManager()
+	cm := NewCacheManager()
+	circm := NewCircuitManager()
+	hm := NewHealthManager(pm, cm, circm)
+
+	summary := hm.GetHealthSummary()
+
+	if summary == "" {
+		t.Error("Expected non-empty health summary")
+	}
+
+	// Should contain key health indicators
+	expectedParts := []string{"Health:", "Providers:", "Circuits:", "Cache:", "Latency P99:"}
+	for _, part := range expectedParts {
+		if !strings.Contains(summary, part) {
+			t.Errorf("Expected summary to contain '%s', got: %s", part, summary)
+		}
+	}
+}
+
+func TestHealthManager_IsHealthy(t *testing.T) {
+	pm := NewProviderManager()
+	cm := NewCacheManager()
+	circm := NewCircuitManager()
+	hm := NewHealthManager(pm, cm, circm)
+
+	// Should be healthy initially
+	if !hm.IsHealthy() {
+		t.Error("Expected system to be healthy initially")
+	}
+
+	// Open all circuits to make it unhealthy
+	for provider := range DefaultProviders {
+		circm.ForceOpen(provider)
+	}
+
+	// Should be unhealthy now
+	if hm.IsHealthy() {
+		t.Error("Expected system to be unhealthy after opening circuits")
+	}
+}
+
+func TestHealthManager_CalculateOverallHealth(t *testing.T) {
+	pm := NewProviderManager()
+	cm := NewCacheManager()
+	circm := NewCircuitManager()
+	hm := NewHealthManager(pm, cm, circm)
+
+	testCases := []struct {
+		name     string
+		summary  HealthSummary
+		expected string
+	}{
+		{
+			name: "healthy system",
+			summary: HealthSummary{
+				ProvidersHealthy:  4,
+				ProvidersTotal:    5,
+				CircuitsClosed:    4,
+				CircuitsTotal:     5,
+				CacheHitRate:      85.0,
+				OverallLatencyP99: 2 * time.Second,
+			},
+			expected: "healthy",
+		},
+		{
+			name: "degraded system",
+			summary: HealthSummary{
+				ProvidersHealthy:  2,
+				ProvidersTotal:    5,
+				CircuitsClosed:    2,
+				CircuitsTotal:     5,
+				CacheHitRate:      60.0,
+				OverallLatencyP99: 5 * time.Second,
+			},
+			expected: "degraded",
+		},
+		{
+			name: "unhealthy system",
+			summary: HealthSummary{
+				ProvidersHealthy:  1,
+				ProvidersTotal:    5,
+				CircuitsClosed:    1,
+				CircuitsTotal:     5,
+				CacheHitRate:      30.0,
+				OverallLatencyP99: 15 * time.Second,
+			},
+			expected: "unhealthy",
+		},
+	}
+
+	for _, tc := range testCases {
+		t.Run(tc.name, func(t *testing.T) {
+			result := hm.calculateOverallHealth(tc.summary)
+			if result != tc.expected {
+				t.Errorf("Expected %s, got %s", tc.expected, result)
+			}
+		})
+	}
+}
+
+func TestProviderHealth_Status(t *testing.T) {
+	pm := NewProviderManager()
+	cm := NewCacheManager()
+	circm := NewCircuitManager()
+	hm := NewHealthManager(pm, cm, circm)
+
+	// Test healthy provider
+	snapshot := hm.GetHealthSnapshot()
+	binanceHealth := snapshot.Providers["binance"]
+
+	// Should be healthy initially (circuit closed, high health %)
+	if binanceHealth.Status != "healthy" {
+		t.Errorf("Expected healthy status, got %s", binanceHealth.Status)
+	}
+
+	// Open circuit to make it degraded/unhealthy
+	circm.ForceOpen("binance")
+
+	snapshot = hm.GetHealthSnapshot()
+	binanceHealth = snapshot.Providers["binance"]
+
+	// Should not be healthy now
+	if binanceHealth.Status == "healthy" {
+		t.Error("Expected unhealthy status after opening circuit")
+	}
+}
+
+func TestHealthManager_ConcurrentAccess(t *testing.T) {
+	pm := NewProviderManager()
+	cm := NewCacheManager()
+	circm := NewCircuitManager()
+	hm := NewHealthManager(pm, cm, circm)
+
+	// Test concurrent access doesn't cause panics
+	done := make(chan bool)
+
+	for i := 0; i < 10; i++ {
+		go func(id int) {
+			for j := 0; j < 50; j++ {
+				hm.RecordLatency("binance", time.Duration(j)*time.Millisecond)
+				hm.GetHealthSnapshot()
+				hm.GetHealthSummary()
+				hm.IsHealthy()
+			}
+			done <- true
+		}(i)
+	}
+
+	// Wait for all goroutines to complete
+	for i := 0; i < 10; i++ {
+		<-done
+	}
+}
+
+func TestHealthSnapshot_Serialization(t *testing.T) {
+	pm := NewProviderManager()
+	cm := NewCacheManager()
+	circm := NewCircuitManager()
+	hm := NewHealthManager(pm, cm, circm)
+
+	// Add some data
+	pm.RecordRequest("binance", 1)
+	hm.RecordLatency("binance", 100*time.Millisecond)
+
+	snapshot := hm.GetHealthSnapshot()
+
+	// Serialize to JSON
+	data, err := json.Marshal(snapshot)
+	if err != nil {
+		t.Fatalf("Failed to marshal snapshot: %v", err)
+	}
+
+	// Deserialize back
+	var restored HealthSnapshot
+	err = json.Unmarshal(data, &restored)
+	if err != nil {
+		t.Fatalf("Failed to unmarshal snapshot: %v", err)
+	}
+
+	// Verify key fields are preserved
+	if restored.OverallHealth != snapshot.OverallHealth {
+		t.Errorf("Expected overall health %s, got %s", snapshot.OverallHealth, restored.OverallHealth)
+	}
+
+	if len(restored.Providers) != len(snapshot.Providers) {
+		t.Errorf("Expected %d providers, got %d", len(snapshot.Providers), len(restored.Providers))
+	}
+}
diff --git a/internal/datasources/limits.go b/internal/datasources/limits.go
index 7ad21a4..f5e1ec3 100644
--- a/internal/datasources/limits.go
+++ b/internal/datasources/limits.go
@@ -1,299 +1,299 @@
-package datasources
-
-import (
-	"fmt"
-	"net/http"
-	"strconv"
-	"sync"
-	"time"
-)
-
-// ProviderLimits defines rate limit configuration for each data provider
-type ProviderLimits struct {
-	Name           string
-	RequestsPerSec int
-	BurstLimit     int
-	MonthlyQuota   int64
-	DailyQuota     int64
-	WeightBased    bool // for Binance-style weight system
-}
-
-// RateLimiter tracks usage and enforces limits for a provider
-type RateLimiter struct {
-	provider       ProviderLimits
-	tokens         int
-	lastRefill     time.Time
-	requestsToday  int64
-	requestsMonth  int64
-	weightUsed     int // for Binance weight tracking
-	mu             sync.RWMutex
-}
-
-// ProviderManager manages rate limiters for all data providers
-type ProviderManager struct {
-	limiters map[string]*RateLimiter
-	mu       sync.RWMutex
-}
-
-// Define provider configurations
-var DefaultProviders = map[string]ProviderLimits{
-	"binance": {
-		Name:           "Binance",
-		RequestsPerSec: 20,
-		BurstLimit:     40,
-		MonthlyQuota:   0, // no monthly limit
-		DailyQuota:     0, // no daily limit
-		WeightBased:    true,
-	},
-	"coingecko": {
-		Name:           "CoinGecko",
-		RequestsPerSec: 10,
-		BurstLimit:     20,
-		MonthlyQuota:   10000,
-		DailyQuota:     0,
-		WeightBased:    false,
-	},
-	"moralis": {
-		Name:           "Moralis",
-		RequestsPerSec: 25,
-		BurstLimit:     50,
-		MonthlyQuota:   0,
-		DailyQuota:     2000000, // 2M CU per day
-		WeightBased:    false,
-	},
-	"dexscreener": {
-		Name:           "DEXScreener",
-		RequestsPerSec: 30,
-		BurstLimit:     60,
-		MonthlyQuota:   0,
-		DailyQuota:     0,
-		WeightBased:    false,
-	},
-	"kraken": {
-		Name:           "Kraken",
-		RequestsPerSec: 1,
-		BurstLimit:     2,
-		MonthlyQuota:   0,
-		DailyQuota:     0,
-		WeightBased:    false,
-	},
-}
-
-// NewProviderManager creates a new provider manager with default configurations
-func NewProviderManager() *ProviderManager {
-	pm := &ProviderManager{
-		limiters: make(map[string]*RateLimiter),
-	}
-	
-	for name, limits := range DefaultProviders {
-		pm.limiters[name] = &RateLimiter{
-			provider:   limits,
-			tokens:     limits.BurstLimit,
-			lastRefill: time.Now(),
-		}
-	}
-	
-	return pm
-}
-
-// CanMakeRequest checks if a request can be made to the specified provider
-func (pm *ProviderManager) CanMakeRequest(providerName string) bool {
-	pm.mu.RLock()
-	limiter, exists := pm.limiters[providerName]
-	pm.mu.RUnlock()
-	
-	if !exists {
-		return false
-	}
-	
-	return limiter.canMakeRequest()
-}
-
-// RecordRequest records a request made to the specified provider
-func (pm *ProviderManager) RecordRequest(providerName string, weight int) error {
-	pm.mu.RLock()
-	limiter, exists := pm.limiters[providerName]
-	pm.mu.RUnlock()
-	
-	if !exists {
-		return fmt.Errorf("unknown provider: %s", providerName)
-	}
-	
-	return limiter.recordRequest(weight)
-}
-
-// ProcessResponseHeaders updates rate limit state from response headers
-func (pm *ProviderManager) ProcessResponseHeaders(providerName string, headers http.Header) {
-	pm.mu.RLock()
-	limiter, exists := pm.limiters[providerName]
-	pm.mu.RUnlock()
-	
-	if !exists {
-		return
-	}
-	
-	limiter.processHeaders(headers)
-}
-
-// GetUsageStats returns current usage statistics for a provider
-func (pm *ProviderManager) GetUsageStats(providerName string) (UsageStats, error) {
-	pm.mu.RLock()
-	limiter, exists := pm.limiters[providerName]
-	pm.mu.RUnlock()
-	
-	if !exists {
-		return UsageStats{}, fmt.Errorf("unknown provider: %s", providerName)
-	}
-	
-	return limiter.getUsageStats(), nil
-}
-
-// UsageStats represents current usage statistics for a provider
-type UsageStats struct {
-	Provider       string    `json:"provider"`
-	RequestsToday  int64     `json:"requests_today"`
-	RequestsMonth  int64     `json:"requests_month"`
-	WeightUsed     int       `json:"weight_used"`
-	TokensLeft     int       `json:"tokens_left"`
-	LastRequest    time.Time `json:"last_request"`
-	DailyQuota     int64     `json:"daily_quota"`
-	MonthlyQuota   int64     `json:"monthly_quota"`
-	HealthPercent  float64   `json:"health_percent"`
-}
-
-func (rl *RateLimiter) canMakeRequest() bool {
-	rl.mu.Lock()
-	defer rl.mu.Unlock()
-	
-	rl.refillTokens()
-	
-	// Check token bucket
-	if rl.tokens <= 0 {
-		return false
-	}
-	
-	// Check daily quota
-	if rl.provider.DailyQuota > 0 && rl.requestsToday >= rl.provider.DailyQuota {
-		return false
-	}
-	
-	// Check monthly quota
-	if rl.provider.MonthlyQuota > 0 && rl.requestsMonth >= rl.provider.MonthlyQuota {
-		return false
-	}
-	
-	return true
-}
-
-func (rl *RateLimiter) recordRequest(weight int) error {
-	rl.mu.Lock()
-	defer rl.mu.Unlock()
-	
-	if !rl.canMakeRequestUnsafe() {
-		return fmt.Errorf("rate limit exceeded for provider %s", rl.provider.Name)
-	}
-	
-	rl.tokens--
-	rl.requestsToday++
-	rl.requestsMonth++
-	
-	if rl.provider.WeightBased {
-		rl.weightUsed += weight
-	}
-	
-	return nil
-}
-
-func (rl *RateLimiter) canMakeRequestUnsafe() bool {
-	rl.refillTokens()
-	
-	if rl.tokens <= 0 {
-		return false
-	}
-	
-	if rl.provider.DailyQuota > 0 && rl.requestsToday >= rl.provider.DailyQuota {
-		return false
-	}
-	
-	if rl.provider.MonthlyQuota > 0 && rl.requestsMonth >= rl.provider.MonthlyQuota {
-		return false
-	}
-	
-	return true
-}
-
-func (rl *RateLimiter) refillTokens() {
-	now := time.Now()
-	elapsed := now.Sub(rl.lastRefill)
-	
-	if elapsed > time.Second {
-		tokensToAdd := int(elapsed.Seconds()) * rl.provider.RequestsPerSec
-		rl.tokens = minInt(rl.provider.BurstLimit, rl.tokens+tokensToAdd)
-		rl.lastRefill = now
-	}
-}
-
-func (rl *RateLimiter) processHeaders(headers http.Header) {
-	rl.mu.Lock()
-	defer rl.mu.Unlock()
-	
-	// Process Binance weight headers
-	if rl.provider.WeightBased {
-		if weight := headers.Get("X-MBX-USED-WEIGHT-1M"); weight != "" {
-			if w, err := strconv.Atoi(weight); err == nil {
-				rl.weightUsed = w
-			}
-		}
-	}
-}
-
-func (rl *RateLimiter) getUsageStats() UsageStats {
-	rl.mu.RLock()
-	defer rl.mu.RUnlock()
-	
-	healthPercent := 100.0
-	
-	// Calculate health based on quotas
-	if rl.provider.DailyQuota > 0 {
-		dailyUsage := float64(rl.requestsToday) / float64(rl.provider.DailyQuota) * 100
-		healthPercent = minFloat64(healthPercent, 100-dailyUsage)
-	}
-	
-	if rl.provider.MonthlyQuota > 0 {
-		monthlyUsage := float64(rl.requestsMonth) / float64(rl.provider.MonthlyQuota) * 100
-		healthPercent = minFloat64(healthPercent, 100-monthlyUsage)
-	}
-	
-	return UsageStats{
-		Provider:      rl.provider.Name,
-		RequestsToday: rl.requestsToday,
-		RequestsMonth: rl.requestsMonth,
-		WeightUsed:    rl.weightUsed,
-		TokensLeft:    rl.tokens,
-		LastRequest:   rl.lastRefill,
-		DailyQuota:    rl.provider.DailyQuota,
-		MonthlyQuota:  rl.provider.MonthlyQuota,
-		HealthPercent: maxFloat64(0, healthPercent),
-	}
-}
-
-func minInt(a, b int) int {
-	if a < b {
-		return a
-	}
-	return b
-}
-
-func minFloat64(a, b float64) float64 {
-	if a < b {
-		return a
-	}
-	return b
-}
-
-func maxFloat64(a, b float64) float64 {
-	if a > b {
-		return a
-	}
-	return b
-}
\ No newline at end of file
+package datasources
+
+import (
+	"fmt"
+	"net/http"
+	"strconv"
+	"sync"
+	"time"
+)
+
+// ProviderLimits defines rate limit configuration for each data provider
+type ProviderLimits struct {
+	Name           string
+	RequestsPerSec int
+	BurstLimit     int
+	MonthlyQuota   int64
+	DailyQuota     int64
+	WeightBased    bool // for Binance-style weight system
+}
+
+// RateLimiter tracks usage and enforces limits for a provider
+type RateLimiter struct {
+	provider      ProviderLimits
+	tokens        int
+	lastRefill    time.Time
+	requestsToday int64
+	requestsMonth int64
+	weightUsed    int // for Binance weight tracking
+	mu            sync.RWMutex
+}
+
+// ProviderManager manages rate limiters for all data providers
+type ProviderManager struct {
+	limiters map[string]*RateLimiter
+	mu       sync.RWMutex
+}
+
+// Define provider configurations
+var DefaultProviders = map[string]ProviderLimits{
+	"binance": {
+		Name:           "Binance",
+		RequestsPerSec: 20,
+		BurstLimit:     40,
+		MonthlyQuota:   0, // no monthly limit
+		DailyQuota:     0, // no daily limit
+		WeightBased:    true,
+	},
+	"coingecko": {
+		Name:           "CoinGecko",
+		RequestsPerSec: 10,
+		BurstLimit:     20,
+		MonthlyQuota:   10000,
+		DailyQuota:     0,
+		WeightBased:    false,
+	},
+	"moralis": {
+		Name:           "Moralis",
+		RequestsPerSec: 25,
+		BurstLimit:     50,
+		MonthlyQuota:   0,
+		DailyQuota:     2000000, // 2M CU per day
+		WeightBased:    false,
+	},
+	"dexscreener": {
+		Name:           "DEXScreener",
+		RequestsPerSec: 30,
+		BurstLimit:     60,
+		MonthlyQuota:   0,
+		DailyQuota:     0,
+		WeightBased:    false,
+	},
+	"kraken": {
+		Name:           "Kraken",
+		RequestsPerSec: 1,
+		BurstLimit:     2,
+		MonthlyQuota:   0,
+		DailyQuota:     0,
+		WeightBased:    false,
+	},
+}
+
+// NewProviderManager creates a new provider manager with default configurations
+func NewProviderManager() *ProviderManager {
+	pm := &ProviderManager{
+		limiters: make(map[string]*RateLimiter),
+	}
+
+	for name, limits := range DefaultProviders {
+		pm.limiters[name] = &RateLimiter{
+			provider:   limits,
+			tokens:     limits.BurstLimit,
+			lastRefill: time.Now(),
+		}
+	}
+
+	return pm
+}
+
+// CanMakeRequest checks if a request can be made to the specified provider
+func (pm *ProviderManager) CanMakeRequest(providerName string) bool {
+	pm.mu.RLock()
+	limiter, exists := pm.limiters[providerName]
+	pm.mu.RUnlock()
+
+	if !exists {
+		return false
+	}
+
+	return limiter.canMakeRequest()
+}
+
+// RecordRequest records a request made to the specified provider
+func (pm *ProviderManager) RecordRequest(providerName string, weight int) error {
+	pm.mu.RLock()
+	limiter, exists := pm.limiters[providerName]
+	pm.mu.RUnlock()
+
+	if !exists {
+		return fmt.Errorf("unknown provider: %s", providerName)
+	}
+
+	return limiter.recordRequest(weight)
+}
+
+// ProcessResponseHeaders updates rate limit state from response headers
+func (pm *ProviderManager) ProcessResponseHeaders(providerName string, headers http.Header) {
+	pm.mu.RLock()
+	limiter, exists := pm.limiters[providerName]
+	pm.mu.RUnlock()
+
+	if !exists {
+		return
+	}
+
+	limiter.processHeaders(headers)
+}
+
+// GetUsageStats returns current usage statistics for a provider
+func (pm *ProviderManager) GetUsageStats(providerName string) (UsageStats, error) {
+	pm.mu.RLock()
+	limiter, exists := pm.limiters[providerName]
+	pm.mu.RUnlock()
+
+	if !exists {
+		return UsageStats{}, fmt.Errorf("unknown provider: %s", providerName)
+	}
+
+	return limiter.getUsageStats(), nil
+}
+
+// UsageStats represents current usage statistics for a provider
+type UsageStats struct {
+	Provider      string    `json:"provider"`
+	RequestsToday int64     `json:"requests_today"`
+	RequestsMonth int64     `json:"requests_month"`
+	WeightUsed    int       `json:"weight_used"`
+	TokensLeft    int       `json:"tokens_left"`
+	LastRequest   time.Time `json:"last_request"`
+	DailyQuota    int64     `json:"daily_quota"`
+	MonthlyQuota  int64     `json:"monthly_quota"`
+	HealthPercent float64   `json:"health_percent"`
+}
+
+func (rl *RateLimiter) canMakeRequest() bool {
+	rl.mu.Lock()
+	defer rl.mu.Unlock()
+
+	rl.refillTokens()
+
+	// Check token bucket
+	if rl.tokens <= 0 {
+		return false
+	}
+
+	// Check daily quota
+	if rl.provider.DailyQuota > 0 && rl.requestsToday >= rl.provider.DailyQuota {
+		return false
+	}
+
+	// Check monthly quota
+	if rl.provider.MonthlyQuota > 0 && rl.requestsMonth >= rl.provider.MonthlyQuota {
+		return false
+	}
+
+	return true
+}
+
+func (rl *RateLimiter) recordRequest(weight int) error {
+	rl.mu.Lock()
+	defer rl.mu.Unlock()
+
+	if !rl.canMakeRequestUnsafe() {
+		return fmt.Errorf("rate limit exceeded for provider %s", rl.provider.Name)
+	}
+
+	rl.tokens--
+	rl.requestsToday++
+	rl.requestsMonth++
+
+	if rl.provider.WeightBased {
+		rl.weightUsed += weight
+	}
+
+	return nil
+}
+
+func (rl *RateLimiter) canMakeRequestUnsafe() bool {
+	rl.refillTokens()
+
+	if rl.tokens <= 0 {
+		return false
+	}
+
+	if rl.provider.DailyQuota > 0 && rl.requestsToday >= rl.provider.DailyQuota {
+		return false
+	}
+
+	if rl.provider.MonthlyQuota > 0 && rl.requestsMonth >= rl.provider.MonthlyQuota {
+		return false
+	}
+
+	return true
+}
+
+func (rl *RateLimiter) refillTokens() {
+	now := time.Now()
+	elapsed := now.Sub(rl.lastRefill)
+
+	if elapsed > time.Second {
+		tokensToAdd := int(elapsed.Seconds()) * rl.provider.RequestsPerSec
+		rl.tokens = minInt(rl.provider.BurstLimit, rl.tokens+tokensToAdd)
+		rl.lastRefill = now
+	}
+}
+
+func (rl *RateLimiter) processHeaders(headers http.Header) {
+	rl.mu.Lock()
+	defer rl.mu.Unlock()
+
+	// Process Binance weight headers
+	if rl.provider.WeightBased {
+		if weight := headers.Get("X-MBX-USED-WEIGHT-1M"); weight != "" {
+			if w, err := strconv.Atoi(weight); err == nil {
+				rl.weightUsed = w
+			}
+		}
+	}
+}
+
+func (rl *RateLimiter) getUsageStats() UsageStats {
+	rl.mu.RLock()
+	defer rl.mu.RUnlock()
+
+	healthPercent := 100.0
+
+	// Calculate health based on quotas
+	if rl.provider.DailyQuota > 0 {
+		dailyUsage := float64(rl.requestsToday) / float64(rl.provider.DailyQuota) * 100
+		healthPercent = minFloat64(healthPercent, 100-dailyUsage)
+	}
+
+	if rl.provider.MonthlyQuota > 0 {
+		monthlyUsage := float64(rl.requestsMonth) / float64(rl.provider.MonthlyQuota) * 100
+		healthPercent = minFloat64(healthPercent, 100-monthlyUsage)
+	}
+
+	return UsageStats{
+		Provider:      rl.provider.Name,
+		RequestsToday: rl.requestsToday,
+		RequestsMonth: rl.requestsMonth,
+		WeightUsed:    rl.weightUsed,
+		TokensLeft:    rl.tokens,
+		LastRequest:   rl.lastRefill,
+		DailyQuota:    rl.provider.DailyQuota,
+		MonthlyQuota:  rl.provider.MonthlyQuota,
+		HealthPercent: maxFloat64(0, healthPercent),
+	}
+}
+
+func minInt(a, b int) int {
+	if a < b {
+		return a
+	}
+	return b
+}
+
+func minFloat64(a, b float64) float64 {
+	if a < b {
+		return a
+	}
+	return b
+}
+
+func maxFloat64(a, b float64) float64 {
+	if a > b {
+		return a
+	}
+	return b
+}
diff --git a/internal/datasources/limits_test.go b/internal/datasources/limits_test.go
index b405ef9..d115cf9 100644
--- a/internal/datasources/limits_test.go
+++ b/internal/datasources/limits_test.go
@@ -1,187 +1,187 @@
-package datasources
-
-import (
-	"net/http"
-	"testing"
-	"time"
-)
-
-func TestProviderManager_CanMakeRequest(t *testing.T) {
-	pm := NewProviderManager()
-	
-	// Should be able to make requests initially
-	if !pm.CanMakeRequest("binance") {
-		t.Error("Should be able to make request to binance initially")
-	}
-	
-	// Unknown provider should return false
-	if pm.CanMakeRequest("unknown") {
-		t.Error("Should not be able to make request to unknown provider")
-	}
-}
-
-func TestProviderManager_RecordRequest(t *testing.T) {
-	pm := NewProviderManager()
-	
-	// Should be able to record requests
-	err := pm.RecordRequest("binance", 1)
-	if err != nil {
-		t.Errorf("Should be able to record request: %v", err)
-	}
-	
-	// Unknown provider should return error
-	err = pm.RecordRequest("unknown", 1)
-	if err == nil {
-		t.Error("Should return error for unknown provider")
-	}
-}
-
-func TestProviderManager_UsageStats(t *testing.T) {
-	pm := NewProviderManager()
-	
-	// Record some requests
-	pm.RecordRequest("binance", 1)
-	pm.RecordRequest("binance", 2)
-	
-	stats, err := pm.GetUsageStats("binance")
-	if err != nil {
-		t.Fatalf("Failed to get usage stats: %v", err)
-	}
-	
-	if stats.RequestsToday != 2 {
-		t.Errorf("Expected 2 requests today, got %d", stats.RequestsToday)
-	}
-	
-	if stats.WeightUsed != 3 {
-		t.Errorf("Expected weight used 3, got %d", stats.WeightUsed)
-	}
-}
-
-func TestRateLimiter_TokenBucket(t *testing.T) {
-	pm := NewProviderManager()
-	
-	// Exhaust tokens for binance (burst limit = 40)
-	for i := 0; i < 40; i++ {
-		if !pm.CanMakeRequest("binance") {
-			t.Fatalf("Should be able to make request %d", i+1)
-		}
-		err := pm.RecordRequest("binance", 1)
-		if err != nil {
-			t.Fatalf("Failed to record request %d: %v", i+1, err)
-		}
-	}
-	
-	// Should not be able to make more requests
-	if pm.CanMakeRequest("binance") {
-		t.Error("Should not be able to make request after exhausting tokens")
-	}
-}
-
-func TestRateLimiter_TokenRefill(t *testing.T) {
-	pm := NewProviderManager()
-	
-	// Get binance limiter and exhaust tokens
-	binanceLimiter := pm.limiters["binance"]
-	binanceLimiter.mu.Lock()
-	binanceLimiter.tokens = 0
-	binanceLimiter.lastRefill = time.Now().Add(-2 * time.Second) // 2 seconds ago
-	binanceLimiter.mu.Unlock()
-	
-	// Should be able to make requests again after time passes
-	if !pm.CanMakeRequest("binance") {
-		t.Error("Should be able to make request after token refill")
-	}
-}
-
-func TestRateLimiter_BinanceWeightHeaders(t *testing.T) {
-	pm := NewProviderManager()
-	
-	// Create mock response headers
-	headers := http.Header{}
-	headers.Set("X-MBX-USED-WEIGHT-1M", "150")
-	
-	pm.ProcessResponseHeaders("binance", headers)
-	
-	stats, _ := pm.GetUsageStats("binance")
-	if stats.WeightUsed != 150 {
-		t.Errorf("Expected weight used 150, got %d", stats.WeightUsed)
-	}
-}
-
-func TestRateLimiter_QuotaEnforcement(t *testing.T) {
-	pm := NewProviderManager()
-	
-	// Set a low daily quota for testing
-	coingeckoLimiter := pm.limiters["coingecko"]
-	coingeckoLimiter.mu.Lock()
-	coingeckoLimiter.provider.DailyQuota = 5
-	coingeckoLimiter.mu.Unlock()
-	
-	// Make requests up to quota
-	for i := 0; i < 5; i++ {
-		if !pm.CanMakeRequest("coingecko") {
-			t.Fatalf("Should be able to make request %d", i+1)
-		}
-		pm.RecordRequest("coingecko", 1)
-	}
-	
-	// Should not be able to make more requests
-	if pm.CanMakeRequest("coingecko") {
-		t.Error("Should not be able to make request after exceeding daily quota")
-	}
-}
-
-func TestUsageStats_HealthPercent(t *testing.T) {
-	pm := NewProviderManager()
-	
-	// Set quotas for testing
-	coingeckoLimiter := pm.limiters["coingecko"]
-	coingeckoLimiter.mu.Lock()
-	coingeckoLimiter.provider.DailyQuota = 100
-	coingeckoLimiter.requestsToday = 50 // 50% used
-	coingeckoLimiter.mu.Unlock()
-	
-	stats, _ := pm.GetUsageStats("coingecko")
-	expectedHealth := 50.0 // 100 - 50% used
-	if stats.HealthPercent != expectedHealth {
-		t.Errorf("Expected health percent %.1f, got %.1f", expectedHealth, stats.HealthPercent)
-	}
-}
-
-func TestProviderLimits_Configuration(t *testing.T) {
-	// Test that all providers have valid configurations
-	for name, limits := range DefaultProviders {
-		if limits.Name == "" {
-			t.Errorf("Provider %s has empty name", name)
-		}
-		if limits.RequestsPerSec <= 0 {
-			t.Errorf("Provider %s has invalid RequestsPerSec: %d", name, limits.RequestsPerSec)
-		}
-		if limits.BurstLimit <= 0 {
-			t.Errorf("Provider %s has invalid BurstLimit: %d", name, limits.BurstLimit)
-		}
-	}
-}
-
-func TestProviderManager_ConcurrentAccess(t *testing.T) {
-	pm := NewProviderManager()
-	
-	// Test concurrent access doesn't cause panics
-	done := make(chan bool)
-	
-	for i := 0; i < 10; i++ {
-		go func() {
-			for j := 0; j < 100; j++ {
-				pm.CanMakeRequest("binance")
-				pm.RecordRequest("binance", 1)
-				pm.GetUsageStats("binance")
-			}
-			done <- true
-		}()
-	}
-	
-	// Wait for all goroutines to complete
-	for i := 0; i < 10; i++ {
-		<-done
-	}
-}
\ No newline at end of file
+package datasources
+
+import (
+	"net/http"
+	"testing"
+	"time"
+)
+
+func TestProviderManager_CanMakeRequest(t *testing.T) {
+	pm := NewProviderManager()
+
+	// Should be able to make requests initially
+	if !pm.CanMakeRequest("binance") {
+		t.Error("Should be able to make request to binance initially")
+	}
+
+	// Unknown provider should return false
+	if pm.CanMakeRequest("unknown") {
+		t.Error("Should not be able to make request to unknown provider")
+	}
+}
+
+func TestProviderManager_RecordRequest(t *testing.T) {
+	pm := NewProviderManager()
+
+	// Should be able to record requests
+	err := pm.RecordRequest("binance", 1)
+	if err != nil {
+		t.Errorf("Should be able to record request: %v", err)
+	}
+
+	// Unknown provider should return error
+	err = pm.RecordRequest("unknown", 1)
+	if err == nil {
+		t.Error("Should return error for unknown provider")
+	}
+}
+
+func TestProviderManager_UsageStats(t *testing.T) {
+	pm := NewProviderManager()
+
+	// Record some requests
+	pm.RecordRequest("binance", 1)
+	pm.RecordRequest("binance", 2)
+
+	stats, err := pm.GetUsageStats("binance")
+	if err != nil {
+		t.Fatalf("Failed to get usage stats: %v", err)
+	}
+
+	if stats.RequestsToday != 2 {
+		t.Errorf("Expected 2 requests today, got %d", stats.RequestsToday)
+	}
+
+	if stats.WeightUsed != 3 {
+		t.Errorf("Expected weight used 3, got %d", stats.WeightUsed)
+	}
+}
+
+func TestRateLimiter_TokenBucket(t *testing.T) {
+	pm := NewProviderManager()
+
+	// Exhaust tokens for binance (burst limit = 40)
+	for i := 0; i < 40; i++ {
+		if !pm.CanMakeRequest("binance") {
+			t.Fatalf("Should be able to make request %d", i+1)
+		}
+		err := pm.RecordRequest("binance", 1)
+		if err != nil {
+			t.Fatalf("Failed to record request %d: %v", i+1, err)
+		}
+	}
+
+	// Should not be able to make more requests
+	if pm.CanMakeRequest("binance") {
+		t.Error("Should not be able to make request after exhausting tokens")
+	}
+}
+
+func TestRateLimiter_TokenRefill(t *testing.T) {
+	pm := NewProviderManager()
+
+	// Get binance limiter and exhaust tokens
+	binanceLimiter := pm.limiters["binance"]
+	binanceLimiter.mu.Lock()
+	binanceLimiter.tokens = 0
+	binanceLimiter.lastRefill = time.Now().Add(-2 * time.Second) // 2 seconds ago
+	binanceLimiter.mu.Unlock()
+
+	// Should be able to make requests again after time passes
+	if !pm.CanMakeRequest("binance") {
+		t.Error("Should be able to make request after token refill")
+	}
+}
+
+func TestRateLimiter_BinanceWeightHeaders(t *testing.T) {
+	pm := NewProviderManager()
+
+	// Create mock response headers
+	headers := http.Header{}
+	headers.Set("X-MBX-USED-WEIGHT-1M", "150")
+
+	pm.ProcessResponseHeaders("binance", headers)
+
+	stats, _ := pm.GetUsageStats("binance")
+	if stats.WeightUsed != 150 {
+		t.Errorf("Expected weight used 150, got %d", stats.WeightUsed)
+	}
+}
+
+func TestRateLimiter_QuotaEnforcement(t *testing.T) {
+	pm := NewProviderManager()
+
+	// Set a low daily quota for testing
+	coingeckoLimiter := pm.limiters["coingecko"]
+	coingeckoLimiter.mu.Lock()
+	coingeckoLimiter.provider.DailyQuota = 5
+	coingeckoLimiter.mu.Unlock()
+
+	// Make requests up to quota
+	for i := 0; i < 5; i++ {
+		if !pm.CanMakeRequest("coingecko") {
+			t.Fatalf("Should be able to make request %d", i+1)
+		}
+		pm.RecordRequest("coingecko", 1)
+	}
+
+	// Should not be able to make more requests
+	if pm.CanMakeRequest("coingecko") {
+		t.Error("Should not be able to make request after exceeding daily quota")
+	}
+}
+
+func TestUsageStats_HealthPercent(t *testing.T) {
+	pm := NewProviderManager()
+
+	// Set quotas for testing
+	coingeckoLimiter := pm.limiters["coingecko"]
+	coingeckoLimiter.mu.Lock()
+	coingeckoLimiter.provider.DailyQuota = 100
+	coingeckoLimiter.requestsToday = 50 // 50% used
+	coingeckoLimiter.mu.Unlock()
+
+	stats, _ := pm.GetUsageStats("coingecko")
+	expectedHealth := 50.0 // 100 - 50% used
+	if stats.HealthPercent != expectedHealth {
+		t.Errorf("Expected health percent %.1f, got %.1f", expectedHealth, stats.HealthPercent)
+	}
+}
+
+func TestProviderLimits_Configuration(t *testing.T) {
+	// Test that all providers have valid configurations
+	for name, limits := range DefaultProviders {
+		if limits.Name == "" {
+			t.Errorf("Provider %s has empty name", name)
+		}
+		if limits.RequestsPerSec <= 0 {
+			t.Errorf("Provider %s has invalid RequestsPerSec: %d", name, limits.RequestsPerSec)
+		}
+		if limits.BurstLimit <= 0 {
+			t.Errorf("Provider %s has invalid BurstLimit: %d", name, limits.BurstLimit)
+		}
+	}
+}
+
+func TestProviderManager_ConcurrentAccess(t *testing.T) {
+	pm := NewProviderManager()
+
+	// Test concurrent access doesn't cause panics
+	done := make(chan bool)
+
+	for i := 0; i < 10; i++ {
+		go func() {
+			for j := 0; j < 100; j++ {
+				pm.CanMakeRequest("binance")
+				pm.RecordRequest("binance", 1)
+				pm.GetUsageStats("binance")
+			}
+			done <- true
+		}()
+	}
+
+	// Wait for all goroutines to complete
+	for i := 0; i < 10; i++ {
+		<-done
+	}
+}
diff --git a/internal/domain/guards/evaluator.go b/internal/domain/guards/evaluator.go
index b04a612..6c051f5 100644
--- a/internal/domain/guards/evaluator.go
+++ b/internal/domain/guards/evaluator.go
@@ -1,9 +1,5 @@
 package guards
 
-import (
-	"fmt"
-)
-
 // GuardEvaluator orchestrates all guard evaluations with regime awareness
 type GuardEvaluator struct {
 	config      GuardConfig
diff --git a/internal/domain/guards/late_fill.go b/internal/domain/guards/late_fill.go
index e1bedec..daf3610 100644
--- a/internal/domain/guards/late_fill.go
+++ b/internal/domain/guards/late_fill.go
@@ -2,7 +2,6 @@ package guards
 
 import (
 	"fmt"
-	"time"
 )
 
 // EvaluateLateFillGuard checks execution timing with regime awareness
diff --git a/internal/domain/microstructure/checker.go b/internal/domain/microstructure/checker.go
index 7fed433..9d17207 100644
--- a/internal/domain/microstructure/checker.go
+++ b/internal/domain/microstructure/checker.go
@@ -138,8 +138,8 @@ func (c *Checker) CheckAssetEligibility(ctx context.Context, symbol string, venu
 
 		// For demo purposes, using mock VADR/ADV values
 		// In production, these would come from separate data sources
-		vadr := 2.1   // Mock VADR > 1.75
-		adv := 500000 // Mock ADV
+		vadr := 2.1     // Mock VADR > 1.75
+		adv := 500000.0 // Mock ADV
 
 		// Validate microstructure
 		metrics := c.ValidateOrderBook(ctx, orderBook, vadr, adv)
diff --git a/internal/domain/microstructure/proofs.go b/internal/domain/microstructure/proofs.go
index c03c726..7f96e9c 100644
--- a/internal/domain/microstructure/proofs.go
+++ b/internal/domain/microstructure/proofs.go
@@ -6,6 +6,7 @@ import (
 	"fmt"
 	"os"
 	"path/filepath"
+	"strings"
 	"time"
 
 	"cryptorun/internal/data/venue/types"
@@ -102,7 +103,7 @@ func (pg *ProofGenerator) ListAvailableProofs(ctx context.Context) ([]ProofSumma
 		if filepath.Base(path) == "master_proof.json" ||
 			(info != nil && !info.IsDir() && filepath.Ext(path) == ".json" &&
 				filepath.Base(filepath.Dir(path)) == "microstructure" &&
-				filepath.Contains(filepath.Base(path), "_master_proof.json")) {
+				strings.Contains(filepath.Base(path), "_master_proof.json")) {
 
 			// Extract date and symbol from path
 			parts := filepath.SplitList(path)
diff --git a/internal/exits/logic.go b/internal/exits/logic.go
index f7a5877..34d7371 100644
--- a/internal/exits/logic.go
+++ b/internal/exits/logic.go
@@ -1,346 +1,346 @@
-package exits
-
-import (
-	"context"
-	"fmt"
-	"time"
-)
-
-// ExitReason represents the reason for exit with precedence
-type ExitReason int
-
-const (
-	NoExit ExitReason = iota
-	HardStop           // Highest precedence: hard stop loss hit
-	VenueHealthCut     // Venue degradation (P99 latency, error rates)
-	TimeLimit          // 48-hour time limit reached
-	AccelerationReversal // Momentum acceleration has reversed
-	MomentumFade       // Momentum factor has faded significantly
-	TrailingStop       // Trailing stop activated
-	ProfitTarget       // Profit target achieved (lowest precedence)
-)
-
-func (er ExitReason) String() string {
-	switch er {
-	case NoExit:
-		return "no_exit"
-	case HardStop:
-		return "hard_stop"
-	case VenueHealthCut:
-		return "venue_health_cut"
-	case TimeLimit:
-		return "time_limit"
-	case AccelerationReversal:
-		return "acceleration_reversal"
-	case MomentumFade:
-		return "momentum_fade"
-	case TrailingStop:
-		return "trailing_stop"
-	case ProfitTarget:
-		return "profit_target"
-	default:
-		return "unknown"
-	}
-}
-
-// ExitResult contains the exit evaluation outcome
-type ExitResult struct {
-	Symbol           string     `json:"symbol"`
-	Timestamp        time.Time  `json:"timestamp"`
-	ShouldExit       bool       `json:"should_exit"`
-	ExitReason       ExitReason `json:"exit_reason"`
-	ReasonString     string     `json:"reason_string"`
-	TriggeredBy      string     `json:"triggered_by"`      // Specific trigger description
-	CurrentPrice     float64    `json:"current_price"`
-	EntryPrice       float64    `json:"entry_price"`
-	UnrealizedPnL    float64    `json:"unrealized_pnl"`    // % return
-	HoursHeld        float64    `json:"hours_held"`
-	EvaluationTimeMs int64      `json:"evaluation_time_ms"`
-}
-
-// ExitInputs contains all data required for exit evaluation
-type ExitInputs struct {
-	Symbol              string    `json:"symbol"`
-	EntryPrice          float64   `json:"entry_price"`
-	CurrentPrice        float64   `json:"current_price"`
-	EntryTime           time.Time `json:"entry_time"`
-	CurrentTime         time.Time `json:"current_time"`
-	
-	// Hard stop
-	HardStopPrice       float64   `json:"hard_stop_price"`       // Absolute stop loss price
-	
-	// Venue health metrics
-	VenueP99LatencyMs   int64     `json:"venue_p99_latency_ms"`  // P99 latency in ms
-	VenueErrorRate      float64   `json:"venue_error_rate"`      // Error rate %
-	VenueRejectRate     float64   `json:"venue_reject_rate"`     // Order reject rate %
-	
-	// Time-based exit
-	MaxHoldHours        float64   `json:"max_hold_hours"`        // Maximum hold time (default 48h)
-	
-	// Momentum metrics
-	MomentumScore       float64   `json:"momentum_score"`        // Current momentum score
-	EntryMomentumScore  float64   `json:"entry_momentum_score"`  // Momentum at entry
-	MomentumAccel       float64   `json:"momentum_accel"`        // Acceleration metric
-	EntryAccel          float64   `json:"entry_accel"`           // Acceleration at entry
-	
-	// Trailing stop
-	HighWaterMark       float64   `json:"high_water_mark"`       // Highest price since entry
-	TrailingStopPct     float64   `json:"trailing_stop_pct"`     // Trailing stop percentage
-	
-	// Profit targets
-	ProfitTarget1       float64   `json:"profit_target_1"`       // First profit target %
-	ProfitTarget2       float64   `json:"profit_target_2"`       // Second profit target %
-	ProfitTargetPrice1  float64   `json:"profit_target_price_1"` // Calculated price for target 1
-	ProfitTargetPrice2  float64   `json:"profit_target_price_2"` // Calculated price for target 2
-}
-
-// ExitEvaluator evaluates exit conditions with proper precedence
-type ExitEvaluator struct {
-	config *ExitConfig
-}
-
-// ExitConfig contains exit rule configuration
-type ExitConfig struct {
-	// Hard stop configuration
-	EnableHardStop bool `yaml:"enable_hard_stop"`
-
-	// Venue health thresholds
-	MaxVenueP99LatencyMs int64   `yaml:"max_venue_p99_latency_ms"` // 2000ms default
-	MaxVenueErrorRate    float64 `yaml:"max_venue_error_rate"`     // 3% default
-	MaxVenueRejectRate   float64 `yaml:"max_venue_reject_rate"`    // 5% default
-
-	// Time limit
-	DefaultMaxHoldHours float64 `yaml:"default_max_hold_hours"` // 48 hours default
-
-	// Momentum fade thresholds
-	MomentumFadeThreshold   float64 `yaml:"momentum_fade_threshold"`   // 30% fade from entry
-	AccelReversalThreshold  float64 `yaml:"accel_reversal_threshold"`  // 50% accel loss
-
-	// Trailing stop configuration
-	EnableTrailingStop   bool    `yaml:"enable_trailing_stop"`
-	DefaultTrailingPct   float64 `yaml:"default_trailing_pct"` // 5% default
-
-	// Profit targets
-	EnableProfitTargets  bool    `yaml:"enable_profit_targets"`
-	DefaultProfitTarget1 float64 `yaml:"default_profit_target_1"` // 15% default
-	DefaultProfitTarget2 float64 `yaml:"default_profit_target_2"` // 30% default
-}
-
-// DefaultExitConfig returns production-ready exit configuration
-func DefaultExitConfig() *ExitConfig {
-	return &ExitConfig{
-		EnableHardStop:          true,
-		MaxVenueP99LatencyMs:    2000,  // 2 seconds
-		MaxVenueErrorRate:       3.0,   // 3%
-		MaxVenueRejectRate:      5.0,   // 5%
-		DefaultMaxHoldHours:     48.0,  // 48 hours
-		MomentumFadeThreshold:   30.0,  // 30% fade
-		AccelReversalThreshold:  50.0,  // 50% accel loss
-		EnableTrailingStop:      true,
-		DefaultTrailingPct:      5.0,   // 5%
-		EnableProfitTargets:     true,
-		DefaultProfitTarget1:    15.0,  // 15%
-		DefaultProfitTarget2:    30.0,  // 30%
-	}
-}
-
-// NewExitEvaluator creates a new exit evaluator
-func NewExitEvaluator(config *ExitConfig) *ExitEvaluator {
-	if config == nil {
-		config = DefaultExitConfig()
-	}
-	return &ExitEvaluator{config: config}
-}
-
-// EvaluateExit performs exit evaluation with proper precedence
-func (ee *ExitEvaluator) EvaluateExit(ctx context.Context, inputs ExitInputs) (*ExitResult, error) {
-	startTime := time.Now()
-	
-	result := &ExitResult{
-		Symbol:        inputs.Symbol,
-		Timestamp:     inputs.CurrentTime,
-		ShouldExit:    false,
-		ExitReason:    NoExit,
-		CurrentPrice:  inputs.CurrentPrice,
-		EntryPrice:    inputs.EntryPrice,
-		UnrealizedPnL: ((inputs.CurrentPrice / inputs.EntryPrice) - 1.0) * 100,
-		HoursHeld:     inputs.CurrentTime.Sub(inputs.EntryTime).Hours(),
-	}
-
-	// Evaluate exits in precedence order (highest to lowest)
-	
-	// 1. Hard Stop (highest precedence)
-	if ee.config.EnableHardStop && ee.evaluateHardStop(inputs) {
-		result.ShouldExit = true
-		result.ExitReason = HardStop
-		result.ReasonString = "hard_stop"
-		result.TriggeredBy = fmt.Sprintf("Price %.4f ‚â§ stop %.4f", inputs.CurrentPrice, inputs.HardStopPrice)
-	}
-	
-	// 2. Venue Health Cut
-	if !result.ShouldExit && ee.evaluateVenueHealth(inputs) {
-		result.ShouldExit = true
-		result.ExitReason = VenueHealthCut
-		result.ReasonString = "venue_health_cut"
-		result.TriggeredBy = fmt.Sprintf("Venue degraded: P99=%dms, err=%.1f%%, rej=%.1f%%", 
-			inputs.VenueP99LatencyMs, inputs.VenueErrorRate, inputs.VenueRejectRate)
-	}
-	
-	// 3. Time Limit (48 hours)
-	if !result.ShouldExit && ee.evaluateTimeLimit(inputs) {
-		result.ShouldExit = true
-		result.ExitReason = TimeLimit
-		result.ReasonString = "time_limit"
-		result.TriggeredBy = fmt.Sprintf("Held %.1f hours ‚â• %.1f hour limit", 
-			result.HoursHeld, inputs.MaxHoldHours)
-	}
-	
-	// 4. Acceleration Reversal
-	if !result.ShouldExit && ee.evaluateAccelerationReversal(inputs) {
-		result.ShouldExit = true
-		result.ExitReason = AccelerationReversal
-		result.ReasonString = "acceleration_reversal"
-		accelChange := ((inputs.MomentumAccel / inputs.EntryAccel) - 1.0) * 100
-		result.TriggeredBy = fmt.Sprintf("Acceleration reversed: %.1f%% change", accelChange)
-	}
-	
-	// 5. Momentum Fade
-	if !result.ShouldExit && ee.evaluateMomentumFade(inputs) {
-		result.ShouldExit = true
-		result.ExitReason = MomentumFade
-		result.ReasonString = "momentum_fade"
-		momentumChange := ((inputs.MomentumScore / inputs.EntryMomentumScore) - 1.0) * 100
-		result.TriggeredBy = fmt.Sprintf("Momentum faded: %.1f%% decline", momentumChange)
-	}
-	
-	// 6. Trailing Stop
-	if !result.ShouldExit && ee.config.EnableTrailingStop && ee.evaluateTrailingStop(inputs) {
-		result.ShouldExit = true
-		result.ExitReason = TrailingStop
-		result.ReasonString = "trailing_stop"
-		stopPrice := inputs.HighWaterMark * (1.0 - inputs.TrailingStopPct/100.0)
-		result.TriggeredBy = fmt.Sprintf("Trailing stop: %.4f < %.4f (%.1f%% from HWM %.4f)", 
-			inputs.CurrentPrice, stopPrice, inputs.TrailingStopPct, inputs.HighWaterMark)
-	}
-	
-	// 7. Profit Targets (lowest precedence)
-	if !result.ShouldExit && ee.config.EnableProfitTargets && ee.evaluateProfitTargets(inputs) {
-		result.ShouldExit = true
-		result.ExitReason = ProfitTarget
-		result.ReasonString = "profit_target"
-		
-		if inputs.CurrentPrice >= inputs.ProfitTargetPrice2 {
-			result.TriggeredBy = fmt.Sprintf("Profit target 2 hit: %.4f ‚â• %.4f (+%.1f%%)", 
-				inputs.CurrentPrice, inputs.ProfitTargetPrice2, inputs.ProfitTarget2)
-		} else if inputs.CurrentPrice >= inputs.ProfitTargetPrice1 {
-			result.TriggeredBy = fmt.Sprintf("Profit target 1 hit: %.4f ‚â• %.4f (+%.1f%%)", 
-				inputs.CurrentPrice, inputs.ProfitTargetPrice1, inputs.ProfitTarget1)
-		}
-	}
-
-	result.EvaluationTimeMs = time.Since(startTime).Milliseconds()
-	return result, nil
-}
-
-// evaluateHardStop checks if hard stop loss is triggered
-func (ee *ExitEvaluator) evaluateHardStop(inputs ExitInputs) bool {
-	return inputs.HardStopPrice > 0 && inputs.CurrentPrice <= inputs.HardStopPrice
-}
-
-// evaluateVenueHealth checks if venue performance is degraded
-func (ee *ExitEvaluator) evaluateVenueHealth(inputs ExitInputs) bool {
-	return inputs.VenueP99LatencyMs > ee.config.MaxVenueP99LatencyMs ||
-		   inputs.VenueErrorRate > ee.config.MaxVenueErrorRate ||
-		   inputs.VenueRejectRate > ee.config.MaxVenueRejectRate
-}
-
-// evaluateTimeLimit checks if maximum hold time is reached
-func (ee *ExitEvaluator) evaluateTimeLimit(inputs ExitInputs) bool {
-	maxHours := inputs.MaxHoldHours
-	if maxHours <= 0 {
-		maxHours = ee.config.DefaultMaxHoldHours
-	}
-	
-	hoursHeld := inputs.CurrentTime.Sub(inputs.EntryTime).Hours()
-	return hoursHeld >= maxHours
-}
-
-// evaluateAccelerationReversal checks if momentum acceleration has reversed significantly
-func (ee *ExitEvaluator) evaluateAccelerationReversal(inputs ExitInputs) bool {
-	if inputs.EntryAccel <= 0 {
-		return false // Can't evaluate without entry acceleration
-	}
-	
-	accelChange := ((inputs.MomentumAccel / inputs.EntryAccel) - 1.0) * 100
-	return accelChange <= -ee.config.AccelReversalThreshold
-}
-
-// evaluateMomentumFade checks if momentum has faded significantly from entry
-func (ee *ExitEvaluator) evaluateMomentumFade(inputs ExitInputs) bool {
-	if inputs.EntryMomentumScore <= 0 {
-		return false // Can't evaluate without entry momentum
-	}
-	
-	momentumChange := ((inputs.MomentumScore / inputs.EntryMomentumScore) - 1.0) * 100
-	return momentumChange <= -ee.config.MomentumFadeThreshold
-}
-
-// evaluateTrailingStop checks if trailing stop is triggered
-func (ee *ExitEvaluator) evaluateTrailingStop(inputs ExitInputs) bool {
-	if inputs.HighWaterMark <= inputs.EntryPrice {
-		return false // No profits to protect yet
-	}
-	
-	trailingPct := inputs.TrailingStopPct
-	if trailingPct <= 0 {
-		trailingPct = ee.config.DefaultTrailingPct
-	}
-	
-	stopPrice := inputs.HighWaterMark * (1.0 - trailingPct/100.0)
-	return inputs.CurrentPrice <= stopPrice
-}
-
-// evaluateProfitTargets checks if profit targets are hit
-func (ee *ExitEvaluator) evaluateProfitTargets(inputs ExitInputs) bool {
-	// Check target 2 first (higher target)
-	if inputs.ProfitTargetPrice2 > 0 && inputs.CurrentPrice >= inputs.ProfitTargetPrice2 {
-		return true
-	}
-	
-	// Check target 1
-	if inputs.ProfitTargetPrice1 > 0 && inputs.CurrentPrice >= inputs.ProfitTargetPrice1 {
-		return true
-	}
-	
-	return false
-}
-
-// GetExitSummary returns a concise exit evaluation summary
-func (er *ExitResult) GetExitSummary() string {
-	if er.ShouldExit {
-		return fmt.Sprintf("üö™ EXIT TRIGGERED ‚Äî %s: %s (%.1f%% PnL after %.1fh)", 
-			er.Symbol, er.ExitReason.String(), er.UnrealizedPnL, er.HoursHeld)
-	} else {
-		return fmt.Sprintf("‚úÖ HOLD POSITION ‚Äî %s: %.1f%% PnL after %.1fh", 
-			er.Symbol, er.UnrealizedPnL, er.HoursHeld)
-	}
-}
-
-// GetDetailedExitReport returns comprehensive exit analysis
-func (er *ExitResult) GetDetailedExitReport() string {
-	report := fmt.Sprintf("Exit Evaluation: %s\n", er.Symbol)
-	report += fmt.Sprintf("Decision: %s | PnL: %.1f%% | Held: %.1fh\n", 
-		map[bool]string{true: "EXIT üö™", false: "HOLD ‚úÖ"}[er.ShouldExit],
-		er.UnrealizedPnL, er.HoursHeld)
-	report += fmt.Sprintf("Price: %.4f (entry: %.4f)\n\n", er.CurrentPrice, er.EntryPrice)
-
-	if er.ShouldExit {
-		report += fmt.Sprintf("Exit Reason: %s\n", er.ExitReason.String())
-		report += fmt.Sprintf("Trigger: %s\n", er.TriggeredBy)
-	} else {
-		report += "Position remains open - no exit conditions met\n"
-	}
-
-	report += fmt.Sprintf("\nEvaluation completed in %dms", er.EvaluationTimeMs)
-	return report
-}
\ No newline at end of file
+package exits
+
+import (
+	"context"
+	"fmt"
+	"time"
+)
+
+// ExitReason represents the reason for exit with precedence
+type ExitReason int
+
+const (
+	NoExit               ExitReason = iota
+	HardStop                        // Highest precedence: hard stop loss hit
+	VenueHealthCut                  // Venue degradation (P99 latency, error rates)
+	TimeLimit                       // 48-hour time limit reached
+	AccelerationReversal            // Momentum acceleration has reversed
+	MomentumFade                    // Momentum factor has faded significantly
+	TrailingStop                    // Trailing stop activated
+	ProfitTarget                    // Profit target achieved (lowest precedence)
+)
+
+func (er ExitReason) String() string {
+	switch er {
+	case NoExit:
+		return "no_exit"
+	case HardStop:
+		return "hard_stop"
+	case VenueHealthCut:
+		return "venue_health_cut"
+	case TimeLimit:
+		return "time_limit"
+	case AccelerationReversal:
+		return "acceleration_reversal"
+	case MomentumFade:
+		return "momentum_fade"
+	case TrailingStop:
+		return "trailing_stop"
+	case ProfitTarget:
+		return "profit_target"
+	default:
+		return "unknown"
+	}
+}
+
+// ExitResult contains the exit evaluation outcome
+type ExitResult struct {
+	Symbol           string     `json:"symbol"`
+	Timestamp        time.Time  `json:"timestamp"`
+	ShouldExit       bool       `json:"should_exit"`
+	ExitReason       ExitReason `json:"exit_reason"`
+	ReasonString     string     `json:"reason_string"`
+	TriggeredBy      string     `json:"triggered_by"` // Specific trigger description
+	CurrentPrice     float64    `json:"current_price"`
+	EntryPrice       float64    `json:"entry_price"`
+	UnrealizedPnL    float64    `json:"unrealized_pnl"` // % return
+	HoursHeld        float64    `json:"hours_held"`
+	EvaluationTimeMs int64      `json:"evaluation_time_ms"`
+}
+
+// ExitInputs contains all data required for exit evaluation
+type ExitInputs struct {
+	Symbol       string    `json:"symbol"`
+	EntryPrice   float64   `json:"entry_price"`
+	CurrentPrice float64   `json:"current_price"`
+	EntryTime    time.Time `json:"entry_time"`
+	CurrentTime  time.Time `json:"current_time"`
+
+	// Hard stop
+	HardStopPrice float64 `json:"hard_stop_price"` // Absolute stop loss price
+
+	// Venue health metrics
+	VenueP99LatencyMs int64   `json:"venue_p99_latency_ms"` // P99 latency in ms
+	VenueErrorRate    float64 `json:"venue_error_rate"`     // Error rate %
+	VenueRejectRate   float64 `json:"venue_reject_rate"`    // Order reject rate %
+
+	// Time-based exit
+	MaxHoldHours float64 `json:"max_hold_hours"` // Maximum hold time (default 48h)
+
+	// Momentum metrics
+	MomentumScore      float64 `json:"momentum_score"`       // Current momentum score
+	EntryMomentumScore float64 `json:"entry_momentum_score"` // Momentum at entry
+	MomentumAccel      float64 `json:"momentum_accel"`       // Acceleration metric
+	EntryAccel         float64 `json:"entry_accel"`          // Acceleration at entry
+
+	// Trailing stop
+	HighWaterMark   float64 `json:"high_water_mark"`   // Highest price since entry
+	TrailingStopPct float64 `json:"trailing_stop_pct"` // Trailing stop percentage
+
+	// Profit targets
+	ProfitTarget1      float64 `json:"profit_target_1"`       // First profit target %
+	ProfitTarget2      float64 `json:"profit_target_2"`       // Second profit target %
+	ProfitTargetPrice1 float64 `json:"profit_target_price_1"` // Calculated price for target 1
+	ProfitTargetPrice2 float64 `json:"profit_target_price_2"` // Calculated price for target 2
+}
+
+// ExitEvaluator evaluates exit conditions with proper precedence
+type ExitEvaluator struct {
+	config *ExitConfig
+}
+
+// ExitConfig contains exit rule configuration
+type ExitConfig struct {
+	// Hard stop configuration
+	EnableHardStop bool `yaml:"enable_hard_stop"`
+
+	// Venue health thresholds
+	MaxVenueP99LatencyMs int64   `yaml:"max_venue_p99_latency_ms"` // 2000ms default
+	MaxVenueErrorRate    float64 `yaml:"max_venue_error_rate"`     // 3% default
+	MaxVenueRejectRate   float64 `yaml:"max_venue_reject_rate"`    // 5% default
+
+	// Time limit
+	DefaultMaxHoldHours float64 `yaml:"default_max_hold_hours"` // 48 hours default
+
+	// Momentum fade thresholds
+	MomentumFadeThreshold  float64 `yaml:"momentum_fade_threshold"`  // 30% fade from entry
+	AccelReversalThreshold float64 `yaml:"accel_reversal_threshold"` // 50% accel loss
+
+	// Trailing stop configuration
+	EnableTrailingStop bool    `yaml:"enable_trailing_stop"`
+	DefaultTrailingPct float64 `yaml:"default_trailing_pct"` // 5% default
+
+	// Profit targets
+	EnableProfitTargets  bool    `yaml:"enable_profit_targets"`
+	DefaultProfitTarget1 float64 `yaml:"default_profit_target_1"` // 15% default
+	DefaultProfitTarget2 float64 `yaml:"default_profit_target_2"` // 30% default
+}
+
+// DefaultExitConfig returns production-ready exit configuration
+func DefaultExitConfig() *ExitConfig {
+	return &ExitConfig{
+		EnableHardStop:         true,
+		MaxVenueP99LatencyMs:   2000, // 2 seconds
+		MaxVenueErrorRate:      3.0,  // 3%
+		MaxVenueRejectRate:     5.0,  // 5%
+		DefaultMaxHoldHours:    48.0, // 48 hours
+		MomentumFadeThreshold:  30.0, // 30% fade
+		AccelReversalThreshold: 50.0, // 50% accel loss
+		EnableTrailingStop:     true,
+		DefaultTrailingPct:     5.0, // 5%
+		EnableProfitTargets:    true,
+		DefaultProfitTarget1:   15.0, // 15%
+		DefaultProfitTarget2:   30.0, // 30%
+	}
+}
+
+// NewExitEvaluator creates a new exit evaluator
+func NewExitEvaluator(config *ExitConfig) *ExitEvaluator {
+	if config == nil {
+		config = DefaultExitConfig()
+	}
+	return &ExitEvaluator{config: config}
+}
+
+// EvaluateExit performs exit evaluation with proper precedence
+func (ee *ExitEvaluator) EvaluateExit(ctx context.Context, inputs ExitInputs) (*ExitResult, error) {
+	startTime := time.Now()
+
+	result := &ExitResult{
+		Symbol:        inputs.Symbol,
+		Timestamp:     inputs.CurrentTime,
+		ShouldExit:    false,
+		ExitReason:    NoExit,
+		CurrentPrice:  inputs.CurrentPrice,
+		EntryPrice:    inputs.EntryPrice,
+		UnrealizedPnL: ((inputs.CurrentPrice / inputs.EntryPrice) - 1.0) * 100,
+		HoursHeld:     inputs.CurrentTime.Sub(inputs.EntryTime).Hours(),
+	}
+
+	// Evaluate exits in precedence order (highest to lowest)
+
+	// 1. Hard Stop (highest precedence)
+	if ee.config.EnableHardStop && ee.evaluateHardStop(inputs) {
+		result.ShouldExit = true
+		result.ExitReason = HardStop
+		result.ReasonString = "hard_stop"
+		result.TriggeredBy = fmt.Sprintf("Price %.4f ‚â§ stop %.4f", inputs.CurrentPrice, inputs.HardStopPrice)
+	}
+
+	// 2. Venue Health Cut
+	if !result.ShouldExit && ee.evaluateVenueHealth(inputs) {
+		result.ShouldExit = true
+		result.ExitReason = VenueHealthCut
+		result.ReasonString = "venue_health_cut"
+		result.TriggeredBy = fmt.Sprintf("Venue degraded: P99=%dms, err=%.1f%%, rej=%.1f%%",
+			inputs.VenueP99LatencyMs, inputs.VenueErrorRate, inputs.VenueRejectRate)
+	}
+
+	// 3. Time Limit (48 hours)
+	if !result.ShouldExit && ee.evaluateTimeLimit(inputs) {
+		result.ShouldExit = true
+		result.ExitReason = TimeLimit
+		result.ReasonString = "time_limit"
+		result.TriggeredBy = fmt.Sprintf("Held %.1f hours ‚â• %.1f hour limit",
+			result.HoursHeld, inputs.MaxHoldHours)
+	}
+
+	// 4. Acceleration Reversal
+	if !result.ShouldExit && ee.evaluateAccelerationReversal(inputs) {
+		result.ShouldExit = true
+		result.ExitReason = AccelerationReversal
+		result.ReasonString = "acceleration_reversal"
+		accelChange := ((inputs.MomentumAccel / inputs.EntryAccel) - 1.0) * 100
+		result.TriggeredBy = fmt.Sprintf("Acceleration reversed: %.1f%% change", accelChange)
+	}
+
+	// 5. Momentum Fade
+	if !result.ShouldExit && ee.evaluateMomentumFade(inputs) {
+		result.ShouldExit = true
+		result.ExitReason = MomentumFade
+		result.ReasonString = "momentum_fade"
+		momentumChange := ((inputs.MomentumScore / inputs.EntryMomentumScore) - 1.0) * 100
+		result.TriggeredBy = fmt.Sprintf("Momentum faded: %.1f%% decline", momentumChange)
+	}
+
+	// 6. Trailing Stop
+	if !result.ShouldExit && ee.config.EnableTrailingStop && ee.evaluateTrailingStop(inputs) {
+		result.ShouldExit = true
+		result.ExitReason = TrailingStop
+		result.ReasonString = "trailing_stop"
+		stopPrice := inputs.HighWaterMark * (1.0 - inputs.TrailingStopPct/100.0)
+		result.TriggeredBy = fmt.Sprintf("Trailing stop: %.4f < %.4f (%.1f%% from HWM %.4f)",
+			inputs.CurrentPrice, stopPrice, inputs.TrailingStopPct, inputs.HighWaterMark)
+	}
+
+	// 7. Profit Targets (lowest precedence)
+	if !result.ShouldExit && ee.config.EnableProfitTargets && ee.evaluateProfitTargets(inputs) {
+		result.ShouldExit = true
+		result.ExitReason = ProfitTarget
+		result.ReasonString = "profit_target"
+
+		if inputs.CurrentPrice >= inputs.ProfitTargetPrice2 {
+			result.TriggeredBy = fmt.Sprintf("Profit target 2 hit: %.4f ‚â• %.4f (+%.1f%%)",
+				inputs.CurrentPrice, inputs.ProfitTargetPrice2, inputs.ProfitTarget2)
+		} else if inputs.CurrentPrice >= inputs.ProfitTargetPrice1 {
+			result.TriggeredBy = fmt.Sprintf("Profit target 1 hit: %.4f ‚â• %.4f (+%.1f%%)",
+				inputs.CurrentPrice, inputs.ProfitTargetPrice1, inputs.ProfitTarget1)
+		}
+	}
+
+	result.EvaluationTimeMs = time.Since(startTime).Milliseconds()
+	return result, nil
+}
+
+// evaluateHardStop checks if hard stop loss is triggered
+func (ee *ExitEvaluator) evaluateHardStop(inputs ExitInputs) bool {
+	return inputs.HardStopPrice > 0 && inputs.CurrentPrice <= inputs.HardStopPrice
+}
+
+// evaluateVenueHealth checks if venue performance is degraded
+func (ee *ExitEvaluator) evaluateVenueHealth(inputs ExitInputs) bool {
+	return inputs.VenueP99LatencyMs > ee.config.MaxVenueP99LatencyMs ||
+		inputs.VenueErrorRate > ee.config.MaxVenueErrorRate ||
+		inputs.VenueRejectRate > ee.config.MaxVenueRejectRate
+}
+
+// evaluateTimeLimit checks if maximum hold time is reached
+func (ee *ExitEvaluator) evaluateTimeLimit(inputs ExitInputs) bool {
+	maxHours := inputs.MaxHoldHours
+	if maxHours <= 0 {
+		maxHours = ee.config.DefaultMaxHoldHours
+	}
+
+	hoursHeld := inputs.CurrentTime.Sub(inputs.EntryTime).Hours()
+	return hoursHeld >= maxHours
+}
+
+// evaluateAccelerationReversal checks if momentum acceleration has reversed significantly
+func (ee *ExitEvaluator) evaluateAccelerationReversal(inputs ExitInputs) bool {
+	if inputs.EntryAccel <= 0 {
+		return false // Can't evaluate without entry acceleration
+	}
+
+	accelChange := ((inputs.MomentumAccel / inputs.EntryAccel) - 1.0) * 100
+	return accelChange <= -ee.config.AccelReversalThreshold
+}
+
+// evaluateMomentumFade checks if momentum has faded significantly from entry
+func (ee *ExitEvaluator) evaluateMomentumFade(inputs ExitInputs) bool {
+	if inputs.EntryMomentumScore <= 0 {
+		return false // Can't evaluate without entry momentum
+	}
+
+	momentumChange := ((inputs.MomentumScore / inputs.EntryMomentumScore) - 1.0) * 100
+	return momentumChange <= -ee.config.MomentumFadeThreshold
+}
+
+// evaluateTrailingStop checks if trailing stop is triggered
+func (ee *ExitEvaluator) evaluateTrailingStop(inputs ExitInputs) bool {
+	if inputs.HighWaterMark <= inputs.EntryPrice {
+		return false // No profits to protect yet
+	}
+
+	trailingPct := inputs.TrailingStopPct
+	if trailingPct <= 0 {
+		trailingPct = ee.config.DefaultTrailingPct
+	}
+
+	stopPrice := inputs.HighWaterMark * (1.0 - trailingPct/100.0)
+	return inputs.CurrentPrice <= stopPrice
+}
+
+// evaluateProfitTargets checks if profit targets are hit
+func (ee *ExitEvaluator) evaluateProfitTargets(inputs ExitInputs) bool {
+	// Check target 2 first (higher target)
+	if inputs.ProfitTargetPrice2 > 0 && inputs.CurrentPrice >= inputs.ProfitTargetPrice2 {
+		return true
+	}
+
+	// Check target 1
+	if inputs.ProfitTargetPrice1 > 0 && inputs.CurrentPrice >= inputs.ProfitTargetPrice1 {
+		return true
+	}
+
+	return false
+}
+
+// GetExitSummary returns a concise exit evaluation summary
+func (er *ExitResult) GetExitSummary() string {
+	if er.ShouldExit {
+		return fmt.Sprintf("üö™ EXIT TRIGGERED ‚Äî %s: %s (%.1f%% PnL after %.1fh)",
+			er.Symbol, er.ExitReason.String(), er.UnrealizedPnL, er.HoursHeld)
+	} else {
+		return fmt.Sprintf("‚úÖ HOLD POSITION ‚Äî %s: %.1f%% PnL after %.1fh",
+			er.Symbol, er.UnrealizedPnL, er.HoursHeld)
+	}
+}
+
+// GetDetailedExitReport returns comprehensive exit analysis
+func (er *ExitResult) GetDetailedExitReport() string {
+	report := fmt.Sprintf("Exit Evaluation: %s\n", er.Symbol)
+	report += fmt.Sprintf("Decision: %s | PnL: %.1f%% | Held: %.1fh\n",
+		map[bool]string{true: "EXIT üö™", false: "HOLD ‚úÖ"}[er.ShouldExit],
+		er.UnrealizedPnL, er.HoursHeld)
+	report += fmt.Sprintf("Price: %.4f (entry: %.4f)\n\n", er.CurrentPrice, er.EntryPrice)
+
+	if er.ShouldExit {
+		report += fmt.Sprintf("Exit Reason: %s\n", er.ExitReason.String())
+		report += fmt.Sprintf("Trigger: %s\n", er.TriggeredBy)
+	} else {
+		report += "Position remains open - no exit conditions met\n"
+	}
+
+	report += fmt.Sprintf("\nEvaluation completed in %dms", er.EvaluationTimeMs)
+	return report
+}
diff --git a/internal/gates/api.go b/internal/gates/api.go
index e1b064b..b8d1cd8 100644
--- a/internal/gates/api.go
+++ b/internal/gates/api.go
@@ -80,11 +80,11 @@ type GateReport struct {
 }
 
 // EvaluateEntry performs comprehensive entry evaluation (hard gates + guards)
-func (go *GateOrchestrator) EvaluateEntry(ctx context.Context, inputs EntryEvaluationInputs) (*EntryEvaluationResult, error) {
+func (g *GateOrchestrator) EvaluateEntry(ctx context.Context, inputs EntryEvaluationInputs) (*EntryEvaluationResult, error) {
 	startTime := time.Now()
 	
 	// 1. Evaluate hard gates (Score, VADR, Funding divergence)
-	hardGateResult, err := go.entryEvaluator.EvaluateEntry(ctx, inputs.Symbol, inputs.CompositeScore, inputs.PriceChange24h)
+	hardGateResult, err := g.entryEvaluator.EvaluateEntry(ctx, inputs.Symbol, inputs.CompositeScore, inputs.PriceChange24h)
 	if err != nil {
 		return nil, fmt.Errorf("hard gate evaluation failed: %w", err)
 	}
@@ -103,7 +103,7 @@ func (go *GateOrchestrator) EvaluateEntry(ctx context.Context, inputs EntryEvalu
 		Timestamp:           inputs.Timestamp,
 	}
 	
-	guardResult, err := go.guardMetrics.EvaluateGuards(ctx, guardInputs)
+	guardResult, err := g.guardMetrics.EvaluateGuards(ctx, guardInputs)
 	if err != nil {
 		return nil, fmt.Errorf("guard evaluation failed: %w", err)
 	}
@@ -144,14 +144,14 @@ func (go *GateOrchestrator) EvaluateEntry(ctx context.Context, inputs EntryEvalu
 }
 
 // EvaluateExit performs exit evaluation for existing positions
-func (go *GateOrchestrator) EvaluateExit(ctx context.Context, inputs exits.ExitInputs) (*exits.ExitResult, error) {
-	return go.exitEvaluator.EvaluateExit(ctx, inputs)
+func (g *GateOrchestrator) EvaluateExit(ctx context.Context, inputs exits.ExitInputs) (*exits.ExitResult, error) {
+	return g.exitEvaluator.EvaluateExit(ctx, inputs)
 }
 
 // GenerateGateReport creates a stable JSON report for deterministic testing
-func (go *GateOrchestrator) GenerateGateReport(ctx context.Context, entryInputs EntryEvaluationInputs, exitInputs *exits.ExitInputs) (*GateReport, error) {
+func (g *GateOrchestrator) GenerateGateReport(ctx context.Context, entryInputs EntryEvaluationInputs, exitInputs *exits.ExitInputs) (*GateReport, error) {
 	// Evaluate entry
-	entryResult, err := go.EvaluateEntry(ctx, entryInputs)
+	entryResult, err := g.EvaluateEntry(ctx, entryInputs)
 	if err != nil {
 		return nil, fmt.Errorf("entry evaluation failed: %w", err)
 	}
@@ -189,7 +189,7 @@ func (go *GateOrchestrator) GenerateGateReport(ctx context.Context, entryInputs
 	
 	// Add exit evaluation if position data provided
 	if exitInputs != nil {
-		exitResult, err := go.EvaluateExit(ctx, *exitInputs)
+		exitResult, err := g.EvaluateExit(ctx, *exitInputs)
 		if err == nil {
 			report.ExitEvaluation = map[string]interface{}{
 				"should_exit":    exitResult.ShouldExit,
diff --git a/internal/gates/metrics.go b/internal/gates/metrics.go
index c59a53e..0c57443 100644
--- a/internal/gates/metrics.go
+++ b/internal/gates/metrics.go
@@ -1,237 +1,237 @@
-package gates
-
-import (
-	"context"
-	"fmt"
-	"time"
-)
-
-// GuardMetrics evaluates timing and fatigue guards for entry signals
-type GuardMetrics struct {
-	config *GuardConfig
-}
-
-// GuardConfig contains thresholds for signal freshness, fatigue, and timing guards
-type GuardConfig struct {
-	// Freshness guard: signal must be recent
-	MaxBarsAge int `yaml:"max_bars_age"` // ‚â§2 bars old
-
-	// Fatigue guard: prevent overextended entries
-	FatiguePrice24hThreshold float64 `yaml:"fatigue_price_24h_threshold"` // >12% = fatigued
-	FatigueRSI4hThreshold    float64 `yaml:"fatigue_rsi_4h_threshold"`    // >70 = overbought
-
-	// Proximity guard: price must be close to trigger
-	ProximityATRMultiple float64 `yaml:"proximity_atr_multiple"` // 1.2√ó ATR maximum
-
-	// Late-fill guard: prevent stale execution
-	MaxSecondsSinceTrigger int64 `yaml:"max_seconds_since_trigger"` // <30s
-}
-
-// DefaultGuardConfig returns production-ready guard configuration
-func DefaultGuardConfig() *GuardConfig {
-	return &GuardConfig{
-		MaxBarsAge:               2,    // ‚â§2 bars old
-		FatiguePrice24hThreshold: 12.0, // >12% in 24h
-		FatigueRSI4hThreshold:    70.0, // RSI >70
-		ProximityATRMultiple:     1.2,  // 1.2√ó ATR
-		MaxSecondsSinceTrigger:   30,   // <30 seconds
-	}
-}
-
-// NewGuardMetrics creates a new guard metrics evaluator
-func NewGuardMetrics(config *GuardConfig) *GuardMetrics {
-	if config == nil {
-		config = DefaultGuardConfig()
-	}
-	return &GuardMetrics{config: config}
-}
-
-// GuardInputs contains all data needed for guard evaluation
-type GuardInputs struct {
-	Symbol              string    `json:"symbol"`
-	BarsSinceSignal     int       `json:"bars_since_signal"`      // How many bars ago was signal
-	PriceChange24h      float64   `json:"price_change_24h"`       // 24h price change %
-	RSI4h               float64   `json:"rsi_4h"`                 // 4-hour RSI
-	DistanceFromTrigger float64   `json:"distance_from_trigger"`  // Distance from trigger price
-	ATR1h               float64   `json:"atr_1h"`                 // 1-hour ATR for proximity
-	SecondsSinceTrigger int64     `json:"seconds_since_trigger"`  // Time since trigger bar close
-	HasPullback         bool      `json:"has_pullback"`           // Recent pullback detected
-	HasAcceleration     bool      `json:"has_acceleration"`       // Renewed acceleration
-	Timestamp           time.Time `json:"timestamp"`
-}
-
-// GuardResult contains the evaluation results for all guards
-type GuardResult struct {
-	Symbol         string                `json:"symbol"`
-	Timestamp      time.Time             `json:"timestamp"`
-	AllPassed      bool                  `json:"all_passed"`
-	GuardChecks    map[string]*GateCheck `json:"guard_checks"`
-	FailureReasons []string              `json:"failure_reasons"`
-	PassedGuards   []string              `json:"passed_guards"`
-}
-
-// EvaluateGuards performs comprehensive guard evaluation
-func (gm *GuardMetrics) EvaluateGuards(ctx context.Context, inputs GuardInputs) (*GuardResult, error) {
-	result := &GuardResult{
-		Symbol:         inputs.Symbol,
-		Timestamp:      inputs.Timestamp,
-		GuardChecks:    make(map[string]*GateCheck),
-		FailureReasons: []string{},
-		PassedGuards:   []string{},
-	}
-
-	// Guard 1: Freshness - signal must be ‚â§2 bars old
-	freshnessCheck := gm.evaluateFreshnessGuard(inputs)
-	result.GuardChecks["freshness"] = freshnessCheck
-	if freshnessCheck.Passed {
-		result.PassedGuards = append(result.PassedGuards, "freshness")
-	} else {
-		result.FailureReasons = append(result.FailureReasons,
-			fmt.Sprintf("Stale signal: %d bars old (max %d)", inputs.BarsSinceSignal, gm.config.MaxBarsAge))
-	}
-
-	// Guard 2: Fatigue - prevent overextended entries
-	fatigueCheck := gm.evaluateFatigueGuard(inputs)
-	result.GuardChecks["fatigue"] = fatigueCheck
-	if fatigueCheck.Passed {
-		result.PassedGuards = append(result.PassedGuards, "fatigue")
-	} else {
-		result.FailureReasons = append(result.FailureReasons,
-			fmt.Sprintf("Fatigue detected: 24h %.1f%% + RSI %.1f (no pullback/accel)", 
-				inputs.PriceChange24h, inputs.RSI4h))
-	}
-
-	// Guard 3: Proximity - price must be close to trigger
-	proximityCheck := gm.evaluateProximityGuard(inputs)
-	result.GuardChecks["proximity"] = proximityCheck
-	if proximityCheck.Passed {
-		result.PassedGuards = append(result.PassedGuards, "proximity")
-	} else {
-		maxDist := inputs.ATR1h * gm.config.ProximityATRMultiple
-		result.FailureReasons = append(result.FailureReasons,
-			fmt.Sprintf("Price too far: %.4f > %.4f (%.1fx ATR)", 
-				inputs.DistanceFromTrigger, maxDist, gm.config.ProximityATRMultiple))
-	}
-
-	// Guard 4: Late-fill - execution must be timely
-	lateFillCheck := gm.evaluateLateFillGuard(inputs)
-	result.GuardChecks["late_fill"] = lateFillCheck
-	if lateFillCheck.Passed {
-		result.PassedGuards = append(result.PassedGuards, "late_fill")
-	} else {
-		result.FailureReasons = append(result.FailureReasons,
-			fmt.Sprintf("Late fill: %ds since trigger (max %ds)", 
-				inputs.SecondsSinceTrigger, gm.config.MaxSecondsSinceTrigger))
-	}
-
-	// Overall result
-	result.AllPassed = len(result.FailureReasons) == 0
-
-	return result, nil
-}
-
-// evaluateFreshnessGuard ensures signal is recent (‚â§2 bars old)
-func (gm *GuardMetrics) evaluateFreshnessGuard(inputs GuardInputs) *GateCheck {
-	passed := inputs.BarsSinceSignal <= gm.config.MaxBarsAge
-	
-	return &GateCheck{
-		Name:        "freshness",
-		Passed:      passed,
-		Value:       inputs.BarsSinceSignal,
-		Threshold:   gm.config.MaxBarsAge,
-		Description: fmt.Sprintf("Signal age %d bars ‚â§ %d bars", inputs.BarsSinceSignal, gm.config.MaxBarsAge),
-	}
-}
-
-// evaluateFatigueGuard checks for overextension (24h >12% AND RSI4h >70 UNLESS pullback/acceleration)
-func (gm *GuardMetrics) evaluateFatigueGuard(inputs GuardInputs) *GateCheck {
-	// Check if conditions indicate fatigue
-	isOverextended := inputs.PriceChange24h > gm.config.FatiguePrice24hThreshold && 
-		inputs.RSI4h > gm.config.FatigueRSI4hThreshold
-
-	// Check for exceptions that override fatigue
-	hasException := inputs.HasPullback || inputs.HasAcceleration
-
-	passed := !isOverextended || hasException
-
-	description := fmt.Sprintf("24h %.1f%% (thresh %.1f%%), RSI %.1f (thresh %.1f)", 
-		inputs.PriceChange24h, gm.config.FatiguePrice24hThreshold,
-		inputs.RSI4h, gm.config.FatigueRSI4hThreshold)
-
-	if isOverextended && hasException {
-		description += " - EXCEPTION: pullback/acceleration detected"
-	}
-
-	return &GateCheck{
-		Name:        "fatigue",
-		Passed:      passed,
-		Value:       inputs.PriceChange24h,
-		Threshold:   gm.config.FatiguePrice24hThreshold,
-		Description: description,
-	}
-}
-
-// evaluateProximityGuard ensures price is close to trigger (‚â§1.2√ó ATR)
-func (gm *GuardMetrics) evaluateProximityGuard(inputs GuardInputs) *GateCheck {
-	maxDistance := inputs.ATR1h * gm.config.ProximityATRMultiple
-	passed := inputs.DistanceFromTrigger <= maxDistance
-
-	return &GateCheck{
-		Name:        "proximity",
-		Passed:      passed,
-		Value:       inputs.DistanceFromTrigger,
-		Threshold:   maxDistance,
-		Description: fmt.Sprintf("Distance %.4f ‚â§ %.4f (%.1fx ATR)", 
-			inputs.DistanceFromTrigger, maxDistance, gm.config.ProximityATRMultiple),
-	}
-}
-
-// evaluateLateFillGuard prevents late fills (<30s since trigger)
-func (gm *GuardMetrics) evaluateLateFillGuard(inputs GuardInputs) *GateCheck {
-	passed := inputs.SecondsSinceTrigger < gm.config.MaxSecondsSinceTrigger
-
-	return &GateCheck{
-		Name:        "late_fill",
-		Passed:      passed,
-		Value:       inputs.SecondsSinceTrigger,
-		Threshold:   gm.config.MaxSecondsSinceTrigger,
-		Description: fmt.Sprintf("Fill timing %ds < %ds", inputs.SecondsSinceTrigger, gm.config.MaxSecondsSinceTrigger),
-	}
-}
-
-// GetGuardSummary returns a concise guard evaluation summary
-func (gr *GuardResult) GetGuardSummary() string {
-	if gr.AllPassed {
-		return fmt.Sprintf("‚úÖ GUARDS CLEARED ‚Äî %s (%d/%d passed)", 
-			gr.Symbol, len(gr.PassedGuards), len(gr.GuardChecks))
-	} else {
-		return fmt.Sprintf("‚ùå GUARD BLOCKED ‚Äî %s (%d failures)", 
-			gr.Symbol, len(gr.FailureReasons))
-	}
-}
-
-// GetDetailedGuardReport returns comprehensive guard analysis
-func (gr *GuardResult) GetDetailedGuardReport() string {
-	report := fmt.Sprintf("Guard Evaluation: %s\n", gr.Symbol)
-	report += fmt.Sprintf("Overall: %s\n\n", 
-		map[bool]string{true: "PASS ‚úÖ", false: "FAIL ‚ùå"}[gr.AllPassed])
-
-	// Show all guard results
-	guardOrder := []string{"freshness", "fatigue", "proximity", "late_fill"}
-	
-	for _, guardName := range guardOrder {
-		if check, exists := gr.GuardChecks[guardName]; exists {
-			status := map[bool]string{true: "‚úÖ", false: "‚ùå"}[check.Passed]
-			report += fmt.Sprintf("%s %s: %s\n", status, check.Name, check.Description)
-		}
-	}
-
-	if len(gr.FailureReasons) > 0 {
-		report += fmt.Sprintf("\nFailure Details:\n")
-		for i, reason := range gr.FailureReasons {
-			report += fmt.Sprintf("  %d. %s\n", i+1, reason)
-		}
-	}
-
-	return report
-}
\ No newline at end of file
+package gates
+
+import (
+	"context"
+	"fmt"
+	"time"
+)
+
+// GuardMetrics evaluates timing and fatigue guards for entry signals
+type GuardMetrics struct {
+	config *GuardConfig
+}
+
+// GuardConfig contains thresholds for signal freshness, fatigue, and timing guards
+type GuardConfig struct {
+	// Freshness guard: signal must be recent
+	MaxBarsAge int `yaml:"max_bars_age"` // ‚â§2 bars old
+
+	// Fatigue guard: prevent overextended entries
+	FatiguePrice24hThreshold float64 `yaml:"fatigue_price_24h_threshold"` // >12% = fatigued
+	FatigueRSI4hThreshold    float64 `yaml:"fatigue_rsi_4h_threshold"`    // >70 = overbought
+
+	// Proximity guard: price must be close to trigger
+	ProximityATRMultiple float64 `yaml:"proximity_atr_multiple"` // 1.2√ó ATR maximum
+
+	// Late-fill guard: prevent stale execution
+	MaxSecondsSinceTrigger int64 `yaml:"max_seconds_since_trigger"` // <30s
+}
+
+// DefaultGuardConfig returns production-ready guard configuration
+func DefaultGuardConfig() *GuardConfig {
+	return &GuardConfig{
+		MaxBarsAge:               2,    // ‚â§2 bars old
+		FatiguePrice24hThreshold: 12.0, // >12% in 24h
+		FatigueRSI4hThreshold:    70.0, // RSI >70
+		ProximityATRMultiple:     1.2,  // 1.2√ó ATR
+		MaxSecondsSinceTrigger:   30,   // <30 seconds
+	}
+}
+
+// NewGuardMetrics creates a new guard metrics evaluator
+func NewGuardMetrics(config *GuardConfig) *GuardMetrics {
+	if config == nil {
+		config = DefaultGuardConfig()
+	}
+	return &GuardMetrics{config: config}
+}
+
+// GuardInputs contains all data needed for guard evaluation
+type GuardInputs struct {
+	Symbol              string    `json:"symbol"`
+	BarsSinceSignal     int       `json:"bars_since_signal"`     // How many bars ago was signal
+	PriceChange24h      float64   `json:"price_change_24h"`      // 24h price change %
+	RSI4h               float64   `json:"rsi_4h"`                // 4-hour RSI
+	DistanceFromTrigger float64   `json:"distance_from_trigger"` // Distance from trigger price
+	ATR1h               float64   `json:"atr_1h"`                // 1-hour ATR for proximity
+	SecondsSinceTrigger int64     `json:"seconds_since_trigger"` // Time since trigger bar close
+	HasPullback         bool      `json:"has_pullback"`          // Recent pullback detected
+	HasAcceleration     bool      `json:"has_acceleration"`      // Renewed acceleration
+	Timestamp           time.Time `json:"timestamp"`
+}
+
+// GuardResult contains the evaluation results for all guards
+type GuardResult struct {
+	Symbol         string                `json:"symbol"`
+	Timestamp      time.Time             `json:"timestamp"`
+	AllPassed      bool                  `json:"all_passed"`
+	GuardChecks    map[string]*GateCheck `json:"guard_checks"`
+	FailureReasons []string              `json:"failure_reasons"`
+	PassedGuards   []string              `json:"passed_guards"`
+}
+
+// EvaluateGuards performs comprehensive guard evaluation
+func (gm *GuardMetrics) EvaluateGuards(ctx context.Context, inputs GuardInputs) (*GuardResult, error) {
+	result := &GuardResult{
+		Symbol:         inputs.Symbol,
+		Timestamp:      inputs.Timestamp,
+		GuardChecks:    make(map[string]*GateCheck),
+		FailureReasons: []string{},
+		PassedGuards:   []string{},
+	}
+
+	// Guard 1: Freshness - signal must be ‚â§2 bars old
+	freshnessCheck := gm.evaluateFreshnessGuard(inputs)
+	result.GuardChecks["freshness"] = freshnessCheck
+	if freshnessCheck.Passed {
+		result.PassedGuards = append(result.PassedGuards, "freshness")
+	} else {
+		result.FailureReasons = append(result.FailureReasons,
+			fmt.Sprintf("Stale signal: %d bars old (max %d)", inputs.BarsSinceSignal, gm.config.MaxBarsAge))
+	}
+
+	// Guard 2: Fatigue - prevent overextended entries
+	fatigueCheck := gm.evaluateFatigueGuard(inputs)
+	result.GuardChecks["fatigue"] = fatigueCheck
+	if fatigueCheck.Passed {
+		result.PassedGuards = append(result.PassedGuards, "fatigue")
+	} else {
+		result.FailureReasons = append(result.FailureReasons,
+			fmt.Sprintf("Fatigue detected: 24h %.1f%% + RSI %.1f (no pullback/accel)",
+				inputs.PriceChange24h, inputs.RSI4h))
+	}
+
+	// Guard 3: Proximity - price must be close to trigger
+	proximityCheck := gm.evaluateProximityGuard(inputs)
+	result.GuardChecks["proximity"] = proximityCheck
+	if proximityCheck.Passed {
+		result.PassedGuards = append(result.PassedGuards, "proximity")
+	} else {
+		maxDist := inputs.ATR1h * gm.config.ProximityATRMultiple
+		result.FailureReasons = append(result.FailureReasons,
+			fmt.Sprintf("Price too far: %.4f > %.4f (%.1fx ATR)",
+				inputs.DistanceFromTrigger, maxDist, gm.config.ProximityATRMultiple))
+	}
+
+	// Guard 4: Late-fill - execution must be timely
+	lateFillCheck := gm.evaluateLateFillGuard(inputs)
+	result.GuardChecks["late_fill"] = lateFillCheck
+	if lateFillCheck.Passed {
+		result.PassedGuards = append(result.PassedGuards, "late_fill")
+	} else {
+		result.FailureReasons = append(result.FailureReasons,
+			fmt.Sprintf("Late fill: %ds since trigger (max %ds)",
+				inputs.SecondsSinceTrigger, gm.config.MaxSecondsSinceTrigger))
+	}
+
+	// Overall result
+	result.AllPassed = len(result.FailureReasons) == 0
+
+	return result, nil
+}
+
+// evaluateFreshnessGuard ensures signal is recent (‚â§2 bars old)
+func (gm *GuardMetrics) evaluateFreshnessGuard(inputs GuardInputs) *GateCheck {
+	passed := inputs.BarsSinceSignal <= gm.config.MaxBarsAge
+
+	return &GateCheck{
+		Name:        "freshness",
+		Passed:      passed,
+		Value:       inputs.BarsSinceSignal,
+		Threshold:   gm.config.MaxBarsAge,
+		Description: fmt.Sprintf("Signal age %d bars ‚â§ %d bars", inputs.BarsSinceSignal, gm.config.MaxBarsAge),
+	}
+}
+
+// evaluateFatigueGuard checks for overextension (24h >12% AND RSI4h >70 UNLESS pullback/acceleration)
+func (gm *GuardMetrics) evaluateFatigueGuard(inputs GuardInputs) *GateCheck {
+	// Check if conditions indicate fatigue
+	isOverextended := inputs.PriceChange24h > gm.config.FatiguePrice24hThreshold &&
+		inputs.RSI4h > gm.config.FatigueRSI4hThreshold
+
+	// Check for exceptions that override fatigue
+	hasException := inputs.HasPullback || inputs.HasAcceleration
+
+	passed := !isOverextended || hasException
+
+	description := fmt.Sprintf("24h %.1f%% (thresh %.1f%%), RSI %.1f (thresh %.1f)",
+		inputs.PriceChange24h, gm.config.FatiguePrice24hThreshold,
+		inputs.RSI4h, gm.config.FatigueRSI4hThreshold)
+
+	if isOverextended && hasException {
+		description += " - EXCEPTION: pullback/acceleration detected"
+	}
+
+	return &GateCheck{
+		Name:        "fatigue",
+		Passed:      passed,
+		Value:       inputs.PriceChange24h,
+		Threshold:   gm.config.FatiguePrice24hThreshold,
+		Description: description,
+	}
+}
+
+// evaluateProximityGuard ensures price is close to trigger (‚â§1.2√ó ATR)
+func (gm *GuardMetrics) evaluateProximityGuard(inputs GuardInputs) *GateCheck {
+	maxDistance := inputs.ATR1h * gm.config.ProximityATRMultiple
+	passed := inputs.DistanceFromTrigger <= maxDistance
+
+	return &GateCheck{
+		Name:      "proximity",
+		Passed:    passed,
+		Value:     inputs.DistanceFromTrigger,
+		Threshold: maxDistance,
+		Description: fmt.Sprintf("Distance %.4f ‚â§ %.4f (%.1fx ATR)",
+			inputs.DistanceFromTrigger, maxDistance, gm.config.ProximityATRMultiple),
+	}
+}
+
+// evaluateLateFillGuard prevents late fills (<30s since trigger)
+func (gm *GuardMetrics) evaluateLateFillGuard(inputs GuardInputs) *GateCheck {
+	passed := inputs.SecondsSinceTrigger < gm.config.MaxSecondsSinceTrigger
+
+	return &GateCheck{
+		Name:        "late_fill",
+		Passed:      passed,
+		Value:       inputs.SecondsSinceTrigger,
+		Threshold:   gm.config.MaxSecondsSinceTrigger,
+		Description: fmt.Sprintf("Fill timing %ds < %ds", inputs.SecondsSinceTrigger, gm.config.MaxSecondsSinceTrigger),
+	}
+}
+
+// GetGuardSummary returns a concise guard evaluation summary
+func (gr *GuardResult) GetGuardSummary() string {
+	if gr.AllPassed {
+		return fmt.Sprintf("‚úÖ GUARDS CLEARED ‚Äî %s (%d/%d passed)",
+			gr.Symbol, len(gr.PassedGuards), len(gr.GuardChecks))
+	} else {
+		return fmt.Sprintf("‚ùå GUARD BLOCKED ‚Äî %s (%d failures)",
+			gr.Symbol, len(gr.FailureReasons))
+	}
+}
+
+// GetDetailedGuardReport returns comprehensive guard analysis
+func (gr *GuardResult) GetDetailedGuardReport() string {
+	report := fmt.Sprintf("Guard Evaluation: %s\n", gr.Symbol)
+	report += fmt.Sprintf("Overall: %s\n\n",
+		map[bool]string{true: "PASS ‚úÖ", false: "FAIL ‚ùå"}[gr.AllPassed])
+
+	// Show all guard results
+	guardOrder := []string{"freshness", "fatigue", "proximity", "late_fill"}
+
+	for _, guardName := range guardOrder {
+		if check, exists := gr.GuardChecks[guardName]; exists {
+			status := map[bool]string{true: "‚úÖ", false: "‚ùå"}[check.Passed]
+			report += fmt.Sprintf("%s %s: %s\n", status, check.Name, check.Description)
+		}
+	}
+
+	if len(gr.FailureReasons) > 0 {
+		report += fmt.Sprintf("\nFailure Details:\n")
+		for i, reason := range gr.FailureReasons {
+			report += fmt.Sprintf("  %d. %s\n", i+1, reason)
+		}
+	}
+
+	return report
+}
diff --git a/internal/microstructure/api.go b/internal/microstructure/api.go
index c2423a8..88ae8ca 100644
--- a/internal/microstructure/api.go
+++ b/internal/microstructure/api.go
@@ -1,223 +1,223 @@
-// Package microstructure provides execution feasibility gates and venue health checks
-// using exchange-native L1/L2 data (Binance/OKX/Coinbase). No aggregators allowed.
-package microstructure
-
-import (
-	"context"
-	"time"
-)
-
-// GateReport provides comprehensive microstructure gate evaluation results
-type GateReport struct {
-	Symbol    string    `json:"symbol"`
-	Venue     string    `json:"venue"`
-	Timestamp time.Time `json:"timestamp"`
-
-	// Gate results
-	DepthOK  bool `json:"depth_ok"`  // Depth within ¬±2% meets tier requirements
-	SpreadOK bool `json:"spread_ok"` // Spread ‚â§ tier cap (25-80 bps)
-	VadrOK   bool `json:"vadr_ok"`   // VADR ‚â• tier minimum (1.75-1.85√ó)
-
-	// Detailed metrics
-	Details GateDetails `json:"details"`
-
-	// Overall assessment
-	ExecutionFeasible bool   `json:"execution_feasible"` // All gates passed
-	RecommendedAction string `json:"recommended_action"` // "proceed", "halve_size", "defer"
-	FailureReasons    []string `json:"failure_reasons,omitempty"`
-}
-
-// GateDetails contains the raw measurements and calculations
-type GateDetails struct {
-	// Depth measurements (USD)
-	BidDepthUSD float64 `json:"bid_depth_usd"` // Total bids within -2%
-	AskDepthUSD float64 `json:"ask_depth_usd"` // Total asks within +2%
-	TotalDepthUSD float64 `json:"total_depth_usd"` // Combined depth
-	DepthRequiredUSD float64 `json:"depth_required_usd"` // Tier requirement
-
-	// Spread measurements
-	SpreadBps    float64 `json:"spread_bps"`     // Current spread in basis points
-	SpreadCapBps float64 `json:"spread_cap_bps"` // Tier cap in basis points
-
-	// VADR measurements
-	VADRCurrent float64 `json:"vadr_current"` // Current VADR multiple
-	VADRMinimum float64 `json:"vadr_minimum"` // Tier minimum required
-
-	// Venue health
-	VenueHealth VenueHealthStatus `json:"venue_health"`
-
-	// Liquidity tier
-	LiquidityTier string `json:"liquidity_tier"` // "tier1", "tier2", "tier3"
-	ADV           float64 `json:"adv"`            // Average Daily Volume (USD)
-
-	// Processing metadata  
-	DataAge       time.Duration `json:"data_age"`        // Age of L1/L2 data
-	ProcessingMs  int64         `json:"processing_ms"`   // Gate evaluation time
-	DataQuality   string        `json:"data_quality"`    // "excellent", "good", "degraded"
-}
-
-// VenueHealthStatus tracks venue operational metrics
-type VenueHealthStatus struct {
-	Healthy       bool          `json:"healthy"`
-	RejectRate    float64       `json:"reject_rate"`    // % orders rejected (last 15min)
-	LatencyP99Ms  int64         `json:"latency_p99_ms"` // 99th percentile latency 
-	ErrorRate     float64       `json:"error_rate"`     // % API errors (last 15min)
-	LastUpdate    time.Time     `json:"last_update"`
-	Recommendation string       `json:"recommendation"` // "full_size", "halve_size", "avoid"
-	UptimePercent float64       `json:"uptime_percent"` // 24h uptime percentage
-}
-
-// OrderBookSnapshot represents L1/L2 order book data from exchange
-type OrderBookSnapshot struct {
-	Symbol    string          `json:"symbol"`
-	Venue     string          `json:"venue"`
-	Timestamp time.Time       `json:"timestamp"`
-	Bids      []PriceLevel    `json:"bids"` // Descending by price
-	Asks      []PriceLevel    `json:"asks"` // Ascending by price
-	LastPrice float64         `json:"last_price"`
-	Metadata  SnapshotMetadata `json:"metadata"`
-}
-
-// PriceLevel represents a single order book level
-type PriceLevel struct {
-	Price float64 `json:"price"` // Price per unit
-	Size  float64 `json:"size"`  // Quantity available
-}
-
-// SnapshotMetadata contains snapshot quality information
-type SnapshotMetadata struct {
-	Source       string        `json:"source"`        // "binance", "okx", "coinbase"
-	Sequence     int64         `json:"sequence"`      // Exchange sequence number
-	IsStale      bool          `json:"is_stale"`      // Data older than 5s
-	UpdateAge    time.Duration `json:"update_age"`    // Time since last update
-	BookQuality  string        `json:"book_quality"`  // "full", "partial", "degraded"
-}
-
-// LiquidityTier defines execution size and quality requirements by ADV
-type LiquidityTier struct {
-	Name             string  `json:"name"`               // "tier1", "tier2", "tier3"
-	ADVMin           float64 `json:"adv_min"`            // Minimum ADV (USD) for tier
-	ADVMax           float64 `json:"adv_max"`            // Maximum ADV (USD) for tier
-	DepthMinUSD      float64 `json:"depth_min_usd"`      // Minimum depth within ¬±2%
-	SpreadCapBps     float64 `json:"spread_cap_bps"`     // Maximum spread (basis points)
-	VADRMinimum      float64 `json:"vadr_minimum"`       // Minimum VADR multiple
-	Description      string  `json:"description"`        // Human readable description
-}
-
-// Evaluator is the main interface for microstructure gate evaluation
-type Evaluator interface {
-	// EvaluateGates performs comprehensive gate evaluation for a symbol/venue
-	EvaluateGates(ctx context.Context, symbol, venue string, orderbook *OrderBookSnapshot, adv float64) (*GateReport, error)
-	
-	// GetLiquidityTier determines tier based on ADV
-	GetLiquidityTier(adv float64) *LiquidityTier
-	
-	// UpdateVenueHealth updates venue health metrics
-	UpdateVenueHealth(venue string, health VenueHealthStatus) error
-	
-	// GetVenueHealth retrieves current venue health status
-	GetVenueHealth(venue string) (*VenueHealthStatus, error)
-}
-
-// Config holds microstructure evaluator configuration
-type Config struct {
-	// Rolling averages
-	SpreadWindowSeconds int `yaml:"spread_window_seconds"` // Default: 60
-	DepthWindowSeconds  int `yaml:"depth_window_seconds"`  // Default: 60
-	
-	// Data quality thresholds
-	MaxDataAgeSeconds   int `yaml:"max_data_age_seconds"`   // Default: 5
-	MinBookLevels       int `yaml:"min_book_levels"`        // Default: 5
-	
-	// Venue health thresholds
-	RejectRateThreshold   float64 `yaml:"reject_rate_threshold"`   // Default: 5.0%
-	LatencyThresholdMs    int64   `yaml:"latency_threshold_ms"`    // Default: 2000ms
-	ErrorRateThreshold    float64 `yaml:"error_rate_threshold"`    // Default: 3.0%
-	
-	// Supported venues (USD pairs only)
-	SupportedVenues []string `yaml:"supported_venues"` // ["binance", "okx", "coinbase"]
-	
-	// Liquidity tiers configuration
-	LiquidityTiers []LiquidityTier `yaml:"liquidity_tiers"`
-}
-
-// DefaultConfig returns production-ready configuration
-func DefaultConfig() *Config {
-	return &Config{
-		SpreadWindowSeconds: 60,
-		DepthWindowSeconds:  60,
-		MaxDataAgeSeconds:   5,
-		MinBookLevels:       5,
-		RejectRateThreshold: 5.0,
-		LatencyThresholdMs:  2000,
-		ErrorRateThreshold:  3.0,
-		SupportedVenues:     []string{"binance", "okx", "coinbase"},
-		LiquidityTiers: []LiquidityTier{
-			{
-				Name:         "tier1",
-				ADVMin:       5000000,  // $5M+ ADV
-				ADVMax:       1e12,     // No upper limit
-				DepthMinUSD:  150000,   // $150k depth
-				SpreadCapBps: 25,       // 25 bps max spread
-				VADRMinimum:  1.85,     // 1.85√ó minimum VADR
-				Description:  "High liquidity: Large caps, stablecoins",
-			},
-			{
-				Name:         "tier2", 
-				ADVMin:       1000000,  // $1M-5M ADV
-				ADVMax:       5000000,
-				DepthMinUSD:  75000,    // $75k depth
-				SpreadCapBps: 50,       // 50 bps max spread
-				VADRMinimum:  1.80,     // 1.80√ó minimum VADR
-				Description:  "Medium liquidity: Mid caps",
-			},
-			{
-				Name:         "tier3",
-				ADVMin:       100000,   // $100k-1M ADV
-				ADVMax:       1000000,
-				DepthMinUSD:  25000,    // $25k depth
-				SpreadCapBps: 80,       // 80 bps max spread
-				VADRMinimum:  1.75,     // 1.75√ó minimum VADR
-				Description:  "Lower liquidity: Small caps",
-			},
-		},
-	}
-}
-
-// VenueMetrics tracks venue operational performance
-type VenueMetrics struct {
-	Venue           string        `json:"venue"`
-	RecentRequests  []RequestLog  `json:"recent_requests"`  // Last 100 requests
-	RecentErrors    []ErrorLog    `json:"recent_errors"`    // Last 50 errors
-	HealthHistory   []HealthPoint `json:"health_history"`   // Last 24h health
-	LastHealthCheck time.Time     `json:"last_health_check"`
-}
-
-// RequestLog tracks individual API requests
-type RequestLog struct {
-	Timestamp   time.Time `json:"timestamp"`
-	Endpoint    string    `json:"endpoint"`
-	LatencyMs   int64     `json:"latency_ms"`
-	Success     bool      `json:"success"`
-	StatusCode  int       `json:"status_code"`
-	ErrorCode   string    `json:"error_code,omitempty"`
-}
-
-// ErrorLog tracks API errors
-type ErrorLog struct {
-	Timestamp   time.Time `json:"timestamp"`
-	Endpoint    string    `json:"endpoint"`
-	ErrorType   string    `json:"error_type"`
-	ErrorCode   string    `json:"error_code"`
-	Message     string    `json:"message"`
-	Recoverable bool      `json:"recoverable"`
-}
-
-// HealthPoint tracks venue health over time
-type HealthPoint struct {
-	Timestamp     time.Time `json:"timestamp"`
-	Healthy       bool      `json:"healthy"`
-	RejectRate    float64   `json:"reject_rate"`
-	LatencyP99Ms  int64     `json:"latency_p99_ms"`
-	ErrorRate     float64   `json:"error_rate"`
-}
\ No newline at end of file
+// Package microstructure provides execution feasibility gates and venue health checks
+// using exchange-native L1/L2 data (Binance/OKX/Coinbase). No aggregators allowed.
+package microstructure
+
+import (
+	"context"
+	"time"
+)
+
+// GateReport provides comprehensive microstructure gate evaluation results
+type GateReport struct {
+	Symbol    string    `json:"symbol"`
+	Venue     string    `json:"venue"`
+	Timestamp time.Time `json:"timestamp"`
+
+	// Gate results
+	DepthOK  bool `json:"depth_ok"`  // Depth within ¬±2% meets tier requirements
+	SpreadOK bool `json:"spread_ok"` // Spread ‚â§ tier cap (25-80 bps)
+	VadrOK   bool `json:"vadr_ok"`   // VADR ‚â• tier minimum (1.75-1.85√ó)
+
+	// Detailed metrics
+	Details GateDetails `json:"details"`
+
+	// Overall assessment
+	ExecutionFeasible bool     `json:"execution_feasible"` // All gates passed
+	RecommendedAction string   `json:"recommended_action"` // "proceed", "halve_size", "defer"
+	FailureReasons    []string `json:"failure_reasons,omitempty"`
+}
+
+// GateDetails contains the raw measurements and calculations
+type GateDetails struct {
+	// Depth measurements (USD)
+	BidDepthUSD      float64 `json:"bid_depth_usd"`      // Total bids within -2%
+	AskDepthUSD      float64 `json:"ask_depth_usd"`      // Total asks within +2%
+	TotalDepthUSD    float64 `json:"total_depth_usd"`    // Combined depth
+	DepthRequiredUSD float64 `json:"depth_required_usd"` // Tier requirement
+
+	// Spread measurements
+	SpreadBps    float64 `json:"spread_bps"`     // Current spread in basis points
+	SpreadCapBps float64 `json:"spread_cap_bps"` // Tier cap in basis points
+
+	// VADR measurements
+	VADRCurrent float64 `json:"vadr_current"` // Current VADR multiple
+	VADRMinimum float64 `json:"vadr_minimum"` // Tier minimum required
+
+	// Venue health
+	VenueHealth VenueHealthStatus `json:"venue_health"`
+
+	// Liquidity tier
+	LiquidityTier string  `json:"liquidity_tier"` // "tier1", "tier2", "tier3"
+	ADV           float64 `json:"adv"`            // Average Daily Volume (USD)
+
+	// Processing metadata
+	DataAge      time.Duration `json:"data_age"`      // Age of L1/L2 data
+	ProcessingMs int64         `json:"processing_ms"` // Gate evaluation time
+	DataQuality  string        `json:"data_quality"`  // "excellent", "good", "degraded"
+}
+
+// VenueHealthStatus tracks venue operational metrics
+type VenueHealthStatus struct {
+	Healthy        bool      `json:"healthy"`
+	RejectRate     float64   `json:"reject_rate"`    // % orders rejected (last 15min)
+	LatencyP99Ms   int64     `json:"latency_p99_ms"` // 99th percentile latency
+	ErrorRate      float64   `json:"error_rate"`     // % API errors (last 15min)
+	LastUpdate     time.Time `json:"last_update"`
+	Recommendation string    `json:"recommendation"` // "full_size", "halve_size", "avoid"
+	UptimePercent  float64   `json:"uptime_percent"` // 24h uptime percentage
+}
+
+// OrderBookSnapshot represents L1/L2 order book data from exchange
+type OrderBookSnapshot struct {
+	Symbol    string           `json:"symbol"`
+	Venue     string           `json:"venue"`
+	Timestamp time.Time        `json:"timestamp"`
+	Bids      []PriceLevel     `json:"bids"` // Descending by price
+	Asks      []PriceLevel     `json:"asks"` // Ascending by price
+	LastPrice float64          `json:"last_price"`
+	Metadata  SnapshotMetadata `json:"metadata"`
+}
+
+// PriceLevel represents a single order book level
+type PriceLevel struct {
+	Price float64 `json:"price"` // Price per unit
+	Size  float64 `json:"size"`  // Quantity available
+}
+
+// SnapshotMetadata contains snapshot quality information
+type SnapshotMetadata struct {
+	Source      string        `json:"source"`       // "binance", "okx", "coinbase"
+	Sequence    int64         `json:"sequence"`     // Exchange sequence number
+	IsStale     bool          `json:"is_stale"`     // Data older than 5s
+	UpdateAge   time.Duration `json:"update_age"`   // Time since last update
+	BookQuality string        `json:"book_quality"` // "full", "partial", "degraded"
+}
+
+// LiquidityTier defines execution size and quality requirements by ADV
+type LiquidityTier struct {
+	Name         string  `json:"name"`           // "tier1", "tier2", "tier3"
+	ADVMin       float64 `json:"adv_min"`        // Minimum ADV (USD) for tier
+	ADVMax       float64 `json:"adv_max"`        // Maximum ADV (USD) for tier
+	DepthMinUSD  float64 `json:"depth_min_usd"`  // Minimum depth within ¬±2%
+	SpreadCapBps float64 `json:"spread_cap_bps"` // Maximum spread (basis points)
+	VADRMinimum  float64 `json:"vadr_minimum"`   // Minimum VADR multiple
+	Description  string  `json:"description"`    // Human readable description
+}
+
+// Evaluator is the main interface for microstructure gate evaluation
+type Evaluator interface {
+	// EvaluateGates performs comprehensive gate evaluation for a symbol/venue
+	EvaluateGates(ctx context.Context, symbol, venue string, orderbook *OrderBookSnapshot, adv float64) (*GateReport, error)
+
+	// GetLiquidityTier determines tier based on ADV
+	GetLiquidityTier(adv float64) *LiquidityTier
+
+	// UpdateVenueHealth updates venue health metrics
+	UpdateVenueHealth(venue string, health VenueHealthStatus) error
+
+	// GetVenueHealth retrieves current venue health status
+	GetVenueHealth(venue string) (*VenueHealthStatus, error)
+}
+
+// Config holds microstructure evaluator configuration
+type Config struct {
+	// Rolling averages
+	SpreadWindowSeconds int `yaml:"spread_window_seconds"` // Default: 60
+	DepthWindowSeconds  int `yaml:"depth_window_seconds"`  // Default: 60
+
+	// Data quality thresholds
+	MaxDataAgeSeconds int `yaml:"max_data_age_seconds"` // Default: 5
+	MinBookLevels     int `yaml:"min_book_levels"`      // Default: 5
+
+	// Venue health thresholds
+	RejectRateThreshold float64 `yaml:"reject_rate_threshold"` // Default: 5.0%
+	LatencyThresholdMs  int64   `yaml:"latency_threshold_ms"`  // Default: 2000ms
+	ErrorRateThreshold  float64 `yaml:"error_rate_threshold"`  // Default: 3.0%
+
+	// Supported venues (USD pairs only)
+	SupportedVenues []string `yaml:"supported_venues"` // ["binance", "okx", "coinbase"]
+
+	// Liquidity tiers configuration
+	LiquidityTiers []LiquidityTier `yaml:"liquidity_tiers"`
+}
+
+// DefaultConfig returns production-ready configuration
+func DefaultConfig() *Config {
+	return &Config{
+		SpreadWindowSeconds: 60,
+		DepthWindowSeconds:  60,
+		MaxDataAgeSeconds:   5,
+		MinBookLevels:       5,
+		RejectRateThreshold: 5.0,
+		LatencyThresholdMs:  2000,
+		ErrorRateThreshold:  3.0,
+		SupportedVenues:     []string{"binance", "okx", "coinbase"},
+		LiquidityTiers: []LiquidityTier{
+			{
+				Name:         "tier1",
+				ADVMin:       5000000, // $5M+ ADV
+				ADVMax:       1e12,    // No upper limit
+				DepthMinUSD:  150000,  // $150k depth
+				SpreadCapBps: 25,      // 25 bps max spread
+				VADRMinimum:  1.85,    // 1.85√ó minimum VADR
+				Description:  "High liquidity: Large caps, stablecoins",
+			},
+			{
+				Name:         "tier2",
+				ADVMin:       1000000, // $1M-5M ADV
+				ADVMax:       5000000,
+				DepthMinUSD:  75000, // $75k depth
+				SpreadCapBps: 50,    // 50 bps max spread
+				VADRMinimum:  1.80,  // 1.80√ó minimum VADR
+				Description:  "Medium liquidity: Mid caps",
+			},
+			{
+				Name:         "tier3",
+				ADVMin:       100000, // $100k-1M ADV
+				ADVMax:       1000000,
+				DepthMinUSD:  25000, // $25k depth
+				SpreadCapBps: 80,    // 80 bps max spread
+				VADRMinimum:  1.75,  // 1.75√ó minimum VADR
+				Description:  "Lower liquidity: Small caps",
+			},
+		},
+	}
+}
+
+// VenueMetrics tracks venue operational performance
+type VenueMetrics struct {
+	Venue           string        `json:"venue"`
+	RecentRequests  []RequestLog  `json:"recent_requests"` // Last 100 requests
+	RecentErrors    []ErrorLog    `json:"recent_errors"`   // Last 50 errors
+	HealthHistory   []HealthPoint `json:"health_history"`  // Last 24h health
+	LastHealthCheck time.Time     `json:"last_health_check"`
+}
+
+// RequestLog tracks individual API requests
+type RequestLog struct {
+	Timestamp  time.Time `json:"timestamp"`
+	Endpoint   string    `json:"endpoint"`
+	LatencyMs  int64     `json:"latency_ms"`
+	Success    bool      `json:"success"`
+	StatusCode int       `json:"status_code"`
+	ErrorCode  string    `json:"error_code,omitempty"`
+}
+
+// ErrorLog tracks API errors
+type ErrorLog struct {
+	Timestamp   time.Time `json:"timestamp"`
+	Endpoint    string    `json:"endpoint"`
+	ErrorType   string    `json:"error_type"`
+	ErrorCode   string    `json:"error_code"`
+	Message     string    `json:"message"`
+	Recoverable bool      `json:"recoverable"`
+}
+
+// HealthPoint tracks venue health over time
+type HealthPoint struct {
+	Timestamp    time.Time `json:"timestamp"`
+	Healthy      bool      `json:"healthy"`
+	RejectRate   float64   `json:"reject_rate"`
+	LatencyP99Ms int64     `json:"latency_p99_ms"`
+	ErrorRate    float64   `json:"error_rate"`
+}
diff --git a/internal/microstructure/depth.go b/internal/microstructure/depth.go
index abaa46c..9f67afd 100644
--- a/internal/microstructure/depth.go
+++ b/internal/microstructure/depth.go
@@ -1,216 +1,216 @@
-package microstructure
-
-import (
-	"fmt"
-	"math"
-)
-
-// DepthCalculator computes order book depth within ¬±2% price bounds
-type DepthCalculator struct {
-	windowSeconds int
-	priceBounds   float64 // Default: 0.02 (2%)
-}
-
-// NewDepthCalculator creates a depth calculator with rolling window
-func NewDepthCalculator(windowSeconds int) *DepthCalculator {
-	return &DepthCalculator{
-		windowSeconds: windowSeconds,
-		priceBounds:   0.02, // ¬±2%
-	}
-}
-
-// DepthResult contains depth calculation results
-type DepthResult struct {
-	BidDepthUSD   float64 `json:"bid_depth_usd"`   // Total bid liquidity within bounds
-	AskDepthUSD   float64 `json:"ask_depth_usd"`   // Total ask liquidity within bounds
-	TotalDepthUSD float64 `json:"total_depth_usd"` // Combined depth
-	BidLevels     int     `json:"bid_levels"`      // Number of bid levels included
-	AskLevels     int     `json:"ask_levels"`      // Number of ask levels included
-	LastPrice     float64 `json:"last_price"`      // Reference price
-	BidBound      float64 `json:"bid_bound"`       // Lower bound price (-2%)
-	AskBound      float64 `json:"ask_bound"`       // Upper bound price (+2%)
-}
-
-// CalculateDepth computes depth within ¬±2% of last trade price
-func (dc *DepthCalculator) CalculateDepth(orderbook *OrderBookSnapshot) (*DepthResult, error) {
-	if orderbook == nil {
-		return nil, fmt.Errorf("order book snapshot is nil")
-	}
-
-	if orderbook.LastPrice <= 0 {
-		return nil, fmt.Errorf("invalid last price: %.6f", orderbook.LastPrice)
-	}
-
-	if len(orderbook.Bids) == 0 && len(orderbook.Asks) == 0 {
-		return nil, fmt.Errorf("empty order book for %s", orderbook.Symbol)
-	}
-
-	lastPrice := orderbook.LastPrice
-	bidBound := lastPrice * (1.0 - dc.priceBounds)  // -2%
-	askBound := lastPrice * (1.0 + dc.priceBounds)  // +2%
-
-	result := &DepthResult{
-		LastPrice: lastPrice,
-		BidBound:  bidBound,
-		AskBound:  askBound,
-	}
-
-	// Calculate bid depth (prices >= bidBound)
-	bidDepthUSD := 0.0
-	bidLevels := 0
-	for _, bid := range orderbook.Bids {
-		if bid.Price >= bidBound {
-			bidDepthUSD += bid.Price * bid.Size
-			bidLevels++
-		} else {
-			// Bids are sorted descending, so we can break early
-			break
-		}
-	}
-
-	// Calculate ask depth (prices <= askBound)
-	askDepthUSD := 0.0
-	askLevels := 0
-	for _, ask := range orderbook.Asks {
-		if ask.Price <= askBound {
-			askDepthUSD += ask.Price * ask.Size
-			askLevels++
-		} else {
-			// Asks are sorted ascending, so we can break early
-			break
-		}
-	}
-
-	result.BidDepthUSD = bidDepthUSD
-	result.AskDepthUSD = askDepthUSD
-	result.TotalDepthUSD = bidDepthUSD + askDepthUSD
-	result.BidLevels = bidLevels
-	result.AskLevels = askLevels
-
-	return result, nil
-}
-
-// ValidateDepthRequirement checks if depth meets tier requirements
-func (dc *DepthCalculator) ValidateDepthRequirement(depthResult *DepthResult, tier *LiquidityTier) (bool, string) {
-	if depthResult == nil || tier == nil {
-		return false, "invalid inputs"
-	}
-
-	if depthResult.TotalDepthUSD >= tier.DepthMinUSD {
-		return true, fmt.Sprintf("depth $%.0f ‚â• $%.0f (%s)", 
-			depthResult.TotalDepthUSD, tier.DepthMinUSD, tier.Name)
-	}
-
-	return false, fmt.Sprintf("insufficient depth: $%.0f < $%.0f (%s requirement)", 
-		depthResult.TotalDepthUSD, tier.DepthMinUSD, tier.Name)
-}
-
-// GetDepthSummary returns a human-readable depth summary
-func (dc *DepthCalculator) GetDepthSummary(depthResult *DepthResult) string {
-	if depthResult == nil {
-		return "no depth data"
-	}
-
-	return fmt.Sprintf("Depth: $%.0f total ($%.0f bids @ %d levels, $%.0f asks @ %d levels) within ¬±2%% of $%.4f",
-		depthResult.TotalDepthUSD,
-		depthResult.BidDepthUSD, depthResult.BidLevels,
-		depthResult.AskDepthUSD, depthResult.AskLevels,
-		depthResult.LastPrice)
-}
-
-// CalculateDepthBalance returns bid/ask depth balance ratio
-func (dc *DepthCalculator) CalculateDepthBalance(depthResult *DepthResult) float64 {
-	if depthResult == nil || depthResult.TotalDepthUSD == 0 {
-		return 0.5 // Neutral if no data
-	}
-
-	// Returns 0.0-1.0, where 0.5 is perfectly balanced
-	// <0.5 = ask-heavy, >0.5 = bid-heavy
-	return depthResult.BidDepthUSD / depthResult.TotalDepthUSD
-}
-
-// EstimateMarketImpact estimates price impact for a given trade size
-func (dc *DepthCalculator) EstimateMarketImpact(orderbook *OrderBookSnapshot, tradeSizeUSD float64, side string) (*MarketImpact, error) {
-	if orderbook == nil {
-		return nil, fmt.Errorf("order book snapshot is nil")
-	}
-
-	if tradeSizeUSD <= 0 {
-		return nil, fmt.Errorf("invalid trade size: %.2f", tradeSizeUSD)
-	}
-
-	var levels []PriceLevel
-	var isAscending bool
-
-	switch side {
-	case "buy":
-		levels = orderbook.Asks
-		isAscending = true
-	case "sell":
-		levels = orderbook.Bids
-		isAscending = false
-	default:
-		return nil, fmt.Errorf("invalid side: %s (must be 'buy' or 'sell')", side)
-	}
-
-	if len(levels) == 0 {
-		return nil, fmt.Errorf("no %s side liquidity", side)
-	}
-
-	impact := &MarketImpact{
-		Side:          side,
-		RequestedUSD:  tradeSizeUSD,
-		StartPrice:    orderbook.LastPrice,
-		LevelsConsumed: 0,
-	}
-
-	remainingUSD := tradeSizeUSD
-	totalCost := 0.0
-	totalQuantity := 0.0
-
-	for i, level := range levels {
-		if remainingUSD <= 0 {
-			break
-		}
-
-		levelValueUSD := level.Price * level.Size
-		consumedUSD := math.Min(remainingUSD, levelValueUSD)
-		consumedQuantity := consumedUSD / level.Price
-
-		totalCost += consumedUSD
-		totalQuantity += consumedQuantity
-		remainingUSD -= consumedUSD
-		impact.LevelsConsumed = i + 1
-		impact.FinalPrice = level.Price
-	}
-
-	if remainingUSD > 0 {
-		impact.InsufficientLiquidity = true
-		impact.ShortfallUSD = remainingUSD
-	}
-
-	if totalQuantity > 0 {
-		impact.AveragePrice = totalCost / totalQuantity
-		impact.SlippageBps = math.Abs(impact.AveragePrice - impact.StartPrice) / impact.StartPrice * 10000
-	}
-
-	impact.FilledUSD = totalCost
-	impact.FilledQuantity = totalQuantity
-
-	return impact, nil
-}
-
-// MarketImpact contains market impact analysis results
-type MarketImpact struct {
-	Side                  string  `json:"side"`                    // "buy" or "sell"
-	RequestedUSD          float64 `json:"requested_usd"`           // Requested trade size
-	FilledUSD             float64 `json:"filled_usd"`              // Actually filled amount
-	FilledQuantity        float64 `json:"filled_quantity"`         // Filled quantity in base units
-	StartPrice            float64 `json:"start_price"`             // Starting reference price
-	AveragePrice          float64 `json:"average_price"`           // Volume-weighted average price
-	FinalPrice            float64 `json:"final_price"`             // Final level price consumed
-	SlippageBps           float64 `json:"slippage_bps"`            // Slippage in basis points
-	LevelsConsumed        int     `json:"levels_consumed"`         // Number of order book levels
-	InsufficientLiquidity bool    `json:"insufficient_liquidity"`  // Could not fill completely
-	ShortfallUSD          float64 `json:"shortfall_usd,omitempty"` // Unfilled amount if insufficient
-}
\ No newline at end of file
+package microstructure
+
+import (
+	"fmt"
+	"math"
+)
+
+// DepthCalculator computes order book depth within ¬±2% price bounds
+type DepthCalculator struct {
+	windowSeconds int
+	priceBounds   float64 // Default: 0.02 (2%)
+}
+
+// NewDepthCalculator creates a depth calculator with rolling window
+func NewDepthCalculator(windowSeconds int) *DepthCalculator {
+	return &DepthCalculator{
+		windowSeconds: windowSeconds,
+		priceBounds:   0.02, // ¬±2%
+	}
+}
+
+// DepthResult contains depth calculation results
+type DepthResult struct {
+	BidDepthUSD   float64 `json:"bid_depth_usd"`   // Total bid liquidity within bounds
+	AskDepthUSD   float64 `json:"ask_depth_usd"`   // Total ask liquidity within bounds
+	TotalDepthUSD float64 `json:"total_depth_usd"` // Combined depth
+	BidLevels     int     `json:"bid_levels"`      // Number of bid levels included
+	AskLevels     int     `json:"ask_levels"`      // Number of ask levels included
+	LastPrice     float64 `json:"last_price"`      // Reference price
+	BidBound      float64 `json:"bid_bound"`       // Lower bound price (-2%)
+	AskBound      float64 `json:"ask_bound"`       // Upper bound price (+2%)
+}
+
+// CalculateDepth computes depth within ¬±2% of last trade price
+func (dc *DepthCalculator) CalculateDepth(orderbook *OrderBookSnapshot) (*DepthResult, error) {
+	if orderbook == nil {
+		return nil, fmt.Errorf("order book snapshot is nil")
+	}
+
+	if orderbook.LastPrice <= 0 {
+		return nil, fmt.Errorf("invalid last price: %.6f", orderbook.LastPrice)
+	}
+
+	if len(orderbook.Bids) == 0 && len(orderbook.Asks) == 0 {
+		return nil, fmt.Errorf("empty order book for %s", orderbook.Symbol)
+	}
+
+	lastPrice := orderbook.LastPrice
+	bidBound := lastPrice * (1.0 - dc.priceBounds) // -2%
+	askBound := lastPrice * (1.0 + dc.priceBounds) // +2%
+
+	result := &DepthResult{
+		LastPrice: lastPrice,
+		BidBound:  bidBound,
+		AskBound:  askBound,
+	}
+
+	// Calculate bid depth (prices >= bidBound)
+	bidDepthUSD := 0.0
+	bidLevels := 0
+	for _, bid := range orderbook.Bids {
+		if bid.Price >= bidBound {
+			bidDepthUSD += bid.Price * bid.Size
+			bidLevels++
+		} else {
+			// Bids are sorted descending, so we can break early
+			break
+		}
+	}
+
+	// Calculate ask depth (prices <= askBound)
+	askDepthUSD := 0.0
+	askLevels := 0
+	for _, ask := range orderbook.Asks {
+		if ask.Price <= askBound {
+			askDepthUSD += ask.Price * ask.Size
+			askLevels++
+		} else {
+			// Asks are sorted ascending, so we can break early
+			break
+		}
+	}
+
+	result.BidDepthUSD = bidDepthUSD
+	result.AskDepthUSD = askDepthUSD
+	result.TotalDepthUSD = bidDepthUSD + askDepthUSD
+	result.BidLevels = bidLevels
+	result.AskLevels = askLevels
+
+	return result, nil
+}
+
+// ValidateDepthRequirement checks if depth meets tier requirements
+func (dc *DepthCalculator) ValidateDepthRequirement(depthResult *DepthResult, tier *LiquidityTier) (bool, string) {
+	if depthResult == nil || tier == nil {
+		return false, "invalid inputs"
+	}
+
+	if depthResult.TotalDepthUSD >= tier.DepthMinUSD {
+		return true, fmt.Sprintf("depth $%.0f ‚â• $%.0f (%s)",
+			depthResult.TotalDepthUSD, tier.DepthMinUSD, tier.Name)
+	}
+
+	return false, fmt.Sprintf("insufficient depth: $%.0f < $%.0f (%s requirement)",
+		depthResult.TotalDepthUSD, tier.DepthMinUSD, tier.Name)
+}
+
+// GetDepthSummary returns a human-readable depth summary
+func (dc *DepthCalculator) GetDepthSummary(depthResult *DepthResult) string {
+	if depthResult == nil {
+		return "no depth data"
+	}
+
+	return fmt.Sprintf("Depth: $%.0f total ($%.0f bids @ %d levels, $%.0f asks @ %d levels) within ¬±2%% of $%.4f",
+		depthResult.TotalDepthUSD,
+		depthResult.BidDepthUSD, depthResult.BidLevels,
+		depthResult.AskDepthUSD, depthResult.AskLevels,
+		depthResult.LastPrice)
+}
+
+// CalculateDepthBalance returns bid/ask depth balance ratio
+func (dc *DepthCalculator) CalculateDepthBalance(depthResult *DepthResult) float64 {
+	if depthResult == nil || depthResult.TotalDepthUSD == 0 {
+		return 0.5 // Neutral if no data
+	}
+
+	// Returns 0.0-1.0, where 0.5 is perfectly balanced
+	// <0.5 = ask-heavy, >0.5 = bid-heavy
+	return depthResult.BidDepthUSD / depthResult.TotalDepthUSD
+}
+
+// EstimateMarketImpact estimates price impact for a given trade size
+func (dc *DepthCalculator) EstimateMarketImpact(orderbook *OrderBookSnapshot, tradeSizeUSD float64, side string) (*MarketImpact, error) {
+	if orderbook == nil {
+		return nil, fmt.Errorf("order book snapshot is nil")
+	}
+
+	if tradeSizeUSD <= 0 {
+		return nil, fmt.Errorf("invalid trade size: %.2f", tradeSizeUSD)
+	}
+
+	var levels []PriceLevel
+	var isAscending bool
+
+	switch side {
+	case "buy":
+		levels = orderbook.Asks
+		isAscending = true
+	case "sell":
+		levels = orderbook.Bids
+		isAscending = false
+	default:
+		return nil, fmt.Errorf("invalid side: %s (must be 'buy' or 'sell')", side)
+	}
+
+	if len(levels) == 0 {
+		return nil, fmt.Errorf("no %s side liquidity", side)
+	}
+
+	impact := &MarketImpact{
+		Side:           side,
+		RequestedUSD:   tradeSizeUSD,
+		StartPrice:     orderbook.LastPrice,
+		LevelsConsumed: 0,
+	}
+
+	remainingUSD := tradeSizeUSD
+	totalCost := 0.0
+	totalQuantity := 0.0
+
+	for i, level := range levels {
+		if remainingUSD <= 0 {
+			break
+		}
+
+		levelValueUSD := level.Price * level.Size
+		consumedUSD := math.Min(remainingUSD, levelValueUSD)
+		consumedQuantity := consumedUSD / level.Price
+
+		totalCost += consumedUSD
+		totalQuantity += consumedQuantity
+		remainingUSD -= consumedUSD
+		impact.LevelsConsumed = i + 1
+		impact.FinalPrice = level.Price
+	}
+
+	if remainingUSD > 0 {
+		impact.InsufficientLiquidity = true
+		impact.ShortfallUSD = remainingUSD
+	}
+
+	if totalQuantity > 0 {
+		impact.AveragePrice = totalCost / totalQuantity
+		impact.SlippageBps = math.Abs(impact.AveragePrice-impact.StartPrice) / impact.StartPrice * 10000
+	}
+
+	impact.FilledUSD = totalCost
+	impact.FilledQuantity = totalQuantity
+
+	return impact, nil
+}
+
+// MarketImpact contains market impact analysis results
+type MarketImpact struct {
+	Side                  string  `json:"side"`                    // "buy" or "sell"
+	RequestedUSD          float64 `json:"requested_usd"`           // Requested trade size
+	FilledUSD             float64 `json:"filled_usd"`              // Actually filled amount
+	FilledQuantity        float64 `json:"filled_quantity"`         // Filled quantity in base units
+	StartPrice            float64 `json:"start_price"`             // Starting reference price
+	AveragePrice          float64 `json:"average_price"`           // Volume-weighted average price
+	FinalPrice            float64 `json:"final_price"`             // Final level price consumed
+	SlippageBps           float64 `json:"slippage_bps"`            // Slippage in basis points
+	LevelsConsumed        int     `json:"levels_consumed"`         // Number of order book levels
+	InsufficientLiquidity bool    `json:"insufficient_liquidity"`  // Could not fill completely
+	ShortfallUSD          float64 `json:"shortfall_usd,omitempty"` // Unfilled amount if insufficient
+}
diff --git a/internal/microstructure/evaluator.go b/internal/microstructure/evaluator.go
index 7ec3dbe..63f5987 100644
--- a/internal/microstructure/evaluator.go
+++ b/internal/microstructure/evaluator.go
@@ -1,333 +1,333 @@
-package microstructure
-
-import (
-	"context"
-	"fmt"
-	"time"
-)
-
-// MicrostructureEvaluator is the main implementation of the Evaluator interface
-type MicrostructureEvaluator struct {
-	config         *Config
-	depthCalc      *DepthCalculator
-	spreadCalc     *SpreadCalculator
-	vadrCalc       *VADRCalculator
-	tierManager    *LiquidityTierManager
-	healthMonitor  *VenueHealthMonitor
-}
-
-// NewMicrostructureEvaluator creates a new microstructure evaluator
-func NewMicrostructureEvaluator(config *Config) *MicrostructureEvaluator {
-	if config == nil {
-		config = DefaultConfig()
-	}
-
-	return &MicrostructureEvaluator{
-		config:        config,
-		depthCalc:     NewDepthCalculator(config.DepthWindowSeconds),
-		spreadCalc:    NewSpreadCalculator(config.SpreadWindowSeconds),
-		vadrCalc:      NewVADRCalculator(),
-		tierManager:   NewLiquidityTierManagerWithConfig(config.LiquidityTiers),
-		healthMonitor: NewVenueHealthMonitor(&VenueHealthConfig{
-			RejectRateThreshold: config.RejectRateThreshold,
-			LatencyThresholdMs:  config.LatencyThresholdMs,
-			ErrorRateThreshold:  config.ErrorRateThreshold,
-			WindowDuration:      15 * time.Minute,
-			MinSamplesForHealth: 10,
-			MaxHistorySize:      1000,
-		}),
-	}
-}
-
-// EvaluateGates performs comprehensive gate evaluation for a symbol/venue
-func (me *MicrostructureEvaluator) EvaluateGates(ctx context.Context, symbol, venue string, orderbook *OrderBookSnapshot, adv float64) (*GateReport, error) {
-	startTime := time.Now()
-	
-	// Validate inputs
-	if err := me.validateInputs(symbol, venue, orderbook, adv); err != nil {
-		return nil, fmt.Errorf("input validation failed: %w", err)
-	}
-
-	// Determine liquidity tier
-	tier, err := me.tierManager.GetTierByADV(adv)
-	if err != nil {
-		return nil, fmt.Errorf("failed to determine tier for ADV %.0f: %w", adv, err)
-	}
-
-	// Initialize gate report
-	report := &GateReport{
-		Symbol:    symbol,
-		Venue:     venue,
-		Timestamp: orderbook.Timestamp,
-		Details: GateDetails{
-			LiquidityTier: tier.Name,
-			ADV:           adv,
-		},
-	}
-
-	// Evaluate depth gate
-	depthResult, err := me.depthCalc.CalculateDepth(orderbook)
-	if err != nil {
-		return nil, fmt.Errorf("depth calculation failed: %w", err)
-	}
-
-	report.DepthOK, _ = me.depthCalc.ValidateDepthRequirement(depthResult, tier)
-	report.Details.BidDepthUSD = depthResult.BidDepthUSD
-	report.Details.AskDepthUSD = depthResult.AskDepthUSD
-	report.Details.TotalDepthUSD = depthResult.TotalDepthUSD
-	report.Details.DepthRequiredUSD = tier.DepthMinUSD
-
-	// Evaluate spread gate
-	spreadResult, err := me.spreadCalc.CalculateSpread(orderbook)
-	if err != nil {
-		return nil, fmt.Errorf("spread calculation failed: %w", err)
-	}
-
-	report.SpreadOK, _ = me.spreadCalc.ValidateSpreadRequirement(spreadResult, tier)
-	report.Details.SpreadBps = spreadResult.Current.SpreadBps
-	report.Details.SpreadCapBps = tier.SpreadCapBps
-
-	// Evaluate VADR gate (requires additional input)
-	vadrInput := &VADRInput{
-		High:         orderbook.LastPrice * 1.02, // Approximate 24h high
-		Low:          orderbook.LastPrice * 0.98, // Approximate 24h low
-		Volume:       adv / orderbook.LastPrice,  // Approximate volume
-		ADV:          adv,
-		CurrentPrice: orderbook.LastPrice,
-	}
-
-	vadrResult, err := me.vadrCalc.CalculateVADR(vadrInput, tier)
-	if err != nil {
-		// VADR calculation can fail with insufficient data - mark as failed
-		report.VadrOK = false
-		report.Details.VADRCurrent = 0.0
-		report.Details.VADRMinimum = tier.VADRMinimum
-	} else {
-		report.VadrOK = vadrResult.PassesGate
-		report.Details.VADRCurrent = vadrResult.Current
-		report.Details.VADRMinimum = vadrResult.EffectiveMin
-	}
-
-	// Evaluate venue health
-	venueHealth, err := me.healthMonitor.GetVenueHealth(venue)
-	if err != nil {
-		return nil, fmt.Errorf("venue health check failed: %w", err)
-	}
-
-	report.Details.VenueHealth = *venueHealth
-
-	// Determine overall execution feasibility
-	report.ExecutionFeasible = report.DepthOK && report.SpreadOK && report.VadrOK
-
-	// Generate failure reasons
-	if !report.DepthOK {
-		report.FailureReasons = append(report.FailureReasons,
-			fmt.Sprintf("insufficient depth: $%.0f < $%.0f required (%s)",
-				depthResult.TotalDepthUSD, tier.DepthMinUSD, tier.Name))
-	}
-
-	if !report.SpreadOK {
-		report.FailureReasons = append(report.FailureReasons,
-			fmt.Sprintf("spread too wide: %.1f bps > %.1f bps cap (%s)",
-				spreadResult.Current.SpreadBps, tier.SpreadCapBps, tier.Name))
-	}
-
-	if !report.VadrOK {
-		report.FailureReasons = append(report.FailureReasons,
-			fmt.Sprintf("VADR insufficient: %.3f < %.3f required (%s)",
-				report.Details.VADRCurrent, report.Details.VADRMinimum, tier.Name))
-	}
-
-	// Determine recommended action
-	if report.ExecutionFeasible && venueHealth.Healthy {
-		report.RecommendedAction = "proceed"
-	} else if report.ExecutionFeasible && venueHealth.Recommendation == "halve_size" {
-		report.RecommendedAction = "halve_size"
-	} else {
-		report.RecommendedAction = "defer"
-	}
-
-	// Add venue health issues to failure reasons
-	if !venueHealth.Healthy {
-		report.FailureReasons = append(report.FailureReasons,
-			fmt.Sprintf("venue %s unhealthy: %s", venue, venueHealth.Recommendation))
-	}
-
-	// Set processing metadata
-	report.Details.DataAge = time.Since(orderbook.Timestamp)
-	report.Details.ProcessingMs = time.Since(startTime).Milliseconds()
-
-	// Assess data quality
-	report.Details.DataQuality = me.assessDataQuality(orderbook, depthResult, spreadResult)
-
-	return report, nil
-}
-
-// GetLiquidityTier determines tier based on ADV
-func (me *MicrostructureEvaluator) GetLiquidityTier(adv float64) *LiquidityTier {
-	tier, _ := me.tierManager.GetTierByADV(adv)
-	return tier
-}
-
-// UpdateVenueHealth updates venue health metrics
-func (me *MicrostructureEvaluator) UpdateVenueHealth(venue string, health VenueHealthStatus) error {
-	// This would typically update internal venue health tracking
-	// For now, we'll record synthetic requests to simulate the health status
-	if !health.Healthy {
-		me.healthMonitor.RecordRequest(venue, "synthetic", health.LatencyP99Ms, false, 500, "simulated_error")
-	} else {
-		me.healthMonitor.RecordRequest(venue, "synthetic", health.LatencyP99Ms, true, 200, "")
-	}
-	return nil
-}
-
-// GetVenueHealth retrieves current venue health status
-func (me *MicrostructureEvaluator) GetVenueHealth(venue string) (*VenueHealthStatus, error) {
-	return me.healthMonitor.GetVenueHealth(venue)
-}
-
-// validateInputs performs input validation
-func (me *MicrostructureEvaluator) validateInputs(symbol, venue string, orderbook *OrderBookSnapshot, adv float64) error {
-	if symbol == "" {
-		return fmt.Errorf("symbol is required")
-	}
-
-	if venue == "" {
-		return fmt.Errorf("venue is required")
-	}
-
-	if orderbook == nil {
-		return fmt.Errorf("orderbook snapshot is required")
-	}
-
-	if adv <= 0 {
-		return fmt.Errorf("ADV must be positive, got %.2f", adv)
-	}
-
-	// Check if venue is supported
-	supported := false
-	for _, supportedVenue := range me.config.SupportedVenues {
-		if venue == supportedVenue {
-			supported = true
-			break
-		}
-	}
-	if !supported {
-		return fmt.Errorf("venue %s not supported, must be one of: %v", venue, me.config.SupportedVenues)
-	}
-
-	// Check data freshness
-	dataAge := time.Since(orderbook.Timestamp)
-	if dataAge > time.Duration(me.config.MaxDataAgeSeconds)*time.Second {
-		return fmt.Errorf("orderbook data too stale: %v old (max %ds)",
-			dataAge, me.config.MaxDataAgeSeconds)
-	}
-
-	// Check minimum book levels
-	if len(orderbook.Bids) < me.config.MinBookLevels || len(orderbook.Asks) < me.config.MinBookLevels {
-		return fmt.Errorf("insufficient order book levels: %d bids, %d asks (min %d each)",
-			len(orderbook.Bids), len(orderbook.Asks), me.config.MinBookLevels)
-	}
-
-	return nil
-}
-
-// assessDataQuality evaluates overall data quality
-func (me *MicrostructureEvaluator) assessDataQuality(orderbook *OrderBookSnapshot, depth *DepthResult, spread *SpreadResult) string {
-	score := 0
-	maxScore := 5
-
-	// Age quality
-	if time.Since(orderbook.Timestamp) < 2*time.Second {
-		score++
-	}
-
-	// Book quality
-	if orderbook.Metadata.BookQuality == "full" {
-		score++
-	} else if orderbook.Metadata.BookQuality == "partial" {
-		score += 0 // neutral
-	}
-
-	// Depth levels
-	totalLevels := depth.BidLevels + depth.AskLevels
-	if totalLevels >= 20 {
-		score++
-	} else if totalLevels >= 10 {
-		score += 0 // neutral
-	}
-
-	// Spread stability
-	if spread.SampleCount > 10 && spread.StdDevBps < 5.0 {
-		score++
-	}
-
-	// Overall balance
-	if depth.BidDepthUSD > 0 && depth.AskDepthUSD > 0 {
-		balance := depth.BidDepthUSD / (depth.BidDepthUSD + depth.AskDepthUSD)
-		if balance > 0.3 && balance < 0.7 { // Reasonably balanced
-			score++
-		}
-	}
-
-	switch {
-	case score >= 4:
-		return "excellent"
-	case score >= 3:
-		return "good"
-	default:
-		return "degraded"
-	}
-}
-
-// GetEvaluatorStats returns evaluator performance statistics
-func (me *MicrostructureEvaluator) GetEvaluatorStats() map[string]interface{} {
-	allVenueHealth, _ := me.healthMonitor.GetAllVenueHealth()
-	
-	healthyVenues := 0
-	for _, health := range allVenueHealth {
-		if health.Healthy {
-			healthyVenues++
-		}
-	}
-
-	return map[string]interface{}{
-		"config": map[string]interface{}{
-			"supported_venues": me.config.SupportedVenues,
-			"spread_window_s":  me.config.SpreadWindowSeconds,
-			"depth_window_s":   me.config.DepthWindowSeconds,
-			"max_data_age_s":   me.config.MaxDataAgeSeconds,
-		},
-		"venues": map[string]interface{}{
-			"total_monitored": len(allVenueHealth),
-			"healthy_count":   healthyVenues,
-			"health_rate":     float64(healthyVenues) / float64(len(allVenueHealth)),
-		},
-		"tiers": map[string]interface{}{
-			"tier_count": len(me.config.LiquidityTiers),
-			"tier_names": me.getTierNames(),
-		},
-		"vadr_history": me.vadrCalc.GetVADRHistoryStats(),
-	}
-}
-
-// getTierNames returns list of configured tier names
-func (me *MicrostructureEvaluator) getTierNames() []string {
-	names := make([]string, len(me.config.LiquidityTiers))
-	for i, tier := range me.config.LiquidityTiers {
-		names[i] = tier.Name
-	}
-	return names
-}
-
-// RecordVenueRequest records an API request for venue health tracking
-func (me *MicrostructureEvaluator) RecordVenueRequest(venue, endpoint string, latencyMs int64, success bool, statusCode int, errorCode string) {
-	me.healthMonitor.RecordRequest(venue, endpoint, latencyMs, success, statusCode, errorCode)
-}
-
-// ClearHistory clears all historical data (useful for testing)
-func (me *MicrostructureEvaluator) ClearHistory() {
-	me.spreadCalc.ClearHistory()
-	me.vadrCalc.ClearHistory()
-	me.healthMonitor.ClearHistory()
-}
\ No newline at end of file
+package microstructure
+
+import (
+	"context"
+	"fmt"
+	"time"
+)
+
+// MicrostructureEvaluator is the main implementation of the Evaluator interface
+type MicrostructureEvaluator struct {
+	config        *Config
+	depthCalc     *DepthCalculator
+	spreadCalc    *SpreadCalculator
+	vadrCalc      *VADRCalculator
+	tierManager   *LiquidityTierManager
+	healthMonitor *VenueHealthMonitor
+}
+
+// NewMicrostructureEvaluator creates a new microstructure evaluator
+func NewMicrostructureEvaluator(config *Config) *MicrostructureEvaluator {
+	if config == nil {
+		config = DefaultConfig()
+	}
+
+	return &MicrostructureEvaluator{
+		config:      config,
+		depthCalc:   NewDepthCalculator(config.DepthWindowSeconds),
+		spreadCalc:  NewSpreadCalculator(config.SpreadWindowSeconds),
+		vadrCalc:    NewVADRCalculator(),
+		tierManager: NewLiquidityTierManagerWithConfig(config.LiquidityTiers),
+		healthMonitor: NewVenueHealthMonitor(&VenueHealthConfig{
+			RejectRateThreshold: config.RejectRateThreshold,
+			LatencyThresholdMs:  config.LatencyThresholdMs,
+			ErrorRateThreshold:  config.ErrorRateThreshold,
+			WindowDuration:      15 * time.Minute,
+			MinSamplesForHealth: 10,
+			MaxHistorySize:      1000,
+		}),
+	}
+}
+
+// EvaluateGates performs comprehensive gate evaluation for a symbol/venue
+func (me *MicrostructureEvaluator) EvaluateGates(ctx context.Context, symbol, venue string, orderbook *OrderBookSnapshot, adv float64) (*GateReport, error) {
+	startTime := time.Now()
+
+	// Validate inputs
+	if err := me.validateInputs(symbol, venue, orderbook, adv); err != nil {
+		return nil, fmt.Errorf("input validation failed: %w", err)
+	}
+
+	// Determine liquidity tier
+	tier, err := me.tierManager.GetTierByADV(adv)
+	if err != nil {
+		return nil, fmt.Errorf("failed to determine tier for ADV %.0f: %w", adv, err)
+	}
+
+	// Initialize gate report
+	report := &GateReport{
+		Symbol:    symbol,
+		Venue:     venue,
+		Timestamp: orderbook.Timestamp,
+		Details: GateDetails{
+			LiquidityTier: tier.Name,
+			ADV:           adv,
+		},
+	}
+
+	// Evaluate depth gate
+	depthResult, err := me.depthCalc.CalculateDepth(orderbook)
+	if err != nil {
+		return nil, fmt.Errorf("depth calculation failed: %w", err)
+	}
+
+	report.DepthOK, _ = me.depthCalc.ValidateDepthRequirement(depthResult, tier)
+	report.Details.BidDepthUSD = depthResult.BidDepthUSD
+	report.Details.AskDepthUSD = depthResult.AskDepthUSD
+	report.Details.TotalDepthUSD = depthResult.TotalDepthUSD
+	report.Details.DepthRequiredUSD = tier.DepthMinUSD
+
+	// Evaluate spread gate
+	spreadResult, err := me.spreadCalc.CalculateSpread(orderbook)
+	if err != nil {
+		return nil, fmt.Errorf("spread calculation failed: %w", err)
+	}
+
+	report.SpreadOK, _ = me.spreadCalc.ValidateSpreadRequirement(spreadResult, tier)
+	report.Details.SpreadBps = spreadResult.Current.SpreadBps
+	report.Details.SpreadCapBps = tier.SpreadCapBps
+
+	// Evaluate VADR gate (requires additional input)
+	vadrInput := &VADRInput{
+		High:         orderbook.LastPrice * 1.02, // Approximate 24h high
+		Low:          orderbook.LastPrice * 0.98, // Approximate 24h low
+		Volume:       adv / orderbook.LastPrice,  // Approximate volume
+		ADV:          adv,
+		CurrentPrice: orderbook.LastPrice,
+	}
+
+	vadrResult, err := me.vadrCalc.CalculateVADR(vadrInput, tier)
+	if err != nil {
+		// VADR calculation can fail with insufficient data - mark as failed
+		report.VadrOK = false
+		report.Details.VADRCurrent = 0.0
+		report.Details.VADRMinimum = tier.VADRMinimum
+	} else {
+		report.VadrOK = vadrResult.PassesGate
+		report.Details.VADRCurrent = vadrResult.Current
+		report.Details.VADRMinimum = vadrResult.EffectiveMin
+	}
+
+	// Evaluate venue health
+	venueHealth, err := me.healthMonitor.GetVenueHealth(venue)
+	if err != nil {
+		return nil, fmt.Errorf("venue health check failed: %w", err)
+	}
+
+	report.Details.VenueHealth = *venueHealth
+
+	// Determine overall execution feasibility
+	report.ExecutionFeasible = report.DepthOK && report.SpreadOK && report.VadrOK
+
+	// Generate failure reasons
+	if !report.DepthOK {
+		report.FailureReasons = append(report.FailureReasons,
+			fmt.Sprintf("insufficient depth: $%.0f < $%.0f required (%s)",
+				depthResult.TotalDepthUSD, tier.DepthMinUSD, tier.Name))
+	}
+
+	if !report.SpreadOK {
+		report.FailureReasons = append(report.FailureReasons,
+			fmt.Sprintf("spread too wide: %.1f bps > %.1f bps cap (%s)",
+				spreadResult.Current.SpreadBps, tier.SpreadCapBps, tier.Name))
+	}
+
+	if !report.VadrOK {
+		report.FailureReasons = append(report.FailureReasons,
+			fmt.Sprintf("VADR insufficient: %.3f < %.3f required (%s)",
+				report.Details.VADRCurrent, report.Details.VADRMinimum, tier.Name))
+	}
+
+	// Determine recommended action
+	if report.ExecutionFeasible && venueHealth.Healthy {
+		report.RecommendedAction = "proceed"
+	} else if report.ExecutionFeasible && venueHealth.Recommendation == "halve_size" {
+		report.RecommendedAction = "halve_size"
+	} else {
+		report.RecommendedAction = "defer"
+	}
+
+	// Add venue health issues to failure reasons
+	if !venueHealth.Healthy {
+		report.FailureReasons = append(report.FailureReasons,
+			fmt.Sprintf("venue %s unhealthy: %s", venue, venueHealth.Recommendation))
+	}
+
+	// Set processing metadata
+	report.Details.DataAge = time.Since(orderbook.Timestamp)
+	report.Details.ProcessingMs = time.Since(startTime).Milliseconds()
+
+	// Assess data quality
+	report.Details.DataQuality = me.assessDataQuality(orderbook, depthResult, spreadResult)
+
+	return report, nil
+}
+
+// GetLiquidityTier determines tier based on ADV
+func (me *MicrostructureEvaluator) GetLiquidityTier(adv float64) *LiquidityTier {
+	tier, _ := me.tierManager.GetTierByADV(adv)
+	return tier
+}
+
+// UpdateVenueHealth updates venue health metrics
+func (me *MicrostructureEvaluator) UpdateVenueHealth(venue string, health VenueHealthStatus) error {
+	// This would typically update internal venue health tracking
+	// For now, we'll record synthetic requests to simulate the health status
+	if !health.Healthy {
+		me.healthMonitor.RecordRequest(venue, "synthetic", health.LatencyP99Ms, false, 500, "simulated_error")
+	} else {
+		me.healthMonitor.RecordRequest(venue, "synthetic", health.LatencyP99Ms, true, 200, "")
+	}
+	return nil
+}
+
+// GetVenueHealth retrieves current venue health status
+func (me *MicrostructureEvaluator) GetVenueHealth(venue string) (*VenueHealthStatus, error) {
+	return me.healthMonitor.GetVenueHealth(venue)
+}
+
+// validateInputs performs input validation
+func (me *MicrostructureEvaluator) validateInputs(symbol, venue string, orderbook *OrderBookSnapshot, adv float64) error {
+	if symbol == "" {
+		return fmt.Errorf("symbol is required")
+	}
+
+	if venue == "" {
+		return fmt.Errorf("venue is required")
+	}
+
+	if orderbook == nil {
+		return fmt.Errorf("orderbook snapshot is required")
+	}
+
+	if adv <= 0 {
+		return fmt.Errorf("ADV must be positive, got %.2f", adv)
+	}
+
+	// Check if venue is supported
+	supported := false
+	for _, supportedVenue := range me.config.SupportedVenues {
+		if venue == supportedVenue {
+			supported = true
+			break
+		}
+	}
+	if !supported {
+		return fmt.Errorf("venue %s not supported, must be one of: %v", venue, me.config.SupportedVenues)
+	}
+
+	// Check data freshness
+	dataAge := time.Since(orderbook.Timestamp)
+	if dataAge > time.Duration(me.config.MaxDataAgeSeconds)*time.Second {
+		return fmt.Errorf("orderbook data too stale: %v old (max %ds)",
+			dataAge, me.config.MaxDataAgeSeconds)
+	}
+
+	// Check minimum book levels
+	if len(orderbook.Bids) < me.config.MinBookLevels || len(orderbook.Asks) < me.config.MinBookLevels {
+		return fmt.Errorf("insufficient order book levels: %d bids, %d asks (min %d each)",
+			len(orderbook.Bids), len(orderbook.Asks), me.config.MinBookLevels)
+	}
+
+	return nil
+}
+
+// assessDataQuality evaluates overall data quality
+func (me *MicrostructureEvaluator) assessDataQuality(orderbook *OrderBookSnapshot, depth *DepthResult, spread *SpreadResult) string {
+	score := 0
+	maxScore := 5
+
+	// Age quality
+	if time.Since(orderbook.Timestamp) < 2*time.Second {
+		score++
+	}
+
+	// Book quality
+	if orderbook.Metadata.BookQuality == "full" {
+		score++
+	} else if orderbook.Metadata.BookQuality == "partial" {
+		score += 0 // neutral
+	}
+
+	// Depth levels
+	totalLevels := depth.BidLevels + depth.AskLevels
+	if totalLevels >= 20 {
+		score++
+	} else if totalLevels >= 10 {
+		score += 0 // neutral
+	}
+
+	// Spread stability
+	if spread.SampleCount > 10 && spread.StdDevBps < 5.0 {
+		score++
+	}
+
+	// Overall balance
+	if depth.BidDepthUSD > 0 && depth.AskDepthUSD > 0 {
+		balance := depth.BidDepthUSD / (depth.BidDepthUSD + depth.AskDepthUSD)
+		if balance > 0.3 && balance < 0.7 { // Reasonably balanced
+			score++
+		}
+	}
+
+	switch {
+	case score >= 4:
+		return "excellent"
+	case score >= 3:
+		return "good"
+	default:
+		return "degraded"
+	}
+}
+
+// GetEvaluatorStats returns evaluator performance statistics
+func (me *MicrostructureEvaluator) GetEvaluatorStats() map[string]interface{} {
+	allVenueHealth, _ := me.healthMonitor.GetAllVenueHealth()
+
+	healthyVenues := 0
+	for _, health := range allVenueHealth {
+		if health.Healthy {
+			healthyVenues++
+		}
+	}
+
+	return map[string]interface{}{
+		"config": map[string]interface{}{
+			"supported_venues": me.config.SupportedVenues,
+			"spread_window_s":  me.config.SpreadWindowSeconds,
+			"depth_window_s":   me.config.DepthWindowSeconds,
+			"max_data_age_s":   me.config.MaxDataAgeSeconds,
+		},
+		"venues": map[string]interface{}{
+			"total_monitored": len(allVenueHealth),
+			"healthy_count":   healthyVenues,
+			"health_rate":     float64(healthyVenues) / float64(len(allVenueHealth)),
+		},
+		"tiers": map[string]interface{}{
+			"tier_count": len(me.config.LiquidityTiers),
+			"tier_names": me.getTierNames(),
+		},
+		"vadr_history": me.vadrCalc.GetVADRHistoryStats(),
+	}
+}
+
+// getTierNames returns list of configured tier names
+func (me *MicrostructureEvaluator) getTierNames() []string {
+	names := make([]string, len(me.config.LiquidityTiers))
+	for i, tier := range me.config.LiquidityTiers {
+		names[i] = tier.Name
+	}
+	return names
+}
+
+// RecordVenueRequest records an API request for venue health tracking
+func (me *MicrostructureEvaluator) RecordVenueRequest(venue, endpoint string, latencyMs int64, success bool, statusCode int, errorCode string) {
+	me.healthMonitor.RecordRequest(venue, endpoint, latencyMs, success, statusCode, errorCode)
+}
+
+// ClearHistory clears all historical data (useful for testing)
+func (me *MicrostructureEvaluator) ClearHistory() {
+	me.spreadCalc.ClearHistory()
+	me.vadrCalc.ClearHistory()
+	me.healthMonitor.ClearHistory()
+}
diff --git a/internal/microstructure/evaluator_test.go b/internal/microstructure/evaluator_test.go
index 5e5ebd3..75f0985 100644
--- a/internal/microstructure/evaluator_test.go
+++ b/internal/microstructure/evaluator_test.go
@@ -1,289 +1,289 @@
-package microstructure
-
-import (
-	"context"
-	"testing"
-	"time"
-)
-
-// TestMicrostructureEvaluator tests the main evaluator with synthetic order books
-func TestMicrostructureEvaluator(t *testing.T) {
-	evaluator := NewMicrostructureEvaluator(DefaultConfig())
-	ctx := context.Background()
-
-	tests := []struct {
-		name        string
-		symbol      string
-		venue       string
-		orderbook   *OrderBookSnapshot
-		adv         float64
-		expectPass  bool
-		expectTier  string
-		description string
-	}{
-		{
-			name:   "tier1_btc_passes_all_gates",
-			symbol: "BTC-USD",
-			venue:  "binance",
-			orderbook: createTier1OrderBook("BTC-USD", 50000.0),
-			adv:    10000000, // $10M ADV
-			expectPass: true,
-			expectTier: "tier1",
-			description: "BTC with excellent liquidity should pass all Tier 1 gates",
-		},
-		{
-			name:   "tier2_eth_passes_gates",
-			symbol: "ETH-USD", 
-			venue:  "coinbase",
-			orderbook: createTier2OrderBook("ETH-USD", 3000.0),
-			adv:    3000000, // $3M ADV
-			expectPass: true,
-			expectTier: "tier2",
-			description: "ETH with good liquidity should pass Tier 2 gates",
-		},
-		{
-			name:   "tier3_smallcap_marginal",
-			symbol: "SMALL-USD",
-			venue:  "okx",
-			orderbook: createTier3OrderBook("SMALL-USD", 5.0),
-			adv:    500000, // $500k ADV
-			expectPass: true,
-			expectTier: "tier3",
-			description: "Small cap with minimal liquidity passes Tier 3 gates",
-		},
-		{
-			name:   "wide_spread_fails",
-			symbol: "WIDE-USD",
-			venue:  "binance",
-			orderbook: createWideSpreadOrderBook("WIDE-USD", 100.0),
-			adv:    2000000, // $2M ADV
-			expectPass: false,
-			expectTier: "tier2",
-			description: "Asset with excessive spread should fail gates",
-		},
-		{
-			name:   "thin_depth_fails",
-			symbol: "THIN-USD",
-			venue:  "coinbase",
-			orderbook: createThinDepthOrderBook("THIN-USD", 10.0),
-			adv:    1500000, // $1.5M ADV
-			expectPass: false,
-			expectTier: "tier2",
-			description: "Asset with insufficient depth should fail gates",
-		},
-	}
-
-	for _, tt := range tests {
-		t.Run(tt.name, func(t *testing.T) {
-			report, err := evaluator.EvaluateGates(ctx, tt.symbol, tt.venue, tt.orderbook, tt.adv)
-			if err != nil {
-				t.Fatalf("EvaluateGates failed: %v", err)
-			}
-
-			// Check tier assignment
-			if report.Details.LiquidityTier != tt.expectTier {
-				t.Errorf("Expected tier %s, got %s", tt.expectTier, report.Details.LiquidityTier)
-			}
-
-			// Check overall result
-			if report.ExecutionFeasible != tt.expectPass {
-				t.Errorf("Expected feasible=%v, got %v. Failures: %v", 
-					tt.expectPass, report.ExecutionFeasible, report.FailureReasons)
-			}
-
-			// Verify individual gates align with overall result
-			allGatesPass := report.DepthOK && report.SpreadOK && report.VadrOK
-			if tt.expectPass && !allGatesPass {
-				t.Errorf("Expected pass but gates failed: depth=%v, spread=%v, vadr=%v",
-					report.DepthOK, report.SpreadOK, report.VadrOK)
-			}
-
-			t.Logf("%s: %s (tier=%s, depth=$%.0f, spread=%.1fbps, vadr=%.3f)",
-				tt.description, 
-				map[bool]string{true: "PASS", false: "FAIL"}[report.ExecutionFeasible],
-				report.Details.LiquidityTier,
-				report.Details.TotalDepthUSD,
-				report.Details.SpreadBps,
-				report.Details.VADRCurrent)
-		})
-	}
-}
-
-// TestVenueHealthIntegration tests venue health impact on gate reports
-func TestVenueHealthIntegration(t *testing.T) {
-	evaluator := NewMicrostructureEvaluator(DefaultConfig())
-	ctx := context.Background()
-
-	// Simulate unhealthy venue
-	evaluator.healthMonitor.RecordRequest("binance", "orderbook", 3000, false, 500, "timeout")
-	evaluator.healthMonitor.RecordRequest("binance", "orderbook", 2500, false, 503, "unavailable") 
-	evaluator.healthMonitor.RecordRequest("binance", "orderbook", 4000, false, 429, "rate_limit")
-
-	orderbook := createTier1OrderBook("BTC-USD", 50000.0)
-	report, err := evaluator.EvaluateGates(ctx, "BTC-USD", "binance", orderbook, 10000000)
-	
-	if err != nil {
-		t.Fatalf("EvaluateGates failed: %v", err)
-	}
-
-	// Even if microstructure gates pass, unhealthy venue should recommend size reduction
-	if report.RecommendedAction != "halve_size" && report.RecommendedAction != "avoid" {
-		t.Errorf("Expected venue health to recommend size reduction, got: %s", report.RecommendedAction)
-	}
-
-	venueHealth := report.Details.VenueHealth
-	if venueHealth.Healthy {
-		t.Errorf("Expected venue to be unhealthy after error simulation")
-	}
-
-	t.Logf("Venue health: %v, recommendation: %s (reject_rate=%.1f%%, latency=%dms, error_rate=%.1f%%)",
-		venueHealth.Healthy, venueHealth.Recommendation, 
-		venueHealth.RejectRate, venueHealth.LatencyP99Ms, venueHealth.ErrorRate)
-}
-
-// TestLiquidityTiers tests tier assignment logic
-func TestLiquidityTiers(t *testing.T) {
-	tierManager := NewLiquidityTierManager()
-
-	testCases := []struct {
-		adv        float64
-		expectTier string
-	}{
-		{10000000, "tier1"}, // $10M
-		{3000000,  "tier2"}, // $3M  
-		{500000,   "tier3"}, // $500k
-		{50000,    "tier3"}, // $50k (below minimum, gets tier3)
-	}
-
-	for _, tc := range testCases {
-		tier, _ := tierManager.GetTierByADV(tc.adv)
-		if tier.Name != tc.expectTier {
-			t.Errorf("ADV $%.0f: expected tier %s, got %s", tc.adv, tc.expectTier, tier.Name)
-		}
-	}
-}
-
-// Synthetic order book generators
-
-func createTier1OrderBook(symbol string, midPrice float64) *OrderBookSnapshot {
-	return &OrderBookSnapshot{
-		Symbol:    symbol,
-		Venue:     "binance",
-		Timestamp: time.Now(),
-		LastPrice: midPrice,
-		// Tight spread (20 bps) with deep liquidity
-		Bids: []PriceLevel{
-			{Price: midPrice * 0.9990, Size: 100.0}, // $5M at best bid
-			{Price: midPrice * 0.9985, Size: 80.0},
-			{Price: midPrice * 0.9980, Size: 120.0}, // Deep within 2%
-			{Price: midPrice * 0.9975, Size: 150.0},
-			{Price: midPrice * 0.9900, Size: 200.0},
-		},
-		Asks: []PriceLevel{
-			{Price: midPrice * 1.0010, Size: 100.0}, // $5M at best ask  
-			{Price: midPrice * 1.0015, Size: 85.0},
-			{Price: midPrice * 1.0020, Size: 125.0}, // Deep within 2%
-			{Price: midPrice * 1.0025, Size: 160.0},
-			{Price: midPrice * 1.0100, Size: 180.0},
-		},
-		Metadata: SnapshotMetadata{
-			Source:      "binance",
-			IsStale:     false,
-			BookQuality: "full",
-		},
-	}
-}
-
-func createTier2OrderBook(symbol string, midPrice float64) *OrderBookSnapshot {
-	return &OrderBookSnapshot{
-		Symbol:    symbol,
-		Venue:     "coinbase",
-		Timestamp: time.Now(),
-		LastPrice: midPrice,
-		// Medium spread (45 bps) with adequate liquidity
-		Bids: []PriceLevel{
-			{Price: midPrice * 0.9977, Size: 30.0}, // ~$90k at best
-			{Price: midPrice * 0.9970, Size: 25.0}, 
-			{Price: midPrice * 0.9960, Size: 35.0},
-			{Price: midPrice * 0.9950, Size: 40.0},
-		},
-		Asks: []PriceLevel{
-			{Price: midPrice * 1.0023, Size: 28.0}, // ~$84k at best
-			{Price: midPrice * 1.0030, Size: 22.0},
-			{Price: midPrice * 1.0040, Size: 32.0}, 
-			{Price: midPrice * 1.0050, Size: 38.0},
-		},
-		Metadata: SnapshotMetadata{
-			Source:      "coinbase",
-			IsStale:     false,
-			BookQuality: "full",
-		},
-	}
-}
-
-func createTier3OrderBook(symbol string, midPrice float64) *OrderBookSnapshot {
-	return &OrderBookSnapshot{
-		Symbol:    symbol,
-		Venue:     "okx",
-		Timestamp: time.Now(),
-		LastPrice: midPrice,
-		// Wide spread (75 bps) with minimal liquidity
-		Bids: []PriceLevel{
-			{Price: midPrice * 0.9962, Size: 3000.0}, // ~$15k at best
-			{Price: midPrice * 0.9955, Size: 2800.0}, 
-			{Price: midPrice * 0.9945, Size: 3200.0},
-		},
-		Asks: []PriceLevel{
-			{Price: midPrice * 1.0038, Size: 2900.0}, // ~$14.5k at best
-			{Price: midPrice * 1.0045, Size: 2600.0},
-			{Price: midPrice * 1.0055, Size: 3100.0},
-		},
-		Metadata: SnapshotMetadata{
-			Source:      "okx",
-			IsStale:     false,
-			BookQuality: "partial",
-		},
-	}
-}
-
-func createWideSpreadOrderBook(symbol string, midPrice float64) *OrderBookSnapshot {
-	return &OrderBookSnapshot{
-		Symbol:    symbol,
-		Venue:     "binance",
-		Timestamp: time.Now(),
-		LastPrice: midPrice,
-		// Excessive spread (120 bps) - should fail spread gate
-		Bids: []PriceLevel{
-			{Price: midPrice * 0.9940, Size: 500.0}, // Wide spread
-		},
-		Asks: []PriceLevel{
-			{Price: midPrice * 1.0060, Size: 500.0}, 
-		},
-		Metadata: SnapshotMetadata{
-			Source:      "binance",
-			IsStale:     false,
-			BookQuality: "degraded",
-		},
-	}
-}
-
-func createThinDepthOrderBook(symbol string, midPrice float64) *OrderBookSnapshot {
-	return &OrderBookSnapshot{
-		Symbol:    symbol,
-		Venue:     "coinbase", 
-		Timestamp: time.Now(),
-		LastPrice: midPrice,
-		// Tight spread but very thin depth - should fail depth gate
-		Bids: []PriceLevel{
-			{Price: midPrice * 0.9995, Size: 50.0}, // Only $500 depth
-		},
-		Asks: []PriceLevel{
-			{Price: midPrice * 1.0005, Size: 50.0}, // Only $500 depth
-		},
-		Metadata: SnapshotMetadata{
-			Source:      "coinbase",
-			IsStale:     false,
-			BookQuality: "partial",
-		},
-	}
-}
\ No newline at end of file
+package microstructure
+
+import (
+	"context"
+	"testing"
+	"time"
+)
+
+// TestMicrostructureEvaluator tests the main evaluator with synthetic order books
+func TestMicrostructureEvaluator(t *testing.T) {
+	evaluator := NewMicrostructureEvaluator(DefaultConfig())
+	ctx := context.Background()
+
+	tests := []struct {
+		name        string
+		symbol      string
+		venue       string
+		orderbook   *OrderBookSnapshot
+		adv         float64
+		expectPass  bool
+		expectTier  string
+		description string
+	}{
+		{
+			name:        "tier1_btc_passes_all_gates",
+			symbol:      "BTC-USD",
+			venue:       "binance",
+			orderbook:   createTier1OrderBook("BTC-USD", 50000.0),
+			adv:         10000000, // $10M ADV
+			expectPass:  true,
+			expectTier:  "tier1",
+			description: "BTC with excellent liquidity should pass all Tier 1 gates",
+		},
+		{
+			name:        "tier2_eth_passes_gates",
+			symbol:      "ETH-USD",
+			venue:       "coinbase",
+			orderbook:   createTier2OrderBook("ETH-USD", 3000.0),
+			adv:         3000000, // $3M ADV
+			expectPass:  true,
+			expectTier:  "tier2",
+			description: "ETH with good liquidity should pass Tier 2 gates",
+		},
+		{
+			name:        "tier3_smallcap_marginal",
+			symbol:      "SMALL-USD",
+			venue:       "okx",
+			orderbook:   createTier3OrderBook("SMALL-USD", 5.0),
+			adv:         500000, // $500k ADV
+			expectPass:  true,
+			expectTier:  "tier3",
+			description: "Small cap with minimal liquidity passes Tier 3 gates",
+		},
+		{
+			name:        "wide_spread_fails",
+			symbol:      "WIDE-USD",
+			venue:       "binance",
+			orderbook:   createWideSpreadOrderBook("WIDE-USD", 100.0),
+			adv:         2000000, // $2M ADV
+			expectPass:  false,
+			expectTier:  "tier2",
+			description: "Asset with excessive spread should fail gates",
+		},
+		{
+			name:        "thin_depth_fails",
+			symbol:      "THIN-USD",
+			venue:       "coinbase",
+			orderbook:   createThinDepthOrderBook("THIN-USD", 10.0),
+			adv:         1500000, // $1.5M ADV
+			expectPass:  false,
+			expectTier:  "tier2",
+			description: "Asset with insufficient depth should fail gates",
+		},
+	}
+
+	for _, tt := range tests {
+		t.Run(tt.name, func(t *testing.T) {
+			report, err := evaluator.EvaluateGates(ctx, tt.symbol, tt.venue, tt.orderbook, tt.adv)
+			if err != nil {
+				t.Fatalf("EvaluateGates failed: %v", err)
+			}
+
+			// Check tier assignment
+			if report.Details.LiquidityTier != tt.expectTier {
+				t.Errorf("Expected tier %s, got %s", tt.expectTier, report.Details.LiquidityTier)
+			}
+
+			// Check overall result
+			if report.ExecutionFeasible != tt.expectPass {
+				t.Errorf("Expected feasible=%v, got %v. Failures: %v",
+					tt.expectPass, report.ExecutionFeasible, report.FailureReasons)
+			}
+
+			// Verify individual gates align with overall result
+			allGatesPass := report.DepthOK && report.SpreadOK && report.VadrOK
+			if tt.expectPass && !allGatesPass {
+				t.Errorf("Expected pass but gates failed: depth=%v, spread=%v, vadr=%v",
+					report.DepthOK, report.SpreadOK, report.VadrOK)
+			}
+
+			t.Logf("%s: %s (tier=%s, depth=$%.0f, spread=%.1fbps, vadr=%.3f)",
+				tt.description,
+				map[bool]string{true: "PASS", false: "FAIL"}[report.ExecutionFeasible],
+				report.Details.LiquidityTier,
+				report.Details.TotalDepthUSD,
+				report.Details.SpreadBps,
+				report.Details.VADRCurrent)
+		})
+	}
+}
+
+// TestVenueHealthIntegration tests venue health impact on gate reports
+func TestVenueHealthIntegration(t *testing.T) {
+	evaluator := NewMicrostructureEvaluator(DefaultConfig())
+	ctx := context.Background()
+
+	// Simulate unhealthy venue
+	evaluator.healthMonitor.RecordRequest("binance", "orderbook", 3000, false, 500, "timeout")
+	evaluator.healthMonitor.RecordRequest("binance", "orderbook", 2500, false, 503, "unavailable")
+	evaluator.healthMonitor.RecordRequest("binance", "orderbook", 4000, false, 429, "rate_limit")
+
+	orderbook := createTier1OrderBook("BTC-USD", 50000.0)
+	report, err := evaluator.EvaluateGates(ctx, "BTC-USD", "binance", orderbook, 10000000)
+
+	if err != nil {
+		t.Fatalf("EvaluateGates failed: %v", err)
+	}
+
+	// Even if microstructure gates pass, unhealthy venue should recommend size reduction
+	if report.RecommendedAction != "halve_size" && report.RecommendedAction != "avoid" {
+		t.Errorf("Expected venue health to recommend size reduction, got: %s", report.RecommendedAction)
+	}
+
+	venueHealth := report.Details.VenueHealth
+	if venueHealth.Healthy {
+		t.Errorf("Expected venue to be unhealthy after error simulation")
+	}
+
+	t.Logf("Venue health: %v, recommendation: %s (reject_rate=%.1f%%, latency=%dms, error_rate=%.1f%%)",
+		venueHealth.Healthy, venueHealth.Recommendation,
+		venueHealth.RejectRate, venueHealth.LatencyP99Ms, venueHealth.ErrorRate)
+}
+
+// TestLiquidityTiers tests tier assignment logic
+func TestLiquidityTiers(t *testing.T) {
+	tierManager := NewLiquidityTierManager()
+
+	testCases := []struct {
+		adv        float64
+		expectTier string
+	}{
+		{10000000, "tier1"}, // $10M
+		{3000000, "tier2"},  // $3M
+		{500000, "tier3"},   // $500k
+		{50000, "tier3"},    // $50k (below minimum, gets tier3)
+	}
+
+	for _, tc := range testCases {
+		tier, _ := tierManager.GetTierByADV(tc.adv)
+		if tier.Name != tc.expectTier {
+			t.Errorf("ADV $%.0f: expected tier %s, got %s", tc.adv, tc.expectTier, tier.Name)
+		}
+	}
+}
+
+// Synthetic order book generators
+
+func createTier1OrderBook(symbol string, midPrice float64) *OrderBookSnapshot {
+	return &OrderBookSnapshot{
+		Symbol:    symbol,
+		Venue:     "binance",
+		Timestamp: time.Now(),
+		LastPrice: midPrice,
+		// Tight spread (20 bps) with deep liquidity
+		Bids: []PriceLevel{
+			{Price: midPrice * 0.9990, Size: 100.0}, // $5M at best bid
+			{Price: midPrice * 0.9985, Size: 80.0},
+			{Price: midPrice * 0.9980, Size: 120.0}, // Deep within 2%
+			{Price: midPrice * 0.9975, Size: 150.0},
+			{Price: midPrice * 0.9900, Size: 200.0},
+		},
+		Asks: []PriceLevel{
+			{Price: midPrice * 1.0010, Size: 100.0}, // $5M at best ask
+			{Price: midPrice * 1.0015, Size: 85.0},
+			{Price: midPrice * 1.0020, Size: 125.0}, // Deep within 2%
+			{Price: midPrice * 1.0025, Size: 160.0},
+			{Price: midPrice * 1.0100, Size: 180.0},
+		},
+		Metadata: SnapshotMetadata{
+			Source:      "binance",
+			IsStale:     false,
+			BookQuality: "full",
+		},
+	}
+}
+
+func createTier2OrderBook(symbol string, midPrice float64) *OrderBookSnapshot {
+	return &OrderBookSnapshot{
+		Symbol:    symbol,
+		Venue:     "coinbase",
+		Timestamp: time.Now(),
+		LastPrice: midPrice,
+		// Medium spread (45 bps) with adequate liquidity
+		Bids: []PriceLevel{
+			{Price: midPrice * 0.9977, Size: 30.0}, // ~$90k at best
+			{Price: midPrice * 0.9970, Size: 25.0},
+			{Price: midPrice * 0.9960, Size: 35.0},
+			{Price: midPrice * 0.9950, Size: 40.0},
+		},
+		Asks: []PriceLevel{
+			{Price: midPrice * 1.0023, Size: 28.0}, // ~$84k at best
+			{Price: midPrice * 1.0030, Size: 22.0},
+			{Price: midPrice * 1.0040, Size: 32.0},
+			{Price: midPrice * 1.0050, Size: 38.0},
+		},
+		Metadata: SnapshotMetadata{
+			Source:      "coinbase",
+			IsStale:     false,
+			BookQuality: "full",
+		},
+	}
+}
+
+func createTier3OrderBook(symbol string, midPrice float64) *OrderBookSnapshot {
+	return &OrderBookSnapshot{
+		Symbol:    symbol,
+		Venue:     "okx",
+		Timestamp: time.Now(),
+		LastPrice: midPrice,
+		// Wide spread (75 bps) with minimal liquidity
+		Bids: []PriceLevel{
+			{Price: midPrice * 0.9962, Size: 3000.0}, // ~$15k at best
+			{Price: midPrice * 0.9955, Size: 2800.0},
+			{Price: midPrice * 0.9945, Size: 3200.0},
+		},
+		Asks: []PriceLevel{
+			{Price: midPrice * 1.0038, Size: 2900.0}, // ~$14.5k at best
+			{Price: midPrice * 1.0045, Size: 2600.0},
+			{Price: midPrice * 1.0055, Size: 3100.0},
+		},
+		Metadata: SnapshotMetadata{
+			Source:      "okx",
+			IsStale:     false,
+			BookQuality: "partial",
+		},
+	}
+}
+
+func createWideSpreadOrderBook(symbol string, midPrice float64) *OrderBookSnapshot {
+	return &OrderBookSnapshot{
+		Symbol:    symbol,
+		Venue:     "binance",
+		Timestamp: time.Now(),
+		LastPrice: midPrice,
+		// Excessive spread (120 bps) - should fail spread gate
+		Bids: []PriceLevel{
+			{Price: midPrice * 0.9940, Size: 500.0}, // Wide spread
+		},
+		Asks: []PriceLevel{
+			{Price: midPrice * 1.0060, Size: 500.0},
+		},
+		Metadata: SnapshotMetadata{
+			Source:      "binance",
+			IsStale:     false,
+			BookQuality: "degraded",
+		},
+	}
+}
+
+func createThinDepthOrderBook(symbol string, midPrice float64) *OrderBookSnapshot {
+	return &OrderBookSnapshot{
+		Symbol:    symbol,
+		Venue:     "coinbase",
+		Timestamp: time.Now(),
+		LastPrice: midPrice,
+		// Tight spread but very thin depth - should fail depth gate
+		Bids: []PriceLevel{
+			{Price: midPrice * 0.9995, Size: 50.0}, // Only $500 depth
+		},
+		Asks: []PriceLevel{
+			{Price: midPrice * 1.0005, Size: 50.0}, // Only $500 depth
+		},
+		Metadata: SnapshotMetadata{
+			Source:      "coinbase",
+			IsStale:     false,
+			BookQuality: "partial",
+		},
+	}
+}
diff --git a/internal/microstructure/liquidity_tiers.go b/internal/microstructure/liquidity_tiers.go
index ac76c3f..12ace7c 100644
--- a/internal/microstructure/liquidity_tiers.go
+++ b/internal/microstructure/liquidity_tiers.go
@@ -1,321 +1,321 @@
-package microstructure
-
-import (
-	"fmt"
-	"sort"
-)
-
-// LiquidityTierManager manages tiered liquidity requirements by ADV
-// Tiers: Tier1 ($5M+ ADV), Tier2 ($1-5M ADV), Tier3 ($100k-1M ADV)
-type LiquidityTierManager struct {
-	tiers []LiquidityTier
-}
-
-// NewLiquidityTierManager creates a tier manager with default configuration
-func NewLiquidityTierManager() *LiquidityTierManager {
-	return &LiquidityTierManager{
-		tiers: getDefaultLiquidityTiers(),
-	}
-}
-
-// NewLiquidityTierManagerWithConfig creates a tier manager with custom tiers
-func NewLiquidityTierManagerWithConfig(tiers []LiquidityTier) *LiquidityTierManager {
-	// Sort tiers by ADV minimum (descending)
-	sortedTiers := make([]LiquidityTier, len(tiers))
-	copy(sortedTiers, tiers)
-	
-	sort.Slice(sortedTiers, func(i, j int) bool {
-		return sortedTiers[i].ADVMin > sortedTiers[j].ADVMin
-	})
-	
-	return &LiquidityTierManager{
-		tiers: sortedTiers,
-	}
-}
-
-// getDefaultLiquidityTiers returns the standard 3-tier structure
-func getDefaultLiquidityTiers() []LiquidityTier {
-	return []LiquidityTier{
-		{
-			Name:         "tier1",
-			ADVMin:       5000000,  // $5M+ ADV
-			ADVMax:       1e12,     // No upper limit
-			DepthMinUSD:  150000,   // $150k depth within ¬±2%
-			SpreadCapBps: 25,       // 25 bps max spread
-			VADRMinimum:  1.85,     // 1.85√ó minimum VADR
-			Description:  "High liquidity: Large caps, stablecoins (BTC, ETH, USDT, etc.)",
-		},
-		{
-			Name:         "tier2",
-			ADVMin:       1000000,  // $1-5M ADV
-			ADVMax:       5000000,
-			DepthMinUSD:  75000,    // $75k depth within ¬±2%
-			SpreadCapBps: 50,       // 50 bps max spread
-			VADRMinimum:  1.80,     // 1.80√ó minimum VADR
-			Description:  "Medium liquidity: Mid caps (ADA, SOL, MATIC, etc.)",
-		},
-		{
-			Name:         "tier3",
-			ADVMin:       100000,   // $100k-1M ADV
-			ADVMax:       1000000,
-			DepthMinUSD:  25000,    // $25k depth within ¬±2%
-			SpreadCapBps: 80,       // 80 bps max spread
-			VADRMinimum:  1.75,     // 1.75√ó minimum VADR
-			Description:  "Lower liquidity: Small caps and newer tokens",
-		},
-	}
-}
-
-// GetTierByADV determines the appropriate tier based on Average Daily Volume
-func (ltm *LiquidityTierManager) GetTierByADV(adv float64) (*LiquidityTier, error) {
-	if adv < 0 {
-		return nil, fmt.Errorf("invalid ADV: %.2f (must be non-negative)", adv)
-	}
-	
-	// Find the highest tier that the ADV qualifies for
-	for _, tier := range ltm.tiers {
-		if adv >= tier.ADVMin && adv <= tier.ADVMax {
-			// Return a copy to prevent modification
-			tierCopy := tier
-			return &tierCopy, nil
-		}
-	}
-	
-	// If ADV is below all tiers, return the lowest tier with a warning
-	if len(ltm.tiers) > 0 {
-		lowestTier := ltm.tiers[len(ltm.tiers)-1]
-		tierCopy := lowestTier
-		return &tierCopy, fmt.Errorf("ADV %.0f below minimum tier requirement %.0f, using %s", 
-			adv, tierCopy.ADVMin, tierCopy.Name)
-	}
-	
-	return nil, fmt.Errorf("no tiers configured")
-}
-
-// GetTierByName retrieves a tier by its name
-func (ltm *LiquidityTierManager) GetTierByName(name string) (*LiquidityTier, error) {
-	for _, tier := range ltm.tiers {
-		if tier.Name == name {
-			tierCopy := tier
-			return &tierCopy, nil
-		}
-	}
-	
-	return nil, fmt.Errorf("tier not found: %s", name)
-}
-
-// GetAllTiers returns all configured tiers
-func (ltm *LiquidityTierManager) GetAllTiers() []LiquidityTier {
-	result := make([]LiquidityTier, len(ltm.tiers))
-	copy(result, ltm.tiers)
-	return result
-}
-
-// ValidateSymbolForTier checks if a symbol meets tier requirements
-func (ltm *LiquidityTierManager) ValidateSymbolForTier(symbol string, adv float64, depth, spread, vadr float64) (*TierValidationResult, error) {
-	tier, err := ltm.GetTierByADV(adv)
-	if err != nil {
-		return nil, fmt.Errorf("failed to determine tier: %w", err)
-	}
-	
-	validation := &TierValidationResult{
-		Symbol:      symbol,
-		ADV:         adv,
-		AssignedTier: tier.Name,
-		TierRequirements: *tier,
-		Measurements: TierMeasurements{
-			DepthUSD:  depth,
-			SpreadBps: spread,
-			VADR:      vadr,
-		},
-	}
-	
-	// Check each requirement
-	validation.DepthPass = depth >= tier.DepthMinUSD
-	validation.SpreadPass = spread <= tier.SpreadCapBps
-	validation.VADRPass = vadr >= tier.VADRMinimum
-	
-	// Overall pass requires all gates to pass
-	validation.OverallPass = validation.DepthPass && validation.SpreadPass && validation.VADRPass
-	
-	// Generate failure reasons
-	if !validation.DepthPass {
-		validation.FailureReasons = append(validation.FailureReasons,
-			fmt.Sprintf("depth $%.0f < $%.0f required", depth, tier.DepthMinUSD))
-	}
-	
-	if !validation.SpreadPass {
-		validation.FailureReasons = append(validation.FailureReasons,
-			fmt.Sprintf("spread %.1f bps > %.1f bps cap", spread, tier.SpreadCapBps))
-	}
-	
-	if !validation.VADRPass {
-		validation.FailureReasons = append(validation.FailureReasons,
-			fmt.Sprintf("VADR %.3f < %.3f minimum", vadr, tier.VADRMinimum))
-	}
-	
-	return validation, nil
-}
-
-// TierValidationResult contains validation results against tier requirements
-type TierValidationResult struct {
-	Symbol           string          `json:"symbol"`
-	ADV              float64         `json:"adv"`
-	AssignedTier     string          `json:"assigned_tier"`
-	TierRequirements LiquidityTier   `json:"tier_requirements"`
-	Measurements     TierMeasurements `json:"measurements"`
-	
-	// Gate results
-	DepthPass    bool `json:"depth_pass"`
-	SpreadPass   bool `json:"spread_pass"`
-	VADRPass     bool `json:"vadr_pass"`
-	OverallPass  bool `json:"overall_pass"`
-	
-	// Details
-	FailureReasons []string `json:"failure_reasons,omitempty"`
-}
-
-// TierMeasurements contains actual measurements for tier validation
-type TierMeasurements struct {
-	DepthUSD  float64 `json:"depth_usd"`
-	SpreadBps float64 `json:"spread_bps"`
-	VADR      float64 `json:"vadr"`
-}
-
-// GetTierSummary returns a human-readable tier summary
-func (ltm *LiquidityTierManager) GetTierSummary() []string {
-	summary := make([]string, len(ltm.tiers))
-	
-	for i, tier := range ltm.tiers {
-		advRange := fmt.Sprintf("$%.0f+", tier.ADVMin)
-		if tier.ADVMax < 1e12 {
-			advRange = fmt.Sprintf("$%.0f-$%.0f", tier.ADVMin, tier.ADVMax)
-		}
-		
-		summary[i] = fmt.Sprintf("%s: %s ADV, $%.0fk depth, %.0f bps spread, %.2f√ó VADR",
-			tier.Name,
-			advRange,
-			tier.DepthMinUSD/1000,
-			tier.SpreadCapBps,
-			tier.VADRMinimum)
-	}
-	
-	return summary
-}
-
-// GetTierForSymbolClass returns appropriate tier for common symbol classes
-func (ltm *LiquidityTierManager) GetTierForSymbolClass(symbolClass string) (*LiquidityTier, error) {
-	var targetTier string
-	
-	switch symbolClass {
-	case "major", "stablecoin", "btc", "eth":
-		targetTier = "tier1"
-	case "altcoin", "defi", "layer1":
-		targetTier = "tier2"
-	case "smallcap", "new", "memecoin":
-		targetTier = "tier3"
-	default:
-		return nil, fmt.Errorf("unknown symbol class: %s", symbolClass)
-	}
-	
-	return ltm.GetTierByName(targetTier)
-}
-
-// EstimatePositionSize estimates maximum position size for a tier
-func (ltm *LiquidityTierManager) EstimatePositionSize(tier *LiquidityTier, venueHealthy bool) *PositionSizeEstimate {
-	if tier == nil {
-		return &PositionSizeEstimate{
-			Error: "no tier provided",
-		}
-	}
-	
-	// Base position size on depth requirement
-	baseSize := tier.DepthMinUSD * 0.8 // Use 80% of minimum depth as base
-	
-	// Apply venue health adjustment
-	adjustedSize := baseSize
-	sizeAdjustment := "full_size"
-	
-	if !venueHealthy {
-		adjustedSize = baseSize * 0.5 // Halve size for unhealthy venues
-		sizeAdjustment = "halve_size"
-	}
-	
-	return &PositionSizeEstimate{
-		TierName:        tier.Name,
-		BaseSize:        baseSize,
-		AdjustedSize:    adjustedSize,
-		VenueHealthy:    venueHealthy,
-		SizeAdjustment:  sizeAdjustment,
-		DepthUtilization: (adjustedSize / tier.DepthMinUSD) * 100,
-		Reasoning:       fmt.Sprintf("Base %.0f (80%% of $%.0fk depth), %s due to venue health", 
-			baseSize, tier.DepthMinUSD/1000, sizeAdjustment),
-	}
-}
-
-// PositionSizeEstimate contains position sizing recommendations
-type PositionSizeEstimate struct {
-	TierName         string  `json:"tier_name"`
-	BaseSize         float64 `json:"base_size"`         // Base position size (USD)
-	AdjustedSize     float64 `json:"adjusted_size"`     // Adjusted for venue health
-	VenueHealthy     bool    `json:"venue_healthy"`     // Venue health status
-	SizeAdjustment   string  `json:"size_adjustment"`   // "full_size", "halve_size"
-	DepthUtilization float64 `json:"depth_utilization"` // % of depth being used
-	Reasoning        string  `json:"reasoning"`         // Human-readable reasoning
-	Error            string  `json:"error,omitempty"`   // Error message if any
-}
-
-// CompareSymbolTiers compares two symbols across tiers
-func (ltm *LiquidityTierManager) CompareSymbolTiers(symbol1 string, adv1 float64, symbol2 string, adv2 float64) (*TierComparison, error) {
-	tier1, err1 := ltm.GetTierByADV(adv1)
-	tier2, err2 := ltm.GetTierByADV(adv2)
-	
-	comparison := &TierComparison{
-		Symbol1: symbol1,
-		Symbol2: symbol2,
-		ADV1:    adv1,
-		ADV2:    adv2,
-	}
-	
-	if err1 != nil {
-		comparison.Error = fmt.Sprintf("tier1 error: %v", err1)
-		return comparison, err1
-	}
-	
-	if err2 != nil {
-		comparison.Error = fmt.Sprintf("tier2 error: %v", err2)
-		return comparison, err2
-	}
-	
-	comparison.Tier1 = tier1.Name
-	comparison.Tier2 = tier2.Name
-	comparison.SameTier = tier1.Name == tier2.Name
-	
-	// Determine which has better liquidity
-	if adv1 > adv2 {
-		comparison.BetterLiquidity = symbol1
-		comparison.LiquidityAdvantage = (adv1 - adv2) / adv2 * 100
-	} else if adv2 > adv1 {
-		comparison.BetterLiquidity = symbol2
-		comparison.LiquidityAdvantage = (adv2 - adv1) / adv1 * 100
-	} else {
-		comparison.BetterLiquidity = "equal"
-		comparison.LiquidityAdvantage = 0
-	}
-	
-	return comparison, nil
-}
-
-// TierComparison contains comparison results between two symbols
-type TierComparison struct {
-	Symbol1            string  `json:"symbol1"`
-	Symbol2            string  `json:"symbol2"`
-	ADV1               float64 `json:"adv1"`
-	ADV2               float64 `json:"adv2"`
-	Tier1              string  `json:"tier1"`
-	Tier2              string  `json:"tier2"`
-	SameTier           bool    `json:"same_tier"`
-	BetterLiquidity    string  `json:"better_liquidity"`    // Which symbol has better liquidity
-	LiquidityAdvantage float64 `json:"liquidity_advantage"` // Percentage advantage
-	Error              string  `json:"error,omitempty"`
-}
\ No newline at end of file
+package microstructure
+
+import (
+	"fmt"
+	"sort"
+)
+
+// LiquidityTierManager manages tiered liquidity requirements by ADV
+// Tiers: Tier1 ($5M+ ADV), Tier2 ($1-5M ADV), Tier3 ($100k-1M ADV)
+type LiquidityTierManager struct {
+	tiers []LiquidityTier
+}
+
+// NewLiquidityTierManager creates a tier manager with default configuration
+func NewLiquidityTierManager() *LiquidityTierManager {
+	return &LiquidityTierManager{
+		tiers: getDefaultLiquidityTiers(),
+	}
+}
+
+// NewLiquidityTierManagerWithConfig creates a tier manager with custom tiers
+func NewLiquidityTierManagerWithConfig(tiers []LiquidityTier) *LiquidityTierManager {
+	// Sort tiers by ADV minimum (descending)
+	sortedTiers := make([]LiquidityTier, len(tiers))
+	copy(sortedTiers, tiers)
+
+	sort.Slice(sortedTiers, func(i, j int) bool {
+		return sortedTiers[i].ADVMin > sortedTiers[j].ADVMin
+	})
+
+	return &LiquidityTierManager{
+		tiers: sortedTiers,
+	}
+}
+
+// getDefaultLiquidityTiers returns the standard 3-tier structure
+func getDefaultLiquidityTiers() []LiquidityTier {
+	return []LiquidityTier{
+		{
+			Name:         "tier1",
+			ADVMin:       5000000, // $5M+ ADV
+			ADVMax:       1e12,    // No upper limit
+			DepthMinUSD:  150000,  // $150k depth within ¬±2%
+			SpreadCapBps: 25,      // 25 bps max spread
+			VADRMinimum:  1.85,    // 1.85√ó minimum VADR
+			Description:  "High liquidity: Large caps, stablecoins (BTC, ETH, USDT, etc.)",
+		},
+		{
+			Name:         "tier2",
+			ADVMin:       1000000, // $1-5M ADV
+			ADVMax:       5000000,
+			DepthMinUSD:  75000, // $75k depth within ¬±2%
+			SpreadCapBps: 50,    // 50 bps max spread
+			VADRMinimum:  1.80,  // 1.80√ó minimum VADR
+			Description:  "Medium liquidity: Mid caps (ADA, SOL, MATIC, etc.)",
+		},
+		{
+			Name:         "tier3",
+			ADVMin:       100000, // $100k-1M ADV
+			ADVMax:       1000000,
+			DepthMinUSD:  25000, // $25k depth within ¬±2%
+			SpreadCapBps: 80,    // 80 bps max spread
+			VADRMinimum:  1.75,  // 1.75√ó minimum VADR
+			Description:  "Lower liquidity: Small caps and newer tokens",
+		},
+	}
+}
+
+// GetTierByADV determines the appropriate tier based on Average Daily Volume
+func (ltm *LiquidityTierManager) GetTierByADV(adv float64) (*LiquidityTier, error) {
+	if adv < 0 {
+		return nil, fmt.Errorf("invalid ADV: %.2f (must be non-negative)", adv)
+	}
+
+	// Find the highest tier that the ADV qualifies for
+	for _, tier := range ltm.tiers {
+		if adv >= tier.ADVMin && adv <= tier.ADVMax {
+			// Return a copy to prevent modification
+			tierCopy := tier
+			return &tierCopy, nil
+		}
+	}
+
+	// If ADV is below all tiers, return the lowest tier with a warning
+	if len(ltm.tiers) > 0 {
+		lowestTier := ltm.tiers[len(ltm.tiers)-1]
+		tierCopy := lowestTier
+		return &tierCopy, fmt.Errorf("ADV %.0f below minimum tier requirement %.0f, using %s",
+			adv, tierCopy.ADVMin, tierCopy.Name)
+	}
+
+	return nil, fmt.Errorf("no tiers configured")
+}
+
+// GetTierByName retrieves a tier by its name
+func (ltm *LiquidityTierManager) GetTierByName(name string) (*LiquidityTier, error) {
+	for _, tier := range ltm.tiers {
+		if tier.Name == name {
+			tierCopy := tier
+			return &tierCopy, nil
+		}
+	}
+
+	return nil, fmt.Errorf("tier not found: %s", name)
+}
+
+// GetAllTiers returns all configured tiers
+func (ltm *LiquidityTierManager) GetAllTiers() []LiquidityTier {
+	result := make([]LiquidityTier, len(ltm.tiers))
+	copy(result, ltm.tiers)
+	return result
+}
+
+// ValidateSymbolForTier checks if a symbol meets tier requirements
+func (ltm *LiquidityTierManager) ValidateSymbolForTier(symbol string, adv float64, depth, spread, vadr float64) (*TierValidationResult, error) {
+	tier, err := ltm.GetTierByADV(adv)
+	if err != nil {
+		return nil, fmt.Errorf("failed to determine tier: %w", err)
+	}
+
+	validation := &TierValidationResult{
+		Symbol:           symbol,
+		ADV:              adv,
+		AssignedTier:     tier.Name,
+		TierRequirements: *tier,
+		Measurements: TierMeasurements{
+			DepthUSD:  depth,
+			SpreadBps: spread,
+			VADR:      vadr,
+		},
+	}
+
+	// Check each requirement
+	validation.DepthPass = depth >= tier.DepthMinUSD
+	validation.SpreadPass = spread <= tier.SpreadCapBps
+	validation.VADRPass = vadr >= tier.VADRMinimum
+
+	// Overall pass requires all gates to pass
+	validation.OverallPass = validation.DepthPass && validation.SpreadPass && validation.VADRPass
+
+	// Generate failure reasons
+	if !validation.DepthPass {
+		validation.FailureReasons = append(validation.FailureReasons,
+			fmt.Sprintf("depth $%.0f < $%.0f required", depth, tier.DepthMinUSD))
+	}
+
+	if !validation.SpreadPass {
+		validation.FailureReasons = append(validation.FailureReasons,
+			fmt.Sprintf("spread %.1f bps > %.1f bps cap", spread, tier.SpreadCapBps))
+	}
+
+	if !validation.VADRPass {
+		validation.FailureReasons = append(validation.FailureReasons,
+			fmt.Sprintf("VADR %.3f < %.3f minimum", vadr, tier.VADRMinimum))
+	}
+
+	return validation, nil
+}
+
+// TierValidationResult contains validation results against tier requirements
+type TierValidationResult struct {
+	Symbol           string           `json:"symbol"`
+	ADV              float64          `json:"adv"`
+	AssignedTier     string           `json:"assigned_tier"`
+	TierRequirements LiquidityTier    `json:"tier_requirements"`
+	Measurements     TierMeasurements `json:"measurements"`
+
+	// Gate results
+	DepthPass   bool `json:"depth_pass"`
+	SpreadPass  bool `json:"spread_pass"`
+	VADRPass    bool `json:"vadr_pass"`
+	OverallPass bool `json:"overall_pass"`
+
+	// Details
+	FailureReasons []string `json:"failure_reasons,omitempty"`
+}
+
+// TierMeasurements contains actual measurements for tier validation
+type TierMeasurements struct {
+	DepthUSD  float64 `json:"depth_usd"`
+	SpreadBps float64 `json:"spread_bps"`
+	VADR      float64 `json:"vadr"`
+}
+
+// GetTierSummary returns a human-readable tier summary
+func (ltm *LiquidityTierManager) GetTierSummary() []string {
+	summary := make([]string, len(ltm.tiers))
+
+	for i, tier := range ltm.tiers {
+		advRange := fmt.Sprintf("$%.0f+", tier.ADVMin)
+		if tier.ADVMax < 1e12 {
+			advRange = fmt.Sprintf("$%.0f-$%.0f", tier.ADVMin, tier.ADVMax)
+		}
+
+		summary[i] = fmt.Sprintf("%s: %s ADV, $%.0fk depth, %.0f bps spread, %.2f√ó VADR",
+			tier.Name,
+			advRange,
+			tier.DepthMinUSD/1000,
+			tier.SpreadCapBps,
+			tier.VADRMinimum)
+	}
+
+	return summary
+}
+
+// GetTierForSymbolClass returns appropriate tier for common symbol classes
+func (ltm *LiquidityTierManager) GetTierForSymbolClass(symbolClass string) (*LiquidityTier, error) {
+	var targetTier string
+
+	switch symbolClass {
+	case "major", "stablecoin", "btc", "eth":
+		targetTier = "tier1"
+	case "altcoin", "defi", "layer1":
+		targetTier = "tier2"
+	case "smallcap", "new", "memecoin":
+		targetTier = "tier3"
+	default:
+		return nil, fmt.Errorf("unknown symbol class: %s", symbolClass)
+	}
+
+	return ltm.GetTierByName(targetTier)
+}
+
+// EstimatePositionSize estimates maximum position size for a tier
+func (ltm *LiquidityTierManager) EstimatePositionSize(tier *LiquidityTier, venueHealthy bool) *PositionSizeEstimate {
+	if tier == nil {
+		return &PositionSizeEstimate{
+			Error: "no tier provided",
+		}
+	}
+
+	// Base position size on depth requirement
+	baseSize := tier.DepthMinUSD * 0.8 // Use 80% of minimum depth as base
+
+	// Apply venue health adjustment
+	adjustedSize := baseSize
+	sizeAdjustment := "full_size"
+
+	if !venueHealthy {
+		adjustedSize = baseSize * 0.5 // Halve size for unhealthy venues
+		sizeAdjustment = "halve_size"
+	}
+
+	return &PositionSizeEstimate{
+		TierName:         tier.Name,
+		BaseSize:         baseSize,
+		AdjustedSize:     adjustedSize,
+		VenueHealthy:     venueHealthy,
+		SizeAdjustment:   sizeAdjustment,
+		DepthUtilization: (adjustedSize / tier.DepthMinUSD) * 100,
+		Reasoning: fmt.Sprintf("Base %.0f (80%% of $%.0fk depth), %s due to venue health",
+			baseSize, tier.DepthMinUSD/1000, sizeAdjustment),
+	}
+}
+
+// PositionSizeEstimate contains position sizing recommendations
+type PositionSizeEstimate struct {
+	TierName         string  `json:"tier_name"`
+	BaseSize         float64 `json:"base_size"`         // Base position size (USD)
+	AdjustedSize     float64 `json:"adjusted_size"`     // Adjusted for venue health
+	VenueHealthy     bool    `json:"venue_healthy"`     // Venue health status
+	SizeAdjustment   string  `json:"size_adjustment"`   // "full_size", "halve_size"
+	DepthUtilization float64 `json:"depth_utilization"` // % of depth being used
+	Reasoning        string  `json:"reasoning"`         // Human-readable reasoning
+	Error            string  `json:"error,omitempty"`   // Error message if any
+}
+
+// CompareSymbolTiers compares two symbols across tiers
+func (ltm *LiquidityTierManager) CompareSymbolTiers(symbol1 string, adv1 float64, symbol2 string, adv2 float64) (*TierComparison, error) {
+	tier1, err1 := ltm.GetTierByADV(adv1)
+	tier2, err2 := ltm.GetTierByADV(adv2)
+
+	comparison := &TierComparison{
+		Symbol1: symbol1,
+		Symbol2: symbol2,
+		ADV1:    adv1,
+		ADV2:    adv2,
+	}
+
+	if err1 != nil {
+		comparison.Error = fmt.Sprintf("tier1 error: %v", err1)
+		return comparison, err1
+	}
+
+	if err2 != nil {
+		comparison.Error = fmt.Sprintf("tier2 error: %v", err2)
+		return comparison, err2
+	}
+
+	comparison.Tier1 = tier1.Name
+	comparison.Tier2 = tier2.Name
+	comparison.SameTier = tier1.Name == tier2.Name
+
+	// Determine which has better liquidity
+	if adv1 > adv2 {
+		comparison.BetterLiquidity = symbol1
+		comparison.LiquidityAdvantage = (adv1 - adv2) / adv2 * 100
+	} else if adv2 > adv1 {
+		comparison.BetterLiquidity = symbol2
+		comparison.LiquidityAdvantage = (adv2 - adv1) / adv1 * 100
+	} else {
+		comparison.BetterLiquidity = "equal"
+		comparison.LiquidityAdvantage = 0
+	}
+
+	return comparison, nil
+}
+
+// TierComparison contains comparison results between two symbols
+type TierComparison struct {
+	Symbol1            string  `json:"symbol1"`
+	Symbol2            string  `json:"symbol2"`
+	ADV1               float64 `json:"adv1"`
+	ADV2               float64 `json:"adv2"`
+	Tier1              string  `json:"tier1"`
+	Tier2              string  `json:"tier2"`
+	SameTier           bool    `json:"same_tier"`
+	BetterLiquidity    string  `json:"better_liquidity"`    // Which symbol has better liquidity
+	LiquidityAdvantage float64 `json:"liquidity_advantage"` // Percentage advantage
+	Error              string  `json:"error,omitempty"`
+}
diff --git a/internal/microstructure/spread.go b/internal/microstructure/spread.go
index e0299d9..2716db6 100644
--- a/internal/microstructure/spread.go
+++ b/internal/microstructure/spread.go
@@ -1,295 +1,295 @@
-package microstructure
-
-import (
-	"fmt"
-	"math"
-	"time"
-)
-
-// SpreadCalculator computes bid-ask spreads with rolling averages
-type SpreadCalculator struct {
-	windowSeconds int
-	history       []SpreadPoint
-	maxHistory    int
-}
-
-// NewSpreadCalculator creates a spread calculator with 60s rolling window
-func NewSpreadCalculator(windowSeconds int) *SpreadCalculator {
-	maxHistory := windowSeconds * 2 // Keep 2√ó window for safety
-	if maxHistory < 100 {
-		maxHistory = 100
-	}
-	
-	return &SpreadCalculator{
-		windowSeconds: windowSeconds,
-		maxHistory:    maxHistory,
-		history:       make([]SpreadPoint, 0, maxHistory),
-	}
-}
-
-// SpreadPoint tracks spread at a specific point in time
-type SpreadPoint struct {
-	Timestamp     time.Time `json:"timestamp"`
-	BidPrice      float64   `json:"bid_price"`
-	AskPrice      float64   `json:"ask_price"`
-	SpreadAbs     float64   `json:"spread_abs"`     // Absolute spread (ask - bid)
-	SpreadBps     float64   `json:"spread_bps"`     // Spread in basis points
-	MidPrice      float64   `json:"mid_price"`      // (bid + ask) / 2
-	LastPrice     float64   `json:"last_price"`     // Last trade price
-}
-
-// SpreadResult contains comprehensive spread analysis
-type SpreadResult struct {
-	Current       SpreadPoint `json:"current"`        // Latest measurement
-	RollingAvgBps float64     `json:"rolling_avg_bps"` // 60s rolling average
-	MinBps        float64     `json:"min_bps"`        // Minimum in window
-	MaxBps        float64     `json:"max_bps"`        // Maximum in window
-	StdDevBps     float64     `json:"std_dev_bps"`    // Standard deviation
-	SampleCount   int         `json:"sample_count"`   // Samples in window
-	WindowSeconds int         `json:"window_seconds"` // Rolling window size
-	DataQuality   string      `json:"data_quality"`   // "excellent", "good", "sparse"
-}
-
-// CalculateSpread computes current spread and updates rolling average
-func (sc *SpreadCalculator) CalculateSpread(orderbook *OrderBookSnapshot) (*SpreadResult, error) {
-	if orderbook == nil {
-		return nil, fmt.Errorf("order book snapshot is nil")
-	}
-
-	if len(orderbook.Bids) == 0 || len(orderbook.Asks) == 0 {
-		return nil, fmt.Errorf("incomplete order book: %d bids, %d asks", 
-			len(orderbook.Bids), len(orderbook.Asks))
-	}
-
-	// Get best bid and ask
-	bestBid := orderbook.Bids[0] // Highest bid
-	bestAsk := orderbook.Asks[0] // Lowest ask
-
-	if bestBid.Price <= 0 || bestAsk.Price <= 0 {
-		return nil, fmt.Errorf("invalid prices: bid=%.6f, ask=%.6f", 
-			bestBid.Price, bestAsk.Price)
-	}
-
-	if bestAsk.Price <= bestBid.Price {
-		return nil, fmt.Errorf("crossed book: bid=%.6f >= ask=%.6f", 
-			bestBid.Price, bestAsk.Price)
-	}
-
-	// Calculate current spread
-	spreadAbs := bestAsk.Price - bestBid.Price
-	midPrice := (bestBid.Price + bestAsk.Price) / 2.0
-	spreadBps := (spreadAbs / midPrice) * 10000.0
-
-	currentPoint := SpreadPoint{
-		Timestamp: orderbook.Timestamp,
-		BidPrice:  bestBid.Price,
-		AskPrice:  bestAsk.Price,
-		SpreadAbs: spreadAbs,
-		SpreadBps: spreadBps,
-		MidPrice:  midPrice,
-		LastPrice: orderbook.LastPrice,
-	}
-
-	// Add to history
-	sc.addToHistory(currentPoint)
-
-	// Calculate rolling statistics
-	result := sc.calculateRollingStats(currentPoint)
-	
-	return result, nil
-}
-
-// addToHistory adds a spread point and manages history size
-func (sc *SpreadCalculator) addToHistory(point SpreadPoint) {
-	sc.history = append(sc.history, point)
-	
-	// Trim to max history size
-	if len(sc.history) > sc.maxHistory {
-		sc.history = sc.history[1:]
-	}
-}
-
-// calculateRollingStats computes rolling statistics over the window
-func (sc *SpreadCalculator) calculateRollingStats(current SpreadPoint) *SpreadResult {
-	cutoff := current.Timestamp.Add(-time.Duration(sc.windowSeconds) * time.Second)
-	
-	// Filter to window
-	var windowPoints []SpreadPoint
-	for _, point := range sc.history {
-		if point.Timestamp.After(cutoff) {
-			windowPoints = append(windowPoints, point)
-		}
-	}
-	
-	if len(windowPoints) == 0 {
-		// No history, use current point
-		return &SpreadResult{
-			Current:       current,
-			RollingAvgBps: current.SpreadBps,
-			MinBps:        current.SpreadBps,
-			MaxBps:        current.SpreadBps,
-			StdDevBps:     0.0,
-			SampleCount:   1,
-			WindowSeconds: sc.windowSeconds,
-			DataQuality:   "sparse",
-		}
-	}
-
-	// Calculate statistics
-	sum := 0.0
-	minBps := math.Inf(1)
-	maxBps := math.Inf(-1)
-	
-	for _, point := range windowPoints {
-		sum += point.SpreadBps
-		if point.SpreadBps < minBps {
-			minBps = point.SpreadBps
-		}
-		if point.SpreadBps > maxBps {
-			maxBps = point.SpreadBps
-		}
-	}
-	
-	avgBps := sum / float64(len(windowPoints))
-	
-	// Calculate standard deviation
-	sumSquares := 0.0
-	for _, point := range windowPoints {
-		diff := point.SpreadBps - avgBps
-		sumSquares += diff * diff
-	}
-	stdDevBps := math.Sqrt(sumSquares / float64(len(windowPoints)))
-	
-	// Assess data quality
-	dataQuality := "excellent"
-	samplesPerSecond := float64(len(windowPoints)) / float64(sc.windowSeconds)
-	if samplesPerSecond < 0.1 {
-		dataQuality = "sparse"
-	} else if samplesPerSecond < 0.5 {
-		dataQuality = "good"
-	}
-	
-	return &SpreadResult{
-		Current:       current,
-		RollingAvgBps: avgBps,
-		MinBps:        minBps,
-		MaxBps:        maxBps,
-		StdDevBps:     stdDevBps,
-		SampleCount:   len(windowPoints),
-		WindowSeconds: sc.windowSeconds,
-		DataQuality:   dataQuality,
-	}
-}
-
-// ValidateSpreadRequirement checks if spread meets tier cap
-func (sc *SpreadCalculator) ValidateSpreadRequirement(spreadResult *SpreadResult, tier *LiquidityTier) (bool, string) {
-	if spreadResult == nil || tier == nil {
-		return false, "invalid inputs"
-	}
-	
-	// Use rolling average for stability, fall back to current if no history
-	spreadToCheck := spreadResult.RollingAvgBps
-	if spreadResult.SampleCount <= 1 {
-		spreadToCheck = spreadResult.Current.SpreadBps
-	}
-	
-	if spreadToCheck <= tier.SpreadCapBps {
-		return true, fmt.Sprintf("spread %.1f bps ‚â§ %.1f bps (%s cap)", 
-			spreadToCheck, tier.SpreadCapBps, tier.Name)
-	}
-	
-	return false, fmt.Sprintf("spread too wide: %.1f bps > %.1f bps (%s cap)", 
-		spreadToCheck, tier.SpreadCapBps, tier.Name)
-}
-
-// GetSpreadSummary returns human-readable spread summary
-func (sc *SpreadCalculator) GetSpreadSummary(spreadResult *SpreadResult) string {
-	if spreadResult == nil {
-		return "no spread data"
-	}
-	
-	if spreadResult.SampleCount <= 1 {
-		return fmt.Sprintf("Spread: %.1f bps (bid: $%.4f, ask: $%.4f, mid: $%.4f)",
-			spreadResult.Current.SpreadBps,
-			spreadResult.Current.BidPrice,
-			spreadResult.Current.AskPrice,
-			spreadResult.Current.MidPrice)
-	}
-	
-	return fmt.Sprintf("Spread: %.1f bps avg (current: %.1f, range: %.1f-%.1f, %d samples)",
-		spreadResult.RollingAvgBps,
-		spreadResult.Current.SpreadBps,
-		spreadResult.MinBps,
-		spreadResult.MaxBps,
-		spreadResult.SampleCount)
-}
-
-// IsSpreadStable checks if spread is stable within acceptable variance
-func (sc *SpreadCalculator) IsSpreadStable(spreadResult *SpreadResult, maxStdDevBps float64) bool {
-	if spreadResult == nil || spreadResult.SampleCount < 10 {
-		return false // Need sufficient samples
-	}
-	
-	return spreadResult.StdDevBps <= maxStdDevBps
-}
-
-// EstimateExecutionCost estimates total execution cost including spread impact
-func (sc *SpreadCalculator) EstimateExecutionCost(spreadResult *SpreadResult, tradeSizeUSD float64, side string) *ExecutionCost {
-	if spreadResult == nil {
-		return &ExecutionCost{
-			Error: "no spread data available",
-		}
-	}
-	
-	current := spreadResult.Current
-	
-	var executionPrice, impactBps float64
-	
-	switch side {
-	case "buy":
-		executionPrice = current.AskPrice
-		// For small trades, main cost is crossing the spread
-		impactBps = (executionPrice - current.MidPrice) / current.MidPrice * 10000
-	case "sell":
-		executionPrice = current.BidPrice
-		impactBps = (current.MidPrice - executionPrice) / current.MidPrice * 10000
-	default:
-		return &ExecutionCost{
-			Error: fmt.Sprintf("invalid side: %s", side),
-		}
-	}
-	
-	quantity := tradeSizeUSD / executionPrice
-	totalCost := quantity * executionPrice
-	
-	return &ExecutionCost{
-		Side:           side,
-		TradeSizeUSD:   tradeSizeUSD,
-		ExecutionPrice: executionPrice,
-		MidPrice:       current.MidPrice,
-		Quantity:       quantity,
-		TotalCost:      totalCost,
-		ImpactBps:      impactBps,
-		SpreadBps:      current.SpreadBps,
-		Timestamp:      current.Timestamp,
-	}
-}
-
-// ExecutionCost contains execution cost estimation
-type ExecutionCost struct {
-	Side           string    `json:"side"`
-	TradeSizeUSD   float64   `json:"trade_size_usd"`
-	ExecutionPrice float64   `json:"execution_price"`
-	MidPrice       float64   `json:"mid_price"`
-	Quantity       float64   `json:"quantity"`
-	TotalCost      float64   `json:"total_cost"`
-	ImpactBps      float64   `json:"impact_bps"`      // Price impact in bps
-	SpreadBps      float64   `json:"spread_bps"`      // Current spread
-	Timestamp      time.Time `json:"timestamp"`
-	Error          string    `json:"error,omitempty"`
-}
-
-// ClearHistory clears spread history (useful for testing)
-func (sc *SpreadCalculator) ClearHistory() {
-	sc.history = sc.history[:0]
-}
\ No newline at end of file
+package microstructure
+
+import (
+	"fmt"
+	"math"
+	"time"
+)
+
+// SpreadCalculator computes bid-ask spreads with rolling averages
+type SpreadCalculator struct {
+	windowSeconds int
+	history       []SpreadPoint
+	maxHistory    int
+}
+
+// NewSpreadCalculator creates a spread calculator with 60s rolling window
+func NewSpreadCalculator(windowSeconds int) *SpreadCalculator {
+	maxHistory := windowSeconds * 2 // Keep 2√ó window for safety
+	if maxHistory < 100 {
+		maxHistory = 100
+	}
+
+	return &SpreadCalculator{
+		windowSeconds: windowSeconds,
+		maxHistory:    maxHistory,
+		history:       make([]SpreadPoint, 0, maxHistory),
+	}
+}
+
+// SpreadPoint tracks spread at a specific point in time
+type SpreadPoint struct {
+	Timestamp time.Time `json:"timestamp"`
+	BidPrice  float64   `json:"bid_price"`
+	AskPrice  float64   `json:"ask_price"`
+	SpreadAbs float64   `json:"spread_abs"` // Absolute spread (ask - bid)
+	SpreadBps float64   `json:"spread_bps"` // Spread in basis points
+	MidPrice  float64   `json:"mid_price"`  // (bid + ask) / 2
+	LastPrice float64   `json:"last_price"` // Last trade price
+}
+
+// SpreadResult contains comprehensive spread analysis
+type SpreadResult struct {
+	Current       SpreadPoint `json:"current"`         // Latest measurement
+	RollingAvgBps float64     `json:"rolling_avg_bps"` // 60s rolling average
+	MinBps        float64     `json:"min_bps"`         // Minimum in window
+	MaxBps        float64     `json:"max_bps"`         // Maximum in window
+	StdDevBps     float64     `json:"std_dev_bps"`     // Standard deviation
+	SampleCount   int         `json:"sample_count"`    // Samples in window
+	WindowSeconds int         `json:"window_seconds"`  // Rolling window size
+	DataQuality   string      `json:"data_quality"`    // "excellent", "good", "sparse"
+}
+
+// CalculateSpread computes current spread and updates rolling average
+func (sc *SpreadCalculator) CalculateSpread(orderbook *OrderBookSnapshot) (*SpreadResult, error) {
+	if orderbook == nil {
+		return nil, fmt.Errorf("order book snapshot is nil")
+	}
+
+	if len(orderbook.Bids) == 0 || len(orderbook.Asks) == 0 {
+		return nil, fmt.Errorf("incomplete order book: %d bids, %d asks",
+			len(orderbook.Bids), len(orderbook.Asks))
+	}
+
+	// Get best bid and ask
+	bestBid := orderbook.Bids[0] // Highest bid
+	bestAsk := orderbook.Asks[0] // Lowest ask
+
+	if bestBid.Price <= 0 || bestAsk.Price <= 0 {
+		return nil, fmt.Errorf("invalid prices: bid=%.6f, ask=%.6f",
+			bestBid.Price, bestAsk.Price)
+	}
+
+	if bestAsk.Price <= bestBid.Price {
+		return nil, fmt.Errorf("crossed book: bid=%.6f >= ask=%.6f",
+			bestBid.Price, bestAsk.Price)
+	}
+
+	// Calculate current spread
+	spreadAbs := bestAsk.Price - bestBid.Price
+	midPrice := (bestBid.Price + bestAsk.Price) / 2.0
+	spreadBps := (spreadAbs / midPrice) * 10000.0
+
+	currentPoint := SpreadPoint{
+		Timestamp: orderbook.Timestamp,
+		BidPrice:  bestBid.Price,
+		AskPrice:  bestAsk.Price,
+		SpreadAbs: spreadAbs,
+		SpreadBps: spreadBps,
+		MidPrice:  midPrice,
+		LastPrice: orderbook.LastPrice,
+	}
+
+	// Add to history
+	sc.addToHistory(currentPoint)
+
+	// Calculate rolling statistics
+	result := sc.calculateRollingStats(currentPoint)
+
+	return result, nil
+}
+
+// addToHistory adds a spread point and manages history size
+func (sc *SpreadCalculator) addToHistory(point SpreadPoint) {
+	sc.history = append(sc.history, point)
+
+	// Trim to max history size
+	if len(sc.history) > sc.maxHistory {
+		sc.history = sc.history[1:]
+	}
+}
+
+// calculateRollingStats computes rolling statistics over the window
+func (sc *SpreadCalculator) calculateRollingStats(current SpreadPoint) *SpreadResult {
+	cutoff := current.Timestamp.Add(-time.Duration(sc.windowSeconds) * time.Second)
+
+	// Filter to window
+	var windowPoints []SpreadPoint
+	for _, point := range sc.history {
+		if point.Timestamp.After(cutoff) {
+			windowPoints = append(windowPoints, point)
+		}
+	}
+
+	if len(windowPoints) == 0 {
+		// No history, use current point
+		return &SpreadResult{
+			Current:       current,
+			RollingAvgBps: current.SpreadBps,
+			MinBps:        current.SpreadBps,
+			MaxBps:        current.SpreadBps,
+			StdDevBps:     0.0,
+			SampleCount:   1,
+			WindowSeconds: sc.windowSeconds,
+			DataQuality:   "sparse",
+		}
+	}
+
+	// Calculate statistics
+	sum := 0.0
+	minBps := math.Inf(1)
+	maxBps := math.Inf(-1)
+
+	for _, point := range windowPoints {
+		sum += point.SpreadBps
+		if point.SpreadBps < minBps {
+			minBps = point.SpreadBps
+		}
+		if point.SpreadBps > maxBps {
+			maxBps = point.SpreadBps
+		}
+	}
+
+	avgBps := sum / float64(len(windowPoints))
+
+	// Calculate standard deviation
+	sumSquares := 0.0
+	for _, point := range windowPoints {
+		diff := point.SpreadBps - avgBps
+		sumSquares += diff * diff
+	}
+	stdDevBps := math.Sqrt(sumSquares / float64(len(windowPoints)))
+
+	// Assess data quality
+	dataQuality := "excellent"
+	samplesPerSecond := float64(len(windowPoints)) / float64(sc.windowSeconds)
+	if samplesPerSecond < 0.1 {
+		dataQuality = "sparse"
+	} else if samplesPerSecond < 0.5 {
+		dataQuality = "good"
+	}
+
+	return &SpreadResult{
+		Current:       current,
+		RollingAvgBps: avgBps,
+		MinBps:        minBps,
+		MaxBps:        maxBps,
+		StdDevBps:     stdDevBps,
+		SampleCount:   len(windowPoints),
+		WindowSeconds: sc.windowSeconds,
+		DataQuality:   dataQuality,
+	}
+}
+
+// ValidateSpreadRequirement checks if spread meets tier cap
+func (sc *SpreadCalculator) ValidateSpreadRequirement(spreadResult *SpreadResult, tier *LiquidityTier) (bool, string) {
+	if spreadResult == nil || tier == nil {
+		return false, "invalid inputs"
+	}
+
+	// Use rolling average for stability, fall back to current if no history
+	spreadToCheck := spreadResult.RollingAvgBps
+	if spreadResult.SampleCount <= 1 {
+		spreadToCheck = spreadResult.Current.SpreadBps
+	}
+
+	if spreadToCheck <= tier.SpreadCapBps {
+		return true, fmt.Sprintf("spread %.1f bps ‚â§ %.1f bps (%s cap)",
+			spreadToCheck, tier.SpreadCapBps, tier.Name)
+	}
+
+	return false, fmt.Sprintf("spread too wide: %.1f bps > %.1f bps (%s cap)",
+		spreadToCheck, tier.SpreadCapBps, tier.Name)
+}
+
+// GetSpreadSummary returns human-readable spread summary
+func (sc *SpreadCalculator) GetSpreadSummary(spreadResult *SpreadResult) string {
+	if spreadResult == nil {
+		return "no spread data"
+	}
+
+	if spreadResult.SampleCount <= 1 {
+		return fmt.Sprintf("Spread: %.1f bps (bid: $%.4f, ask: $%.4f, mid: $%.4f)",
+			spreadResult.Current.SpreadBps,
+			spreadResult.Current.BidPrice,
+			spreadResult.Current.AskPrice,
+			spreadResult.Current.MidPrice)
+	}
+
+	return fmt.Sprintf("Spread: %.1f bps avg (current: %.1f, range: %.1f-%.1f, %d samples)",
+		spreadResult.RollingAvgBps,
+		spreadResult.Current.SpreadBps,
+		spreadResult.MinBps,
+		spreadResult.MaxBps,
+		spreadResult.SampleCount)
+}
+
+// IsSpreadStable checks if spread is stable within acceptable variance
+func (sc *SpreadCalculator) IsSpreadStable(spreadResult *SpreadResult, maxStdDevBps float64) bool {
+	if spreadResult == nil || spreadResult.SampleCount < 10 {
+		return false // Need sufficient samples
+	}
+
+	return spreadResult.StdDevBps <= maxStdDevBps
+}
+
+// EstimateExecutionCost estimates total execution cost including spread impact
+func (sc *SpreadCalculator) EstimateExecutionCost(spreadResult *SpreadResult, tradeSizeUSD float64, side string) *ExecutionCost {
+	if spreadResult == nil {
+		return &ExecutionCost{
+			Error: "no spread data available",
+		}
+	}
+
+	current := spreadResult.Current
+
+	var executionPrice, impactBps float64
+
+	switch side {
+	case "buy":
+		executionPrice = current.AskPrice
+		// For small trades, main cost is crossing the spread
+		impactBps = (executionPrice - current.MidPrice) / current.MidPrice * 10000
+	case "sell":
+		executionPrice = current.BidPrice
+		impactBps = (current.MidPrice - executionPrice) / current.MidPrice * 10000
+	default:
+		return &ExecutionCost{
+			Error: fmt.Sprintf("invalid side: %s", side),
+		}
+	}
+
+	quantity := tradeSizeUSD / executionPrice
+	totalCost := quantity * executionPrice
+
+	return &ExecutionCost{
+		Side:           side,
+		TradeSizeUSD:   tradeSizeUSD,
+		ExecutionPrice: executionPrice,
+		MidPrice:       current.MidPrice,
+		Quantity:       quantity,
+		TotalCost:      totalCost,
+		ImpactBps:      impactBps,
+		SpreadBps:      current.SpreadBps,
+		Timestamp:      current.Timestamp,
+	}
+}
+
+// ExecutionCost contains execution cost estimation
+type ExecutionCost struct {
+	Side           string    `json:"side"`
+	TradeSizeUSD   float64   `json:"trade_size_usd"`
+	ExecutionPrice float64   `json:"execution_price"`
+	MidPrice       float64   `json:"mid_price"`
+	Quantity       float64   `json:"quantity"`
+	TotalCost      float64   `json:"total_cost"`
+	ImpactBps      float64   `json:"impact_bps"` // Price impact in bps
+	SpreadBps      float64   `json:"spread_bps"` // Current spread
+	Timestamp      time.Time `json:"timestamp"`
+	Error          string    `json:"error,omitempty"`
+}
+
+// ClearHistory clears spread history (useful for testing)
+func (sc *SpreadCalculator) ClearHistory() {
+	sc.history = sc.history[:0]
+}
diff --git a/internal/microstructure/vadr.go b/internal/microstructure/vadr.go
index a3cc95e..9e763f2 100644
--- a/internal/microstructure/vadr.go
+++ b/internal/microstructure/vadr.go
@@ -1,275 +1,275 @@
-package microstructure
-
-import (
-	"fmt"
-	"math"
-)
-
-// VADRCalculator computes Volume-Adjusted Daily Range with tier precedence
-// VADR = max(p80_threshold, tier_minimum) where p80 is 80th percentile of 24h VADR
-type VADRCalculator struct {
-	historicalVADR []float64 // 24h rolling history
-	maxHistory     int       // Maximum history size
-}
-
-// NewVADRCalculator creates a VADR calculator
-func NewVADRCalculator() *VADRCalculator {
-	return &VADRCalculator{
-		historicalVADR: make([]float64, 0, 288), // 24h * 12 (5min intervals)
-		maxHistory:     288,
-	}
-}
-
-// VADRResult contains VADR calculation results
-type VADRResult struct {
-	Current       float64 `json:"current"`        // Current VADR multiple
-	P80Threshold  float64 `json:"p80_threshold"`  // 80th percentile of 24h history
-	TierMinimum   float64 `json:"tier_minimum"`   // Tier minimum requirement
-	EffectiveMin  float64 `json:"effective_min"`  // max(p80, tier_min)
-	PassesGate    bool    `json:"passes_gate"`    // current >= effective_min
-	HistoryCount  int     `json:"history_count"`  // Samples in 24h window
-	Percentiles   VADRPercentiles `json:"percentiles"` // Full percentile breakdown
-}
-
-// VADRPercentiles contains percentile analysis of historical VADR
-type VADRPercentiles struct {
-	P10  float64 `json:"p10"`  // 10th percentile
-	P25  float64 `json:"p25"`  // 25th percentile  
-	P50  float64 `json:"p50"`  // Median
-	P75  float64 `json:"p75"`  // 75th percentile
-	P80  float64 `json:"p80"`  // 80th percentile (key threshold)
-	P90  float64 `json:"p90"`  // 90th percentile
-	P95  float64 `json:"p95"`  // 95th percentile
-	Min  float64 `json:"min"`  // Minimum value
-	Max  float64 `json:"max"`  // Maximum value
-	Mean float64 `json:"mean"` // Average value
-}
-
-// VADRInput contains inputs for VADR calculation
-type VADRInput struct {
-	High         float64 `json:"high"`          // 24h high price
-	Low          float64 `json:"low"`           // 24h low price
-	Volume       float64 `json:"volume"`        // 24h volume in base units
-	ADV          float64 `json:"adv"`           // Average Daily Volume (USD)
-	CurrentPrice float64 `json:"current_price"` // Current price for validation
-}
-
-// CalculateVADR computes current VADR and evaluates against tier requirements
-func (vc *VADRCalculator) CalculateVADR(input *VADRInput, tier *LiquidityTier) (*VADRResult, error) {
-	if input == nil || tier == nil {
-		return nil, fmt.Errorf("invalid inputs: input=%v, tier=%v", input, tier)
-	}
-
-	if input.High <= 0 || input.Low <= 0 || input.Volume <= 0 || input.ADV <= 0 {
-		return nil, fmt.Errorf("invalid input values: high=%.6f, low=%.6f, volume=%.2f, adv=%.0f",
-			input.High, input.Low, input.Volume, input.ADV)
-	}
-
-	if input.High < input.Low {
-		return nil, fmt.Errorf("invalid price range: high=%.6f < low=%.6f", input.High, input.Low)
-	}
-
-	// Calculate current VADR
-	// VADR = (High - Low) / (Volume / ADV) = Range / Volume_Multiple
-	priceRange := input.High - input.Low
-	volumeMultiple := input.Volume * input.CurrentPrice / input.ADV
-
-	if volumeMultiple <= 0 {
-		return nil, fmt.Errorf("invalid volume multiple: %.6f", volumeMultiple)
-	}
-
-	currentVADR := priceRange / (input.CurrentPrice * volumeMultiple)
-
-	// Add to history for percentile calculation
-	vc.addToHistory(currentVADR)
-
-	// Calculate percentiles from history
-	percentiles := vc.calculatePercentiles()
-
-	// Determine effective minimum: max(p80, tier_minimum)
-	p80Threshold := percentiles.P80
-	tierMinimum := tier.VADRMinimum
-	effectiveMin := math.Max(p80Threshold, tierMinimum)
-
-	// Check if current VADR passes gate
-	passesGate := currentVADR >= effectiveMin
-
-	result := &VADRResult{
-		Current:       currentVADR,
-		P80Threshold:  p80Threshold,
-		TierMinimum:   tierMinimum,
-		EffectiveMin:  effectiveMin,
-		PassesGate:    passesGate,
-		HistoryCount:  len(vc.historicalVADR),
-		Percentiles:   percentiles,
-	}
-
-	return result, nil
-}
-
-// addToHistory adds VADR value and maintains rolling 24h window
-func (vc *VADRCalculator) addToHistory(vadr float64) {
-	if !math.IsNaN(vadr) && !math.IsInf(vadr, 0) && vadr > 0 {
-		vc.historicalVADR = append(vc.historicalVADR, vadr)
-		
-		// Trim to max history
-		if len(vc.historicalVADR) > vc.maxHistory {
-			vc.historicalVADR = vc.historicalVADR[1:]
-		}
-	}
-}
-
-// calculatePercentiles computes percentile breakdown from history
-func (vc *VADRCalculator) calculatePercentiles() VADRPercentiles {
-	if len(vc.historicalVADR) == 0 {
-		// Return defaults when no history
-		return VADRPercentiles{
-			P10: 1.0, P25: 1.2, P50: 1.5, P75: 1.8, P80: 2.0,
-			P90: 2.5, P95: 3.0, Min: 1.0, Max: 3.0, Mean: 1.75,
-		}
-	}
-
-	// Create sorted copy for percentile calculation
-	sorted := make([]float64, len(vc.historicalVADR))
-	copy(sorted, vc.historicalVADR)
-	
-	// Simple bubble sort (acceptable for small datasets)
-	for i := 0; i < len(sorted); i++ {
-		for j := i + 1; j < len(sorted); j++ {
-			if sorted[i] > sorted[j] {
-				sorted[i], sorted[j] = sorted[j], sorted[i]
-			}
-		}
-	}
-
-	n := len(sorted)
-	
-	// Calculate percentiles using linear interpolation
-	percentiles := VADRPercentiles{
-		P10:  calculatePercentile(sorted, 0.10),
-		P25:  calculatePercentile(sorted, 0.25),
-		P50:  calculatePercentile(sorted, 0.50),
-		P75:  calculatePercentile(sorted, 0.75),
-		P80:  calculatePercentile(sorted, 0.80), // Key threshold
-		P90:  calculatePercentile(sorted, 0.90),
-		P95:  calculatePercentile(sorted, 0.95),
-		Min:  sorted[0],
-		Max:  sorted[n-1],
-	}
-
-	// Calculate mean
-	sum := 0.0
-	for _, val := range sorted {
-		sum += val
-	}
-	percentiles.Mean = sum / float64(n)
-
-	return percentiles
-}
-
-// calculatePercentile computes a specific percentile using linear interpolation
-func calculatePercentile(sorted []float64, p float64) float64 {
-	if len(sorted) == 0 {
-		return 0.0
-	}
-	
-	if len(sorted) == 1 {
-		return sorted[0]
-	}
-
-	// Calculate index position
-	pos := p * float64(len(sorted)-1)
-	lower := int(math.Floor(pos))
-	upper := int(math.Ceil(pos))
-	
-	if lower == upper {
-		return sorted[lower]
-	}
-	
-	// Linear interpolation
-	weight := pos - float64(lower)
-	return sorted[lower]*(1.0-weight) + sorted[upper]*weight
-}
-
-// ValidateVADRRequirement checks if VADR meets effective minimum
-func (vc *VADRCalculator) ValidateVADRRequirement(vadrResult *VADRResult) (bool, string) {
-	if vadrResult == nil {
-		return false, "no VADR data"
-	}
-	
-	if vadrResult.PassesGate {
-		return true, fmt.Sprintf("VADR %.3f ‚â• %.3f (max of p80=%.3f, tier_min=%.3f)",
-			vadrResult.Current, vadrResult.EffectiveMin,
-			vadrResult.P80Threshold, vadrResult.TierMinimum)
-	}
-	
-	return false, fmt.Sprintf("VADR insufficient: %.3f < %.3f (need max of p80=%.3f, tier_min=%.3f)",
-		vadrResult.Current, vadrResult.EffectiveMin,
-		vadrResult.P80Threshold, vadrResult.TierMinimum)
-}
-
-// GetVADRSummary returns human-readable VADR summary
-func (vc *VADRCalculator) GetVADRSummary(vadrResult *VADRResult) string {
-	if vadrResult == nil {
-		return "no VADR data"
-	}
-
-	status := "PASS"
-	if !vadrResult.PassesGate {
-		status = "FAIL"
-	}
-
-	return fmt.Sprintf("VADR: %.3f√ó %s (need ‚â•%.3f, p80=%.3f, tier=%.3f, %d samples)",
-		vadrResult.Current,
-		status,
-		vadrResult.EffectiveMin,
-		vadrResult.P80Threshold,
-		vadrResult.TierMinimum,
-		vadrResult.HistoryCount)
-}
-
-// IsVADRHistoryAdequate checks if we have sufficient history for reliable p80
-func (vc *VADRCalculator) IsVADRHistoryAdequate() bool {
-	return len(vc.historicalVADR) >= 50 // Need at least ~4h of data for reliable p80
-}
-
-// GetVADRHistoryStats returns current history statistics
-func (vc *VADRCalculator) GetVADRHistoryStats() map[string]interface{} {
-	if len(vc.historicalVADR) == 0 {
-		return map[string]interface{}{
-			"count": 0,
-			"status": "no_data",
-		}
-	}
-
-	percentiles := vc.calculatePercentiles()
-	
-	status := "sparse"
-	if len(vc.historicalVADR) >= 50 {
-		status = "adequate"
-	}
-	if len(vc.historicalVADR) >= 200 {
-		status = "excellent"
-	}
-
-	return map[string]interface{}{
-		"count":       len(vc.historicalVADR),
-		"max_history": vc.maxHistory,
-		"status":      status,
-		"percentiles": percentiles,
-		"utilization": float64(len(vc.historicalVADR)) / float64(vc.maxHistory),
-	}
-}
-
-// ClearHistory clears VADR history (useful for testing)
-func (vc *VADRCalculator) ClearHistory() {
-	vc.historicalVADR = vc.historicalVADR[:0]
-}
-
-// LoadHistoricalVADR loads historical VADR values (for initialization)
-func (vc *VADRCalculator) LoadHistoricalVADR(values []float64) {
-	vc.historicalVADR = make([]float64, 0, vc.maxHistory)
-	for _, val := range values {
-		vc.addToHistory(val)
-	}
-}
\ No newline at end of file
+package microstructure
+
+import (
+	"fmt"
+	"math"
+)
+
+// VADRCalculator computes Volume-Adjusted Daily Range with tier precedence
+// VADR = max(p80_threshold, tier_minimum) where p80 is 80th percentile of 24h VADR
+type VADRCalculator struct {
+	historicalVADR []float64 // 24h rolling history
+	maxHistory     int       // Maximum history size
+}
+
+// NewVADRCalculator creates a VADR calculator
+func NewVADRCalculator() *VADRCalculator {
+	return &VADRCalculator{
+		historicalVADR: make([]float64, 0, 288), // 24h * 12 (5min intervals)
+		maxHistory:     288,
+	}
+}
+
+// VADRResult contains VADR calculation results
+type VADRResult struct {
+	Current      float64         `json:"current"`       // Current VADR multiple
+	P80Threshold float64         `json:"p80_threshold"` // 80th percentile of 24h history
+	TierMinimum  float64         `json:"tier_minimum"`  // Tier minimum requirement
+	EffectiveMin float64         `json:"effective_min"` // max(p80, tier_min)
+	PassesGate   bool            `json:"passes_gate"`   // current >= effective_min
+	HistoryCount int             `json:"history_count"` // Samples in 24h window
+	Percentiles  VADRPercentiles `json:"percentiles"`   // Full percentile breakdown
+}
+
+// VADRPercentiles contains percentile analysis of historical VADR
+type VADRPercentiles struct {
+	P10  float64 `json:"p10"`  // 10th percentile
+	P25  float64 `json:"p25"`  // 25th percentile
+	P50  float64 `json:"p50"`  // Median
+	P75  float64 `json:"p75"`  // 75th percentile
+	P80  float64 `json:"p80"`  // 80th percentile (key threshold)
+	P90  float64 `json:"p90"`  // 90th percentile
+	P95  float64 `json:"p95"`  // 95th percentile
+	Min  float64 `json:"min"`  // Minimum value
+	Max  float64 `json:"max"`  // Maximum value
+	Mean float64 `json:"mean"` // Average value
+}
+
+// VADRInput contains inputs for VADR calculation
+type VADRInput struct {
+	High         float64 `json:"high"`          // 24h high price
+	Low          float64 `json:"low"`           // 24h low price
+	Volume       float64 `json:"volume"`        // 24h volume in base units
+	ADV          float64 `json:"adv"`           // Average Daily Volume (USD)
+	CurrentPrice float64 `json:"current_price"` // Current price for validation
+}
+
+// CalculateVADR computes current VADR and evaluates against tier requirements
+func (vc *VADRCalculator) CalculateVADR(input *VADRInput, tier *LiquidityTier) (*VADRResult, error) {
+	if input == nil || tier == nil {
+		return nil, fmt.Errorf("invalid inputs: input=%v, tier=%v", input, tier)
+	}
+
+	if input.High <= 0 || input.Low <= 0 || input.Volume <= 0 || input.ADV <= 0 {
+		return nil, fmt.Errorf("invalid input values: high=%.6f, low=%.6f, volume=%.2f, adv=%.0f",
+			input.High, input.Low, input.Volume, input.ADV)
+	}
+
+	if input.High < input.Low {
+		return nil, fmt.Errorf("invalid price range: high=%.6f < low=%.6f", input.High, input.Low)
+	}
+
+	// Calculate current VADR
+	// VADR = (High - Low) / (Volume / ADV) = Range / Volume_Multiple
+	priceRange := input.High - input.Low
+	volumeMultiple := input.Volume * input.CurrentPrice / input.ADV
+
+	if volumeMultiple <= 0 {
+		return nil, fmt.Errorf("invalid volume multiple: %.6f", volumeMultiple)
+	}
+
+	currentVADR := priceRange / (input.CurrentPrice * volumeMultiple)
+
+	// Add to history for percentile calculation
+	vc.addToHistory(currentVADR)
+
+	// Calculate percentiles from history
+	percentiles := vc.calculatePercentiles()
+
+	// Determine effective minimum: max(p80, tier_minimum)
+	p80Threshold := percentiles.P80
+	tierMinimum := tier.VADRMinimum
+	effectiveMin := math.Max(p80Threshold, tierMinimum)
+
+	// Check if current VADR passes gate
+	passesGate := currentVADR >= effectiveMin
+
+	result := &VADRResult{
+		Current:      currentVADR,
+		P80Threshold: p80Threshold,
+		TierMinimum:  tierMinimum,
+		EffectiveMin: effectiveMin,
+		PassesGate:   passesGate,
+		HistoryCount: len(vc.historicalVADR),
+		Percentiles:  percentiles,
+	}
+
+	return result, nil
+}
+
+// addToHistory adds VADR value and maintains rolling 24h window
+func (vc *VADRCalculator) addToHistory(vadr float64) {
+	if !math.IsNaN(vadr) && !math.IsInf(vadr, 0) && vadr > 0 {
+		vc.historicalVADR = append(vc.historicalVADR, vadr)
+
+		// Trim to max history
+		if len(vc.historicalVADR) > vc.maxHistory {
+			vc.historicalVADR = vc.historicalVADR[1:]
+		}
+	}
+}
+
+// calculatePercentiles computes percentile breakdown from history
+func (vc *VADRCalculator) calculatePercentiles() VADRPercentiles {
+	if len(vc.historicalVADR) == 0 {
+		// Return defaults when no history
+		return VADRPercentiles{
+			P10: 1.0, P25: 1.2, P50: 1.5, P75: 1.8, P80: 2.0,
+			P90: 2.5, P95: 3.0, Min: 1.0, Max: 3.0, Mean: 1.75,
+		}
+	}
+
+	// Create sorted copy for percentile calculation
+	sorted := make([]float64, len(vc.historicalVADR))
+	copy(sorted, vc.historicalVADR)
+
+	// Simple bubble sort (acceptable for small datasets)
+	for i := 0; i < len(sorted); i++ {
+		for j := i + 1; j < len(sorted); j++ {
+			if sorted[i] > sorted[j] {
+				sorted[i], sorted[j] = sorted[j], sorted[i]
+			}
+		}
+	}
+
+	n := len(sorted)
+
+	// Calculate percentiles using linear interpolation
+	percentiles := VADRPercentiles{
+		P10: calculatePercentile(sorted, 0.10),
+		P25: calculatePercentile(sorted, 0.25),
+		P50: calculatePercentile(sorted, 0.50),
+		P75: calculatePercentile(sorted, 0.75),
+		P80: calculatePercentile(sorted, 0.80), // Key threshold
+		P90: calculatePercentile(sorted, 0.90),
+		P95: calculatePercentile(sorted, 0.95),
+		Min: sorted[0],
+		Max: sorted[n-1],
+	}
+
+	// Calculate mean
+	sum := 0.0
+	for _, val := range sorted {
+		sum += val
+	}
+	percentiles.Mean = sum / float64(n)
+
+	return percentiles
+}
+
+// calculatePercentile computes a specific percentile using linear interpolation
+func calculatePercentile(sorted []float64, p float64) float64 {
+	if len(sorted) == 0 {
+		return 0.0
+	}
+
+	if len(sorted) == 1 {
+		return sorted[0]
+	}
+
+	// Calculate index position
+	pos := p * float64(len(sorted)-1)
+	lower := int(math.Floor(pos))
+	upper := int(math.Ceil(pos))
+
+	if lower == upper {
+		return sorted[lower]
+	}
+
+	// Linear interpolation
+	weight := pos - float64(lower)
+	return sorted[lower]*(1.0-weight) + sorted[upper]*weight
+}
+
+// ValidateVADRRequirement checks if VADR meets effective minimum
+func (vc *VADRCalculator) ValidateVADRRequirement(vadrResult *VADRResult) (bool, string) {
+	if vadrResult == nil {
+		return false, "no VADR data"
+	}
+
+	if vadrResult.PassesGate {
+		return true, fmt.Sprintf("VADR %.3f ‚â• %.3f (max of p80=%.3f, tier_min=%.3f)",
+			vadrResult.Current, vadrResult.EffectiveMin,
+			vadrResult.P80Threshold, vadrResult.TierMinimum)
+	}
+
+	return false, fmt.Sprintf("VADR insufficient: %.3f < %.3f (need max of p80=%.3f, tier_min=%.3f)",
+		vadrResult.Current, vadrResult.EffectiveMin,
+		vadrResult.P80Threshold, vadrResult.TierMinimum)
+}
+
+// GetVADRSummary returns human-readable VADR summary
+func (vc *VADRCalculator) GetVADRSummary(vadrResult *VADRResult) string {
+	if vadrResult == nil {
+		return "no VADR data"
+	}
+
+	status := "PASS"
+	if !vadrResult.PassesGate {
+		status = "FAIL"
+	}
+
+	return fmt.Sprintf("VADR: %.3f√ó %s (need ‚â•%.3f, p80=%.3f, tier=%.3f, %d samples)",
+		vadrResult.Current,
+		status,
+		vadrResult.EffectiveMin,
+		vadrResult.P80Threshold,
+		vadrResult.TierMinimum,
+		vadrResult.HistoryCount)
+}
+
+// IsVADRHistoryAdequate checks if we have sufficient history for reliable p80
+func (vc *VADRCalculator) IsVADRHistoryAdequate() bool {
+	return len(vc.historicalVADR) >= 50 // Need at least ~4h of data for reliable p80
+}
+
+// GetVADRHistoryStats returns current history statistics
+func (vc *VADRCalculator) GetVADRHistoryStats() map[string]interface{} {
+	if len(vc.historicalVADR) == 0 {
+		return map[string]interface{}{
+			"count":  0,
+			"status": "no_data",
+		}
+	}
+
+	percentiles := vc.calculatePercentiles()
+
+	status := "sparse"
+	if len(vc.historicalVADR) >= 50 {
+		status = "adequate"
+	}
+	if len(vc.historicalVADR) >= 200 {
+		status = "excellent"
+	}
+
+	return map[string]interface{}{
+		"count":       len(vc.historicalVADR),
+		"max_history": vc.maxHistory,
+		"status":      status,
+		"percentiles": percentiles,
+		"utilization": float64(len(vc.historicalVADR)) / float64(vc.maxHistory),
+	}
+}
+
+// ClearHistory clears VADR history (useful for testing)
+func (vc *VADRCalculator) ClearHistory() {
+	vc.historicalVADR = vc.historicalVADR[:0]
+}
+
+// LoadHistoricalVADR loads historical VADR values (for initialization)
+func (vc *VADRCalculator) LoadHistoricalVADR(values []float64) {
+	vc.historicalVADR = make([]float64, 0, vc.maxHistory)
+	for _, val := range values {
+		vc.addToHistory(val)
+	}
+}
diff --git a/internal/microstructure/venue_health.go b/internal/microstructure/venue_health.go
index 520bafa..81a92c0 100644
--- a/internal/microstructure/venue_health.go
+++ b/internal/microstructure/venue_health.go
@@ -1,353 +1,353 @@
-package microstructure
-
-import (
-	"fmt"
-	"math"
-	"sort"
-	"sync"
-	"time"
-)
-
-// VenueHealthMonitor tracks venue operational health with real-time metrics
-// Triggers: reject_rate>5%, p99_latency>2000ms, error_rate>3% ‚Üí "halve_size"
-type VenueHealthMonitor struct {
-	mu              sync.RWMutex
-	venues          map[string]*VenueMetrics
-	config          *VenueHealthConfig
-	windowDuration  time.Duration
-}
-
-// VenueHealthConfig contains health monitoring configuration
-type VenueHealthConfig struct {
-	RejectRateThreshold   float64       `yaml:"reject_rate_threshold"`   // 5.0% default
-	LatencyThresholdMs    int64         `yaml:"latency_threshold_ms"`    // 2000ms default  
-	ErrorRateThreshold    float64       `yaml:"error_rate_threshold"`    // 3.0% default
-	WindowDuration        time.Duration `yaml:"window_duration"`         // 15min default
-	MinSamplesForHealth   int           `yaml:"min_samples_for_health"`  // 10 minimum
-	MaxHistorySize        int           `yaml:"max_history_size"`        // 1000 max records
-}
-
-// NewVenueHealthMonitor creates a venue health monitor
-func NewVenueHealthMonitor(config *VenueHealthConfig) *VenueHealthMonitor {
-	if config == nil {
-		config = defaultVenueHealthConfig()
-	}
-	
-	return &VenueHealthMonitor{
-		venues:         make(map[string]*VenueMetrics),
-		config:         config,
-		windowDuration: config.WindowDuration,
-	}
-}
-
-// defaultVenueHealthConfig returns default health monitoring configuration
-func defaultVenueHealthConfig() *VenueHealthConfig {
-	return &VenueHealthConfig{
-		RejectRateThreshold: 5.0,
-		LatencyThresholdMs:  2000,
-		ErrorRateThreshold:  3.0,
-		WindowDuration:      15 * time.Minute,
-		MinSamplesForHealth: 10,
-		MaxHistorySize:      1000,
-	}
-}
-
-// RecordRequest records an API request for health tracking
-func (vhm *VenueHealthMonitor) RecordRequest(venue, endpoint string, latencyMs int64, success bool, statusCode int, errorCode string) {
-	vhm.mu.Lock()
-	defer vhm.mu.Unlock()
-	
-	if vhm.venues[venue] == nil {
-		vhm.venues[venue] = &VenueMetrics{
-			Venue:           venue,
-			RecentRequests:  make([]RequestLog, 0, vhm.config.MaxHistorySize),
-			RecentErrors:    make([]ErrorLog, 0, vhm.config.MaxHistorySize/2),
-			HealthHistory:   make([]HealthPoint, 0, 100),
-			LastHealthCheck: time.Now(),
-		}
-	}
-	
-	venueMetrics := vhm.venues[venue]
-	
-	// Record request
-	request := RequestLog{
-		Timestamp:  time.Now(),
-		Endpoint:   endpoint,
-		LatencyMs:  latencyMs,
-		Success:    success,
-		StatusCode: statusCode,
-		ErrorCode:  errorCode,
-	}
-	
-	venueMetrics.RecentRequests = append(venueMetrics.RecentRequests, request)
-	
-	// Trim history if needed
-	if len(venueMetrics.RecentRequests) > vhm.config.MaxHistorySize {
-		venueMetrics.RecentRequests = venueMetrics.RecentRequests[1:]
-	}
-	
-	// Record error if request failed
-	if !success {
-		errorLog := ErrorLog{
-			Timestamp:   time.Now(),
-			Endpoint:    endpoint,
-			ErrorType:   "api_error",
-			ErrorCode:   errorCode,
-			Message:     fmt.Sprintf("HTTP %d: %s", statusCode, errorCode),
-			Recoverable: statusCode < 500, // Client errors are recoverable, server errors may not be
-		}
-		
-		venueMetrics.RecentErrors = append(venueMetrics.RecentErrors, errorLog)
-		
-		// Trim error history
-		if len(venueMetrics.RecentErrors) > vhm.config.MaxHistorySize/2 {
-			venueMetrics.RecentErrors = venueMetrics.RecentErrors[1:]
-		}
-	}
-}
-
-// GetVenueHealth evaluates current venue health status
-func (vhm *VenueHealthMonitor) GetVenueHealth(venue string) (*VenueHealthStatus, error) {
-	vhm.mu.RLock()
-	defer vhm.mu.RUnlock()
-	
-	venueMetrics, exists := vhm.venues[venue]
-	if !exists {
-		// Return default healthy status for unknown venues
-		return &VenueHealthStatus{
-			Healthy:        true,
-			RejectRate:     0.0,
-			LatencyP99Ms:   100,
-			ErrorRate:      0.0,
-			LastUpdate:     time.Now(),
-			Recommendation: "full_size",
-			UptimePercent:  100.0,
-		}, nil
-	}
-	
-	now := time.Now()
-	windowStart := now.Add(-vhm.windowDuration)
-	
-	// Filter requests within window
-	var windowRequests []RequestLog
-	var windowErrors []ErrorLog
-	
-	for _, req := range venueMetrics.RecentRequests {
-		if req.Timestamp.After(windowStart) {
-			windowRequests = append(windowRequests, req)
-		}
-	}
-	
-	for _, err := range venueMetrics.RecentErrors {
-		if err.Timestamp.After(windowStart) {
-			windowErrors = append(windowErrors, err)
-		}
-	}
-	
-	// Calculate metrics
-	health := &VenueHealthStatus{
-		LastUpdate: now,
-	}
-	
-	if len(windowRequests) < vhm.config.MinSamplesForHealth {
-		// Insufficient data - assume healthy but note sparse data
-		health.Healthy = true
-		health.Recommendation = "full_size"
-		health.UptimePercent = 100.0
-		return health, nil
-	}
-	
-	// Calculate reject rate (failed requests / total requests)
-	rejectedCount := 0
-	latencies := make([]int64, 0, len(windowRequests))
-	
-	for _, req := range windowRequests {
-		if !req.Success {
-			rejectedCount++
-		}
-		latencies = append(latencies, req.LatencyMs)
-	}
-	
-	health.RejectRate = float64(rejectedCount) / float64(len(windowRequests)) * 100.0
-	
-	// Calculate P99 latency
-	if len(latencies) > 0 {
-		sort.Slice(latencies, func(i, j int) bool {
-			return latencies[i] < latencies[j]
-		})
-		
-		p99Index := int(math.Ceil(0.99 * float64(len(latencies)))) - 1
-		if p99Index < 0 {
-			p99Index = 0
-		}
-		if p99Index >= len(latencies) {
-			p99Index = len(latencies) - 1
-		}
-		
-		health.LatencyP99Ms = latencies[p99Index]
-	}
-	
-	// Calculate error rate (errors / total requests)
-	health.ErrorRate = float64(len(windowErrors)) / float64(len(windowRequests)) * 100.0
-	
-	// Calculate uptime (successful requests / total requests)
-	successfulCount := len(windowRequests) - rejectedCount
-	health.UptimePercent = float64(successfulCount) / float64(len(windowRequests)) * 100.0
-	
-	// Determine overall health status
-	health.Healthy = true
-	reasons := []string{}
-	
-	if health.RejectRate > vhm.config.RejectRateThreshold {
-		health.Healthy = false
-		reasons = append(reasons, fmt.Sprintf("reject_rate %.1f%% > %.1f%%", 
-			health.RejectRate, vhm.config.RejectRateThreshold))
-	}
-	
-	if health.LatencyP99Ms > vhm.config.LatencyThresholdMs {
-		health.Healthy = false
-		reasons = append(reasons, fmt.Sprintf("p99_latency %dms > %dms", 
-			health.LatencyP99Ms, vhm.config.LatencyThresholdMs))
-	}
-	
-	if health.ErrorRate > vhm.config.ErrorRateThreshold {
-		health.Healthy = false
-		reasons = append(reasons, fmt.Sprintf("error_rate %.1f%% > %.1f%%", 
-			health.ErrorRate, vhm.config.ErrorRateThreshold))
-	}
-	
-	// Determine recommendation
-	if health.Healthy {
-		health.Recommendation = "full_size"
-	} else if len(reasons) == 1 && health.ErrorRate <= vhm.config.ErrorRateThreshold*2 {
-		// Minor issues - halve size
-		health.Recommendation = "halve_size"
-	} else {
-		// Major issues - avoid venue
-		health.Recommendation = "avoid"
-	}
-	
-	// Record health point for history
-	healthPoint := HealthPoint{
-		Timestamp:     now,
-		Healthy:       health.Healthy,
-		RejectRate:    health.RejectRate,
-		LatencyP99Ms:  health.LatencyP99Ms,
-		ErrorRate:     health.ErrorRate,
-	}
-	
-	venueMetrics.HealthHistory = append(venueMetrics.HealthHistory, healthPoint)
-	
-	// Trim health history
-	if len(venueMetrics.HealthHistory) > 100 {
-		venueMetrics.HealthHistory = venueMetrics.HealthHistory[1:]
-	}
-	
-	venueMetrics.LastHealthCheck = now
-	
-	return health, nil
-}
-
-// GetAllVenueHealth returns health status for all monitored venues
-func (vhm *VenueHealthMonitor) GetAllVenueHealth() (map[string]*VenueHealthStatus, error) {
-	vhm.mu.RLock()
-	venueNames := make([]string, 0, len(vhm.venues))
-	for venue := range vhm.venues {
-		venueNames = append(venueNames, venue)
-	}
-	vhm.mu.RUnlock()
-	
-	result := make(map[string]*VenueHealthStatus)
-	
-	for _, venue := range venueNames {
-		health, err := vhm.GetVenueHealth(venue)
-		if err != nil {
-			return nil, fmt.Errorf("failed to get health for venue %s: %w", venue, err)
-		}
-		result[venue] = health
-	}
-	
-	return result, nil
-}
-
-// RecordOrderReject records an order rejection for health tracking
-func (vhm *VenueHealthMonitor) RecordOrderReject(venue, reason string) {
-	vhm.RecordRequest(venue, "order", 0, false, 400, reason)
-}
-
-// RecordConnectionError records a connection error
-func (vhm *VenueHealthMonitor) RecordConnectionError(venue, errorType string) {
-	vhm.RecordRequest(venue, "connection", 0, false, 500, errorType)
-}
-
-// IsVenueHealthy provides a simple boolean health check
-func (vhm *VenueHealthMonitor) IsVenueHealthy(venue string) bool {
-	health, err := vhm.GetVenueHealth(venue)
-	if err != nil {
-		return false // Assume unhealthy if we can't determine
-	}
-	return health.Healthy
-}
-
-// GetVenueRecommendation returns sizing recommendation for a venue
-func (vhm *VenueHealthMonitor) GetVenueRecommendation(venue string) string {
-	health, err := vhm.GetVenueHealth(venue)
-	if err != nil {
-		return "avoid"
-	}
-	return health.Recommendation
-}
-
-// GetHealthSummary returns a summary of all venue health
-func (vhm *VenueHealthMonitor) GetHealthSummary() map[string]interface{} {
-	vhm.mu.RLock()
-	defer vhm.mu.RUnlock()
-	
-	summary := map[string]interface{}{
-		"total_venues":     len(vhm.venues),
-		"monitoring_since": time.Now().Add(-vhm.windowDuration),
-		"window_duration":  vhm.windowDuration.String(),
-		"thresholds": map[string]interface{}{
-			"reject_rate_pct":   vhm.config.RejectRateThreshold,
-			"latency_p99_ms":    vhm.config.LatencyThresholdMs,
-			"error_rate_pct":    vhm.config.ErrorRateThreshold,
-		},
-	}
-	
-	healthyCount := 0
-	venueStats := make(map[string]interface{})
-	
-	for venue := range vhm.venues {
-		health, err := vhm.GetVenueHealth(venue)
-		if err != nil {
-			continue
-		}
-		
-		if health.Healthy {
-			healthyCount++
-		}
-		
-		venueStats[venue] = map[string]interface{}{
-			"healthy":        health.Healthy,
-			"recommendation": health.Recommendation,
-			"reject_rate":    health.RejectRate,
-			"latency_p99":    health.LatencyP99Ms,
-			"error_rate":     health.ErrorRate,
-			"uptime_pct":     health.UptimePercent,
-		}
-	}
-	
-	summary["healthy_venues"] = healthyCount
-	summary["health_rate"] = float64(healthyCount) / float64(len(vhm.venues))
-	summary["venues"] = venueStats
-	
-	return summary
-}
-
-// ClearHistory clears all venue health history (useful for testing)
-func (vhm *VenueHealthMonitor) ClearHistory() {
-	vhm.mu.Lock()
-	defer vhm.mu.Unlock()
-	
-	vhm.venues = make(map[string]*VenueMetrics)
-}
\ No newline at end of file
+package microstructure
+
+import (
+	"fmt"
+	"math"
+	"sort"
+	"sync"
+	"time"
+)
+
+// VenueHealthMonitor tracks venue operational health with real-time metrics
+// Triggers: reject_rate>5%, p99_latency>2000ms, error_rate>3% ‚Üí "halve_size"
+type VenueHealthMonitor struct {
+	mu             sync.RWMutex
+	venues         map[string]*VenueMetrics
+	config         *VenueHealthConfig
+	windowDuration time.Duration
+}
+
+// VenueHealthConfig contains health monitoring configuration
+type VenueHealthConfig struct {
+	RejectRateThreshold float64       `yaml:"reject_rate_threshold"`  // 5.0% default
+	LatencyThresholdMs  int64         `yaml:"latency_threshold_ms"`   // 2000ms default
+	ErrorRateThreshold  float64       `yaml:"error_rate_threshold"`   // 3.0% default
+	WindowDuration      time.Duration `yaml:"window_duration"`        // 15min default
+	MinSamplesForHealth int           `yaml:"min_samples_for_health"` // 10 minimum
+	MaxHistorySize      int           `yaml:"max_history_size"`       // 1000 max records
+}
+
+// NewVenueHealthMonitor creates a venue health monitor
+func NewVenueHealthMonitor(config *VenueHealthConfig) *VenueHealthMonitor {
+	if config == nil {
+		config = defaultVenueHealthConfig()
+	}
+
+	return &VenueHealthMonitor{
+		venues:         make(map[string]*VenueMetrics),
+		config:         config,
+		windowDuration: config.WindowDuration,
+	}
+}
+
+// defaultVenueHealthConfig returns default health monitoring configuration
+func defaultVenueHealthConfig() *VenueHealthConfig {
+	return &VenueHealthConfig{
+		RejectRateThreshold: 5.0,
+		LatencyThresholdMs:  2000,
+		ErrorRateThreshold:  3.0,
+		WindowDuration:      15 * time.Minute,
+		MinSamplesForHealth: 10,
+		MaxHistorySize:      1000,
+	}
+}
+
+// RecordRequest records an API request for health tracking
+func (vhm *VenueHealthMonitor) RecordRequest(venue, endpoint string, latencyMs int64, success bool, statusCode int, errorCode string) {
+	vhm.mu.Lock()
+	defer vhm.mu.Unlock()
+
+	if vhm.venues[venue] == nil {
+		vhm.venues[venue] = &VenueMetrics{
+			Venue:           venue,
+			RecentRequests:  make([]RequestLog, 0, vhm.config.MaxHistorySize),
+			RecentErrors:    make([]ErrorLog, 0, vhm.config.MaxHistorySize/2),
+			HealthHistory:   make([]HealthPoint, 0, 100),
+			LastHealthCheck: time.Now(),
+		}
+	}
+
+	venueMetrics := vhm.venues[venue]
+
+	// Record request
+	request := RequestLog{
+		Timestamp:  time.Now(),
+		Endpoint:   endpoint,
+		LatencyMs:  latencyMs,
+		Success:    success,
+		StatusCode: statusCode,
+		ErrorCode:  errorCode,
+	}
+
+	venueMetrics.RecentRequests = append(venueMetrics.RecentRequests, request)
+
+	// Trim history if needed
+	if len(venueMetrics.RecentRequests) > vhm.config.MaxHistorySize {
+		venueMetrics.RecentRequests = venueMetrics.RecentRequests[1:]
+	}
+
+	// Record error if request failed
+	if !success {
+		errorLog := ErrorLog{
+			Timestamp:   time.Now(),
+			Endpoint:    endpoint,
+			ErrorType:   "api_error",
+			ErrorCode:   errorCode,
+			Message:     fmt.Sprintf("HTTP %d: %s", statusCode, errorCode),
+			Recoverable: statusCode < 500, // Client errors are recoverable, server errors may not be
+		}
+
+		venueMetrics.RecentErrors = append(venueMetrics.RecentErrors, errorLog)
+
+		// Trim error history
+		if len(venueMetrics.RecentErrors) > vhm.config.MaxHistorySize/2 {
+			venueMetrics.RecentErrors = venueMetrics.RecentErrors[1:]
+		}
+	}
+}
+
+// GetVenueHealth evaluates current venue health status
+func (vhm *VenueHealthMonitor) GetVenueHealth(venue string) (*VenueHealthStatus, error) {
+	vhm.mu.RLock()
+	defer vhm.mu.RUnlock()
+
+	venueMetrics, exists := vhm.venues[venue]
+	if !exists {
+		// Return default healthy status for unknown venues
+		return &VenueHealthStatus{
+			Healthy:        true,
+			RejectRate:     0.0,
+			LatencyP99Ms:   100,
+			ErrorRate:      0.0,
+			LastUpdate:     time.Now(),
+			Recommendation: "full_size",
+			UptimePercent:  100.0,
+		}, nil
+	}
+
+	now := time.Now()
+	windowStart := now.Add(-vhm.windowDuration)
+
+	// Filter requests within window
+	var windowRequests []RequestLog
+	var windowErrors []ErrorLog
+
+	for _, req := range venueMetrics.RecentRequests {
+		if req.Timestamp.After(windowStart) {
+			windowRequests = append(windowRequests, req)
+		}
+	}
+
+	for _, err := range venueMetrics.RecentErrors {
+		if err.Timestamp.After(windowStart) {
+			windowErrors = append(windowErrors, err)
+		}
+	}
+
+	// Calculate metrics
+	health := &VenueHealthStatus{
+		LastUpdate: now,
+	}
+
+	if len(windowRequests) < vhm.config.MinSamplesForHealth {
+		// Insufficient data - assume healthy but note sparse data
+		health.Healthy = true
+		health.Recommendation = "full_size"
+		health.UptimePercent = 100.0
+		return health, nil
+	}
+
+	// Calculate reject rate (failed requests / total requests)
+	rejectedCount := 0
+	latencies := make([]int64, 0, len(windowRequests))
+
+	for _, req := range windowRequests {
+		if !req.Success {
+			rejectedCount++
+		}
+		latencies = append(latencies, req.LatencyMs)
+	}
+
+	health.RejectRate = float64(rejectedCount) / float64(len(windowRequests)) * 100.0
+
+	// Calculate P99 latency
+	if len(latencies) > 0 {
+		sort.Slice(latencies, func(i, j int) bool {
+			return latencies[i] < latencies[j]
+		})
+
+		p99Index := int(math.Ceil(0.99*float64(len(latencies)))) - 1
+		if p99Index < 0 {
+			p99Index = 0
+		}
+		if p99Index >= len(latencies) {
+			p99Index = len(latencies) - 1
+		}
+
+		health.LatencyP99Ms = latencies[p99Index]
+	}
+
+	// Calculate error rate (errors / total requests)
+	health.ErrorRate = float64(len(windowErrors)) / float64(len(windowRequests)) * 100.0
+
+	// Calculate uptime (successful requests / total requests)
+	successfulCount := len(windowRequests) - rejectedCount
+	health.UptimePercent = float64(successfulCount) / float64(len(windowRequests)) * 100.0
+
+	// Determine overall health status
+	health.Healthy = true
+	reasons := []string{}
+
+	if health.RejectRate > vhm.config.RejectRateThreshold {
+		health.Healthy = false
+		reasons = append(reasons, fmt.Sprintf("reject_rate %.1f%% > %.1f%%",
+			health.RejectRate, vhm.config.RejectRateThreshold))
+	}
+
+	if health.LatencyP99Ms > vhm.config.LatencyThresholdMs {
+		health.Healthy = false
+		reasons = append(reasons, fmt.Sprintf("p99_latency %dms > %dms",
+			health.LatencyP99Ms, vhm.config.LatencyThresholdMs))
+	}
+
+	if health.ErrorRate > vhm.config.ErrorRateThreshold {
+		health.Healthy = false
+		reasons = append(reasons, fmt.Sprintf("error_rate %.1f%% > %.1f%%",
+			health.ErrorRate, vhm.config.ErrorRateThreshold))
+	}
+
+	// Determine recommendation
+	if health.Healthy {
+		health.Recommendation = "full_size"
+	} else if len(reasons) == 1 && health.ErrorRate <= vhm.config.ErrorRateThreshold*2 {
+		// Minor issues - halve size
+		health.Recommendation = "halve_size"
+	} else {
+		// Major issues - avoid venue
+		health.Recommendation = "avoid"
+	}
+
+	// Record health point for history
+	healthPoint := HealthPoint{
+		Timestamp:    now,
+		Healthy:      health.Healthy,
+		RejectRate:   health.RejectRate,
+		LatencyP99Ms: health.LatencyP99Ms,
+		ErrorRate:    health.ErrorRate,
+	}
+
+	venueMetrics.HealthHistory = append(venueMetrics.HealthHistory, healthPoint)
+
+	// Trim health history
+	if len(venueMetrics.HealthHistory) > 100 {
+		venueMetrics.HealthHistory = venueMetrics.HealthHistory[1:]
+	}
+
+	venueMetrics.LastHealthCheck = now
+
+	return health, nil
+}
+
+// GetAllVenueHealth returns health status for all monitored venues
+func (vhm *VenueHealthMonitor) GetAllVenueHealth() (map[string]*VenueHealthStatus, error) {
+	vhm.mu.RLock()
+	venueNames := make([]string, 0, len(vhm.venues))
+	for venue := range vhm.venues {
+		venueNames = append(venueNames, venue)
+	}
+	vhm.mu.RUnlock()
+
+	result := make(map[string]*VenueHealthStatus)
+
+	for _, venue := range venueNames {
+		health, err := vhm.GetVenueHealth(venue)
+		if err != nil {
+			return nil, fmt.Errorf("failed to get health for venue %s: %w", venue, err)
+		}
+		result[venue] = health
+	}
+
+	return result, nil
+}
+
+// RecordOrderReject records an order rejection for health tracking
+func (vhm *VenueHealthMonitor) RecordOrderReject(venue, reason string) {
+	vhm.RecordRequest(venue, "order", 0, false, 400, reason)
+}
+
+// RecordConnectionError records a connection error
+func (vhm *VenueHealthMonitor) RecordConnectionError(venue, errorType string) {
+	vhm.RecordRequest(venue, "connection", 0, false, 500, errorType)
+}
+
+// IsVenueHealthy provides a simple boolean health check
+func (vhm *VenueHealthMonitor) IsVenueHealthy(venue string) bool {
+	health, err := vhm.GetVenueHealth(venue)
+	if err != nil {
+		return false // Assume unhealthy if we can't determine
+	}
+	return health.Healthy
+}
+
+// GetVenueRecommendation returns sizing recommendation for a venue
+func (vhm *VenueHealthMonitor) GetVenueRecommendation(venue string) string {
+	health, err := vhm.GetVenueHealth(venue)
+	if err != nil {
+		return "avoid"
+	}
+	return health.Recommendation
+}
+
+// GetHealthSummary returns a summary of all venue health
+func (vhm *VenueHealthMonitor) GetHealthSummary() map[string]interface{} {
+	vhm.mu.RLock()
+	defer vhm.mu.RUnlock()
+
+	summary := map[string]interface{}{
+		"total_venues":     len(vhm.venues),
+		"monitoring_since": time.Now().Add(-vhm.windowDuration),
+		"window_duration":  vhm.windowDuration.String(),
+		"thresholds": map[string]interface{}{
+			"reject_rate_pct": vhm.config.RejectRateThreshold,
+			"latency_p99_ms":  vhm.config.LatencyThresholdMs,
+			"error_rate_pct":  vhm.config.ErrorRateThreshold,
+		},
+	}
+
+	healthyCount := 0
+	venueStats := make(map[string]interface{})
+
+	for venue := range vhm.venues {
+		health, err := vhm.GetVenueHealth(venue)
+		if err != nil {
+			continue
+		}
+
+		if health.Healthy {
+			healthyCount++
+		}
+
+		venueStats[venue] = map[string]interface{}{
+			"healthy":        health.Healthy,
+			"recommendation": health.Recommendation,
+			"reject_rate":    health.RejectRate,
+			"latency_p99":    health.LatencyP99Ms,
+			"error_rate":     health.ErrorRate,
+			"uptime_pct":     health.UptimePercent,
+		}
+	}
+
+	summary["healthy_venues"] = healthyCount
+	summary["health_rate"] = float64(healthyCount) / float64(len(vhm.venues))
+	summary["venues"] = venueStats
+
+	return summary
+}
+
+// ClearHistory clears all venue health history (useful for testing)
+func (vhm *VenueHealthMonitor) ClearHistory() {
+	vhm.mu.Lock()
+	defer vhm.mu.Unlock()
+
+	vhm.venues = make(map[string]*VenueMetrics)
+}
diff --git a/internal/premove/api.go b/internal/premove/api.go
index 6e6a5da..9b78b00 100644
--- a/internal/premove/api.go
+++ b/internal/premove/api.go
@@ -1,502 +1,502 @@
-package premove
-
-import (
-	"context"
-	"fmt"
-	"sort"
-	"time"
-
-	"cryptorun/internal/microstructure"
-)
-
-// PreMovementEngine orchestrates the complete Pre-Movement v3.3 system
-type PreMovementEngine struct {
-	scoreEngine     *ScoreEngine
-	gateEvaluator   *GateEvaluator  
-	cvdAnalyzer     *CVDResidualAnalyzer
-	microEvaluator  *microstructure.Evaluator
-	config          *EngineConfig
-}
-
-// NewPreMovementEngine creates a complete Pre-Movement v3.3 engine
-func NewPreMovementEngine(
-	microEvaluator *microstructure.Evaluator,
-	config *EngineConfig,
-) *PreMovementEngine {
-	if config == nil {
-		config = DefaultEngineConfig()
-	}
-
-	return &PreMovementEngine{
-		scoreEngine:    NewScoreEngine(config.ScoreConfig),
-		gateEvaluator:  NewGateEvaluator(microEvaluator, config.GateConfig),
-		cvdAnalyzer:    NewCVDResidualAnalyzer(config.CVDConfig),
-		microEvaluator: microEvaluator,
-		config:         config,
-	}
-}
-
-// EngineConfig contains configuration for the complete Pre-Movement system
-type EngineConfig struct {
-	ScoreConfig *ScoreConfig `yaml:"score_config"`
-	GateConfig  *GateConfig  `yaml:"gate_config"`
-	CVDConfig   *CVDConfig   `yaml:"cvd_config"`
-
-	// API limits
-	MaxCandidates     int   `yaml:"max_candidates"`      // 50 max candidates returned
-	MaxProcessTimeMs  int64 `yaml:"max_process_time_ms"` // 2000ms max processing time
-	RequireScore      bool  `yaml:"require_score"`       // true - require valid score
-	RequireGates      bool  `yaml:"require_gates"`       // true - require gate pass
-	
-	// Data freshness requirements
-	MaxDataStaleness  int64 `yaml:"max_data_staleness"`  // 1800 seconds (30 min)
-	StaleDataWarning  int64 `yaml:"stale_data_warning"`  // 600 seconds (10 min)
-}
-
-// DefaultEngineConfig returns production Pre-Movement engine configuration
-func DefaultEngineConfig() *EngineConfig {
-	return &EngineConfig{
-		ScoreConfig: DefaultScoreConfig(),
-		GateConfig:  DefaultGateConfig(),
-		CVDConfig:   DefaultCVDConfig(),
-
-		// API limits
-		MaxCandidates:    50,
-		MaxProcessTimeMs: 2000, // 2 seconds max
-		RequireScore:     true,
-		RequireGates:     true,
-
-		// Data freshness
-		MaxDataStaleness: 1800, // 30 minutes
-		StaleDataWarning: 600,  // 10 minutes
-	}
-}
-
-// CandidateInput contains all data required for Pre-Movement analysis
-type CandidateInput struct {
-	Symbol    string    `json:"symbol"`
-	Timestamp time.Time `json:"timestamp"`
-
-	// Pre-Movement scoring data
-	PreMovementData *PreMovementData `json:"premove_data"`
-
-	// Gate confirmation data  
-	ConfirmationData *ConfirmationData `json:"confirmation_data"`
-
-	// CVD residual data
-	CVDDataPoints []*CVDDataPoint `json:"cvd_data_points"`
-}
-
-// PreMovementCandidate represents a complete analyzed candidate
-type PreMovementCandidate struct {
-	Symbol           string                               `json:"symbol"`
-	Timestamp        time.Time                            `json:"timestamp"`
-	TotalScore       float64                              `json:"total_score"`       // 0-100 Pre-Movement score
-	ScoreBreakdown   *ScoreResult                         `json:"score_breakdown"`   // Detailed scoring
-	GatesStatus      string                               `json:"gates_status"`      // "CONFIRMED", "BLOCKED", "WARNING"
-	GatesResult      *ConfirmationResult                  `json:"gates_result"`      // Gate evaluation details
-	CVDResult        *CVDResidualResult                   `json:"cvd_result"`        // CVD residual analysis
-	MicroReport      *microstructure.EvaluationResult    `json:"micro_report"`      // Microstructure consultation
-	OverallStatus    string                               `json:"overall_status"`    // "STRONG", "MODERATE", "WEAK", "BLOCKED"
-	Reasons          []string                             `json:"reasons"`           // Key reasons for recommendation
-	Warnings         []string                             `json:"warnings"`          // Data quality or analysis warnings
-	ProcessTimeMs    int64                                `json:"process_time_ms"`   // Individual processing time
-	Rank             int                                  `json:"rank"`              // Rank among candidates (1=best)
-}
-
-// AnalysisResult contains complete Pre-Movement analysis results
-type AnalysisResult struct {
-	Timestamp        time.Time                `json:"timestamp"`
-	TotalCandidates  int                      `json:"total_candidates"`  // Total candidates analyzed
-	ValidCandidates  int                      `json:"valid_candidates"`  // Candidates passing filters
-	StrongCandidates int                      `json:"strong_candidates"` // High-confidence candidates
-	Candidates       []*PreMovementCandidate `json:"candidates"`        // Ranked candidate list
-	ProcessTimeMs    int64                    `json:"process_time_ms"`   // Total processing time
-	DataFreshness    *DataFreshnessReport     `json:"data_freshness"`    // Data quality summary
-	SystemWarnings   []string                 `json:"system_warnings"`   // System-level warnings
-}
-
-// DataFreshnessReport summarizes data quality across all candidates
-type DataFreshnessReport struct {
-	AverageAgeSeconds    int64   `json:"average_age_seconds"`    // Average data age
-	StaleCandidatesCount int     `json:"stale_candidates_count"` // Candidates with stale data
-	StaleCandidatesPct   float64 `json:"stale_candidates_pct"`   // % stale candidates
-	OldestDataSeconds    int64   `json:"oldest_data_seconds"`    // Oldest data point
-	FreshnessGrade       string  `json:"freshness_grade"`        // "A", "B", "C", "D", "F"
-}
-
-// ListCandidates performs complete Pre-Movement analysis and returns ranked candidates
-func (pme *PreMovementEngine) ListCandidates(ctx context.Context, inputs []*CandidateInput, limit int) (*AnalysisResult, error) {
-	startTime := time.Now()
-
-	if limit <= 0 || limit > pme.config.MaxCandidates {
-		limit = pme.config.MaxCandidates
-	}
-
-	result := &AnalysisResult{
-		Timestamp:       time.Now(),
-		TotalCandidates: len(inputs),
-		Candidates:      make([]*PreMovementCandidate, 0, len(inputs)),
-		SystemWarnings:  []string{},
-	}
-
-	// Process each candidate
-	for _, input := range inputs {
-		candidate, err := pme.analyzeCandidate(ctx, input)
-		if err != nil {
-			result.SystemWarnings = append(result.SystemWarnings, 
-				fmt.Sprintf("%s: analysis failed - %v", input.Symbol, err))
-			continue
-		}
-
-		// Apply filters
-		if pme.shouldIncludeCandidate(candidate) {
-			result.Candidates = append(result.Candidates, candidate)
-		}
-	}
-
-	result.ValidCandidates = len(result.Candidates)
-
-	// Rank candidates by overall strength
-	pme.rankCandidates(result.Candidates)
-
-	// Limit results
-	if limit < len(result.Candidates) {
-		result.Candidates = result.Candidates[:limit]
-	}
-
-	// Count strong candidates (top tier)
-	for _, candidate := range result.Candidates {
-		if candidate.OverallStatus == "STRONG" {
-			result.StrongCandidates++
-		}
-	}
-
-	// Assess data freshness
-	result.DataFreshness = pme.assessDataFreshness(result.Candidates)
-
-	result.ProcessTimeMs = time.Since(startTime).Milliseconds()
-
-	// Performance warning
-	if result.ProcessTimeMs > pme.config.MaxProcessTimeMs {
-		result.SystemWarnings = append(result.SystemWarnings,
-			fmt.Sprintf("Analysis took %dms (>%dms threshold)", result.ProcessTimeMs, pme.config.MaxProcessTimeMs))
-	}
-
-	return result, nil
-}
-
-// analyzeCandidate performs complete Pre-Movement analysis for a single candidate
-func (pme *PreMovementEngine) analyzeCandidate(ctx context.Context, input *CandidateInput) (*PreMovementCandidate, error) {
-	startTime := time.Now()
-
-	candidate := &PreMovementCandidate{
-		Symbol:     input.Symbol,
-		Timestamp:  time.Now(),
-		Reasons:    []string{},
-		Warnings:   []string{},
-	}
-
-	// 1. Score calculation
-	if input.PreMovementData != nil {
-		scoreResult, err := pme.scoreEngine.CalculateScore(ctx, input.PreMovementData)
-		if err != nil {
-			return nil, fmt.Errorf("scoring failed: %w", err)
-		}
-		candidate.ScoreBreakdown = scoreResult
-		candidate.TotalScore = scoreResult.TotalScore
-
-		// Add scoring reasons
-		if scoreResult.TotalScore >= 75 {
-			candidate.Reasons = append(candidate.Reasons, fmt.Sprintf("High Pre-Movement score (%.1f)", scoreResult.TotalScore))
-		}
-	}
-
-	// 2. Gate evaluation
-	if input.ConfirmationData != nil {
-		gateResult, err := pme.gateEvaluator.EvaluateConfirmation(ctx, input.ConfirmationData)
-		if err != nil {
-			return nil, fmt.Errorf("gate evaluation failed: %w", err)
-		}
-		candidate.GatesResult = gateResult
-		
-		if gateResult.Passed {
-			candidate.GatesStatus = "CONFIRMED"
-			candidate.Reasons = append(candidate.Reasons, 
-				fmt.Sprintf("%d-of-%d confirmations passed", gateResult.ConfirmationCount, gateResult.RequiredCount))
-		} else {
-			candidate.GatesStatus = "BLOCKED"
-		}
-
-		// Collect gate warnings
-		candidate.Warnings = append(candidate.Warnings, gateResult.Warnings...)
-	}
-
-	// 3. CVD residual analysis
-	if input.CVDDataPoints != nil && len(input.CVDDataPoints) > 0 {
-		cvdResult, err := pme.cvdAnalyzer.AnalyzeCVDResidual(ctx, input.Symbol, input.CVDDataPoints)
-		if err != nil {
-			candidate.Warnings = append(candidate.Warnings, fmt.Sprintf("CVD analysis failed: %v", err))
-		} else {
-			candidate.CVDResult = cvdResult
-			
-			if cvdResult.IsSignificant {
-				candidate.Reasons = append(candidate.Reasons, 
-					fmt.Sprintf("Significant CVD residual (%.1f%%)", cvdResult.PercentileRank))
-			}
-
-			// Collect CVD warnings
-			candidate.Warnings = append(candidate.Warnings, cvdResult.Warnings...)
-		}
-	}
-
-	// 4. Microstructure consultation (from gates result)
-	if candidate.GatesResult != nil && candidate.GatesResult.MicroReport != nil {
-		candidate.MicroReport = candidate.GatesResult.MicroReport
-	}
-
-	// 5. Determine overall status
-	candidate.OverallStatus = pme.determineOverallStatus(candidate)
-
-	candidate.ProcessTimeMs = time.Since(startTime).Milliseconds()
-
-	return candidate, nil
-}
-
-// shouldIncludeCandidate applies filters to determine if candidate should be included
-func (pme *PreMovementEngine) shouldIncludeCandidate(candidate *PreMovementCandidate) bool {
-	// Require valid score if configured
-	if pme.config.RequireScore {
-		if candidate.ScoreBreakdown == nil || !candidate.ScoreBreakdown.IsValid {
-			return false
-		}
-	}
-
-	// Require gate confirmation if configured  
-	if pme.config.RequireGates {
-		if candidate.GatesResult == nil || !candidate.GatesResult.Passed {
-			return false
-		}
-	}
-
-	// Check data freshness
-	if candidate.ScoreBreakdown != nil && candidate.ScoreBreakdown.DataFreshness != nil {
-		ageSeconds := int64(candidate.ScoreBreakdown.DataFreshness.OldestFeedHours * 3600)
-		if ageSeconds > pme.config.MaxDataStaleness {
-			return false // Reject stale data
-		}
-	}
-
-	return true
-}
-
-// determineOverallStatus assigns overall status based on scoring and gates
-func (pme *PreMovementEngine) determineOverallStatus(candidate *PreMovementCandidate) string {
-	// BLOCKED: Failed gates or critical issues
-	if candidate.GatesStatus == "BLOCKED" {
-		return "BLOCKED"
-	}
-
-	score := candidate.TotalScore
-	hasConfirmation := candidate.GatesStatus == "CONFIRMED"
-	hasSignificantCVD := candidate.CVDResult != nil && candidate.CVDResult.IsSignificant
-
-	// STRONG: High score + confirmation + strong signals
-	if score >= 85 && hasConfirmation && hasSignificantCVD {
-		return "STRONG"
-	}
-
-	// MODERATE: Good score + confirmation OR high score alone
-	if (score >= 75 && hasConfirmation) || score >= 90 {
-		return "MODERATE" 
-	}
-
-	// WEAK: Below thresholds but not blocked
-	return "WEAK"
-}
-
-// rankCandidates sorts candidates by overall strength and assigns ranks
-func (pme *PreMovementEngine) rankCandidates(candidates []*PreMovementCandidate) {
-	sort.Slice(candidates, func(i, j int) bool {
-		candi, candj := candidates[i], candidates[j]
-
-		// First sort by overall status priority
-		statusPriority := map[string]int{
-			"STRONG":   4,
-			"MODERATE": 3, 
-			"WEAK":     2,
-			"BLOCKED":  1,
-		}
-
-		priI, priJ := statusPriority[candi.OverallStatus], statusPriority[candj.OverallStatus]
-		if priI != priJ {
-			return priI > priJ
-		}
-
-		// Then by Pre-Movement score
-		if candi.TotalScore != candj.TotalScore {
-			return candi.TotalScore > candj.TotalScore
-		}
-
-		// Then by gate precedence score
-		var precedenceI, precedenceJ float64
-		if candi.GatesResult != nil {
-			precedenceI = candi.GatesResult.PrecedenceScore
-		}
-		if candj.GatesResult != nil {
-			precedenceJ = candj.GatesResult.PrecedenceScore
-		}
-
-		if precedenceI != precedenceJ {
-			return precedenceI > precedenceJ
-		}
-
-		// Finally by CVD significance
-		var cvdI, cvdJ float64
-		if candi.CVDResult != nil {
-			cvdI = candi.CVDResult.SignificanceScore
-		}
-		if candj.CVDResult != nil {
-			cvdJ = candj.CVDResult.SignificanceScore
-		}
-
-		return cvdI > cvdJ
-	})
-
-	// Assign ranks
-	for i, candidate := range candidates {
-		candidate.Rank = i + 1
-	}
-}
-
-// assessDataFreshness evaluates overall data quality
-func (pme *PreMovementEngine) assessDataFreshness(candidates []*PreMovementCandidate) *DataFreshnessReport {
-	report := &DataFreshnessReport{}
-
-	if len(candidates) == 0 {
-		report.FreshnessGrade = "N/A"
-		return report
-	}
-
-	var totalAge, oldestAge int64
-	staleCount := 0
-
-	for _, candidate := range candidates {
-		if candidate.ScoreBreakdown != nil && candidate.ScoreBreakdown.DataFreshness != nil {
-			ageSeconds := int64(candidate.ScoreBreakdown.DataFreshness.OldestFeedHours * 3600)
-			totalAge += ageSeconds
-			
-			if ageSeconds > oldestAge {
-				oldestAge = ageSeconds
-			}
-
-			if ageSeconds > pme.config.StaleDataWarning {
-				staleCount++
-			}
-		}
-	}
-
-	report.AverageAgeSeconds = totalAge / int64(len(candidates))
-	report.OldestDataSeconds = oldestAge
-	report.StaleCandidatesCount = staleCount
-	report.StaleCandidatesPct = float64(staleCount) / float64(len(candidates)) * 100.0
-
-	// Assign freshness grade
-	if report.AverageAgeSeconds < 300 { // < 5 min
-		report.FreshnessGrade = "A"
-	} else if report.AverageAgeSeconds < 600 { // < 10 min
-		report.FreshnessGrade = "B"
-	} else if report.AverageAgeSeconds < 1200 { // < 20 min
-		report.FreshnessGrade = "C"
-	} else if report.AverageAgeSeconds < 1800 { // < 30 min
-		report.FreshnessGrade = "D"
-	} else {
-		report.FreshnessGrade = "F"
-	}
-
-	return report
-}
-
-// GetAnalysisSummary returns a concise summary of Pre-Movement analysis
-func (ar *AnalysisResult) GetAnalysisSummary() string {
-	return fmt.Sprintf("Pre-Movement v3.3 Analysis: %d candidates, %d valid, %d strong (freshness: %s, %dms)",
-		ar.TotalCandidates, ar.ValidCandidates, ar.StrongCandidates, 
-		ar.DataFreshness.FreshnessGrade, ar.ProcessTimeMs)
-}
-
-// GetTopCandidatesSummary returns a summary of top N candidates
-func (ar *AnalysisResult) GetTopCandidatesSummary(n int) string {
-	if n > len(ar.Candidates) {
-		n = len(ar.Candidates)
-	}
-
-	summary := fmt.Sprintf("Top %d Pre-Movement Candidates:\n", n)
-	
-	for i := 0; i < n; i++ {
-		candidate := ar.Candidates[i]
-		status := map[string]string{
-			"STRONG":   "üî•",
-			"MODERATE": "üìà", 
-			"WEAK":     "üìä",
-			"BLOCKED":  "‚ùå",
-		}[candidate.OverallStatus]
-
-		gates := "‚ùå"
-		if candidate.GatesStatus == "CONFIRMED" {
-			gates = "‚úÖ"
-		}
-
-		summary += fmt.Sprintf("  %d. %s %s %s | Score: %.1f | Gates: %s\n",
-			candidate.Rank, status, candidate.Symbol, candidate.OverallStatus, 
-			candidate.TotalScore, gates)
-	}
-
-	return summary
-}
-
-// GetCandidateDetails returns detailed analysis for a specific candidate
-func (ar *AnalysisResult) GetCandidateDetails(symbol string) string {
-	for _, candidate := range ar.Candidates {
-		if candidate.Symbol == symbol {
-			report := fmt.Sprintf("Pre-Movement v3.3 Analysis: %s (Rank #%d)\n", symbol, candidate.Rank)
-			report += fmt.Sprintf("Status: %s | Score: %.1f | Gates: %s | Time: %dms\n\n", 
-				candidate.OverallStatus, candidate.TotalScore, candidate.GatesStatus, candidate.ProcessTimeMs)
-
-			// Key reasons
-			if len(candidate.Reasons) > 0 {
-				report += "Key Reasons:\n"
-				for i, reason := range candidate.Reasons {
-					report += fmt.Sprintf("  %d. %s\n", i+1, reason)
-				}
-				report += "\n"
-			}
-
-			// Score breakdown
-			if candidate.ScoreBreakdown != nil {
-				report += candidate.ScoreBreakdown.GetDetailedBreakdown() + "\n"
-			}
-
-			// Gate details
-			if candidate.GatesResult != nil {
-				report += candidate.GatesResult.GetDetailedReport() + "\n"
-			}
-
-			// CVD analysis
-			if candidate.CVDResult != nil {
-				report += candidate.CVDResult.GetDetailedAnalysis() + "\n"
-			}
-
-			// Warnings
-			if len(candidate.Warnings) > 0 {
-				report += "Warnings:\n"
-				for i, warning := range candidate.Warnings {
-					report += fmt.Sprintf("  %d. %s\n", i+1, warning)
-				}
-			}
-
-			return report
-		}
-	}
-
-	return fmt.Sprintf("Candidate %s not found in analysis results", symbol)
-}
\ No newline at end of file
+package premove
+
+import (
+	"context"
+	"fmt"
+	"sort"
+	"time"
+
+	"cryptorun/internal/microstructure"
+)
+
+// PreMovementEngine orchestrates the complete Pre-Movement v3.3 system
+type PreMovementEngine struct {
+	scoreEngine    *ScoreEngine
+	gateEvaluator  *GateEvaluator
+	cvdAnalyzer    *CVDResidualAnalyzer
+	microEvaluator *microstructure.Evaluator
+	config         *EngineConfig
+}
+
+// NewPreMovementEngine creates a complete Pre-Movement v3.3 engine
+func NewPreMovementEngine(
+	microEvaluator *microstructure.Evaluator,
+	config *EngineConfig,
+) *PreMovementEngine {
+	if config == nil {
+		config = DefaultEngineConfig()
+	}
+
+	return &PreMovementEngine{
+		scoreEngine:    NewScoreEngine(config.ScoreConfig),
+		gateEvaluator:  NewGateEvaluator(microEvaluator, config.GateConfig),
+		cvdAnalyzer:    NewCVDResidualAnalyzer(config.CVDConfig),
+		microEvaluator: microEvaluator,
+		config:         config,
+	}
+}
+
+// EngineConfig contains configuration for the complete Pre-Movement system
+type EngineConfig struct {
+	ScoreConfig *ScoreConfig `yaml:"score_config"`
+	GateConfig  *GateConfig  `yaml:"gate_config"`
+	CVDConfig   *CVDConfig   `yaml:"cvd_config"`
+
+	// API limits
+	MaxCandidates    int   `yaml:"max_candidates"`      // 50 max candidates returned
+	MaxProcessTimeMs int64 `yaml:"max_process_time_ms"` // 2000ms max processing time
+	RequireScore     bool  `yaml:"require_score"`       // true - require valid score
+	RequireGates     bool  `yaml:"require_gates"`       // true - require gate pass
+
+	// Data freshness requirements
+	MaxDataStaleness int64 `yaml:"max_data_staleness"` // 1800 seconds (30 min)
+	StaleDataWarning int64 `yaml:"stale_data_warning"` // 600 seconds (10 min)
+}
+
+// DefaultEngineConfig returns production Pre-Movement engine configuration
+func DefaultEngineConfig() *EngineConfig {
+	return &EngineConfig{
+		ScoreConfig: DefaultScoreConfig(),
+		GateConfig:  DefaultGateConfig(),
+		CVDConfig:   DefaultCVDConfig(),
+
+		// API limits
+		MaxCandidates:    50,
+		MaxProcessTimeMs: 2000, // 2 seconds max
+		RequireScore:     true,
+		RequireGates:     true,
+
+		// Data freshness
+		MaxDataStaleness: 1800, // 30 minutes
+		StaleDataWarning: 600,  // 10 minutes
+	}
+}
+
+// CandidateInput contains all data required for Pre-Movement analysis
+type CandidateInput struct {
+	Symbol    string    `json:"symbol"`
+	Timestamp time.Time `json:"timestamp"`
+
+	// Pre-Movement scoring data
+	PreMovementData *PreMovementData `json:"premove_data"`
+
+	// Gate confirmation data
+	ConfirmationData *ConfirmationData `json:"confirmation_data"`
+
+	// CVD residual data
+	CVDDataPoints []*CVDDataPoint `json:"cvd_data_points"`
+}
+
+// PreMovementCandidate represents a complete analyzed candidate
+type PreMovementCandidate struct {
+	Symbol         string                           `json:"symbol"`
+	Timestamp      time.Time                        `json:"timestamp"`
+	TotalScore     float64                          `json:"total_score"`     // 0-100 Pre-Movement score
+	ScoreBreakdown *ScoreResult                     `json:"score_breakdown"` // Detailed scoring
+	GatesStatus    string                           `json:"gates_status"`    // "CONFIRMED", "BLOCKED", "WARNING"
+	GatesResult    *ConfirmationResult              `json:"gates_result"`    // Gate evaluation details
+	CVDResult      *CVDResidualResult               `json:"cvd_result"`      // CVD residual analysis
+	MicroReport    *microstructure.EvaluationResult `json:"micro_report"`    // Microstructure consultation
+	OverallStatus  string                           `json:"overall_status"`  // "STRONG", "MODERATE", "WEAK", "BLOCKED"
+	Reasons        []string                         `json:"reasons"`         // Key reasons for recommendation
+	Warnings       []string                         `json:"warnings"`        // Data quality or analysis warnings
+	ProcessTimeMs  int64                            `json:"process_time_ms"` // Individual processing time
+	Rank           int                              `json:"rank"`            // Rank among candidates (1=best)
+}
+
+// AnalysisResult contains complete Pre-Movement analysis results
+type AnalysisResult struct {
+	Timestamp        time.Time               `json:"timestamp"`
+	TotalCandidates  int                     `json:"total_candidates"`  // Total candidates analyzed
+	ValidCandidates  int                     `json:"valid_candidates"`  // Candidates passing filters
+	StrongCandidates int                     `json:"strong_candidates"` // High-confidence candidates
+	Candidates       []*PreMovementCandidate `json:"candidates"`        // Ranked candidate list
+	ProcessTimeMs    int64                   `json:"process_time_ms"`   // Total processing time
+	DataFreshness    *DataFreshnessReport    `json:"data_freshness"`    // Data quality summary
+	SystemWarnings   []string                `json:"system_warnings"`   // System-level warnings
+}
+
+// DataFreshnessReport summarizes data quality across all candidates
+type DataFreshnessReport struct {
+	AverageAgeSeconds    int64   `json:"average_age_seconds"`    // Average data age
+	StaleCandidatesCount int     `json:"stale_candidates_count"` // Candidates with stale data
+	StaleCandidatesPct   float64 `json:"stale_candidates_pct"`   // % stale candidates
+	OldestDataSeconds    int64   `json:"oldest_data_seconds"`    // Oldest data point
+	FreshnessGrade       string  `json:"freshness_grade"`        // "A", "B", "C", "D", "F"
+}
+
+// ListCandidates performs complete Pre-Movement analysis and returns ranked candidates
+func (pme *PreMovementEngine) ListCandidates(ctx context.Context, inputs []*CandidateInput, limit int) (*AnalysisResult, error) {
+	startTime := time.Now()
+
+	if limit <= 0 || limit > pme.config.MaxCandidates {
+		limit = pme.config.MaxCandidates
+	}
+
+	result := &AnalysisResult{
+		Timestamp:       time.Now(),
+		TotalCandidates: len(inputs),
+		Candidates:      make([]*PreMovementCandidate, 0, len(inputs)),
+		SystemWarnings:  []string{},
+	}
+
+	// Process each candidate
+	for _, input := range inputs {
+		candidate, err := pme.analyzeCandidate(ctx, input)
+		if err != nil {
+			result.SystemWarnings = append(result.SystemWarnings,
+				fmt.Sprintf("%s: analysis failed - %v", input.Symbol, err))
+			continue
+		}
+
+		// Apply filters
+		if pme.shouldIncludeCandidate(candidate) {
+			result.Candidates = append(result.Candidates, candidate)
+		}
+	}
+
+	result.ValidCandidates = len(result.Candidates)
+
+	// Rank candidates by overall strength
+	pme.rankCandidates(result.Candidates)
+
+	// Limit results
+	if limit < len(result.Candidates) {
+		result.Candidates = result.Candidates[:limit]
+	}
+
+	// Count strong candidates (top tier)
+	for _, candidate := range result.Candidates {
+		if candidate.OverallStatus == "STRONG" {
+			result.StrongCandidates++
+		}
+	}
+
+	// Assess data freshness
+	result.DataFreshness = pme.assessDataFreshness(result.Candidates)
+
+	result.ProcessTimeMs = time.Since(startTime).Milliseconds()
+
+	// Performance warning
+	if result.ProcessTimeMs > pme.config.MaxProcessTimeMs {
+		result.SystemWarnings = append(result.SystemWarnings,
+			fmt.Sprintf("Analysis took %dms (>%dms threshold)", result.ProcessTimeMs, pme.config.MaxProcessTimeMs))
+	}
+
+	return result, nil
+}
+
+// analyzeCandidate performs complete Pre-Movement analysis for a single candidate
+func (pme *PreMovementEngine) analyzeCandidate(ctx context.Context, input *CandidateInput) (*PreMovementCandidate, error) {
+	startTime := time.Now()
+
+	candidate := &PreMovementCandidate{
+		Symbol:    input.Symbol,
+		Timestamp: time.Now(),
+		Reasons:   []string{},
+		Warnings:  []string{},
+	}
+
+	// 1. Score calculation
+	if input.PreMovementData != nil {
+		scoreResult, err := pme.scoreEngine.CalculateScore(ctx, input.PreMovementData)
+		if err != nil {
+			return nil, fmt.Errorf("scoring failed: %w", err)
+		}
+		candidate.ScoreBreakdown = scoreResult
+		candidate.TotalScore = scoreResult.TotalScore
+
+		// Add scoring reasons
+		if scoreResult.TotalScore >= 75 {
+			candidate.Reasons = append(candidate.Reasons, fmt.Sprintf("High Pre-Movement score (%.1f)", scoreResult.TotalScore))
+		}
+	}
+
+	// 2. Gate evaluation
+	if input.ConfirmationData != nil {
+		gateResult, err := pme.gateEvaluator.EvaluateConfirmation(ctx, input.ConfirmationData)
+		if err != nil {
+			return nil, fmt.Errorf("gate evaluation failed: %w", err)
+		}
+		candidate.GatesResult = gateResult
+
+		if gateResult.Passed {
+			candidate.GatesStatus = "CONFIRMED"
+			candidate.Reasons = append(candidate.Reasons,
+				fmt.Sprintf("%d-of-%d confirmations passed", gateResult.ConfirmationCount, gateResult.RequiredCount))
+		} else {
+			candidate.GatesStatus = "BLOCKED"
+		}
+
+		// Collect gate warnings
+		candidate.Warnings = append(candidate.Warnings, gateResult.Warnings...)
+	}
+
+	// 3. CVD residual analysis
+	if input.CVDDataPoints != nil && len(input.CVDDataPoints) > 0 {
+		cvdResult, err := pme.cvdAnalyzer.AnalyzeCVDResidual(ctx, input.Symbol, input.CVDDataPoints)
+		if err != nil {
+			candidate.Warnings = append(candidate.Warnings, fmt.Sprintf("CVD analysis failed: %v", err))
+		} else {
+			candidate.CVDResult = cvdResult
+
+			if cvdResult.IsSignificant {
+				candidate.Reasons = append(candidate.Reasons,
+					fmt.Sprintf("Significant CVD residual (%.1f%%)", cvdResult.PercentileRank))
+			}
+
+			// Collect CVD warnings
+			candidate.Warnings = append(candidate.Warnings, cvdResult.Warnings...)
+		}
+	}
+
+	// 4. Microstructure consultation (from gates result)
+	if candidate.GatesResult != nil && candidate.GatesResult.MicroReport != nil {
+		candidate.MicroReport = candidate.GatesResult.MicroReport
+	}
+
+	// 5. Determine overall status
+	candidate.OverallStatus = pme.determineOverallStatus(candidate)
+
+	candidate.ProcessTimeMs = time.Since(startTime).Milliseconds()
+
+	return candidate, nil
+}
+
+// shouldIncludeCandidate applies filters to determine if candidate should be included
+func (pme *PreMovementEngine) shouldIncludeCandidate(candidate *PreMovementCandidate) bool {
+	// Require valid score if configured
+	if pme.config.RequireScore {
+		if candidate.ScoreBreakdown == nil || !candidate.ScoreBreakdown.IsValid {
+			return false
+		}
+	}
+
+	// Require gate confirmation if configured
+	if pme.config.RequireGates {
+		if candidate.GatesResult == nil || !candidate.GatesResult.Passed {
+			return false
+		}
+	}
+
+	// Check data freshness
+	if candidate.ScoreBreakdown != nil && candidate.ScoreBreakdown.DataFreshness != nil {
+		ageSeconds := int64(candidate.ScoreBreakdown.DataFreshness.OldestFeedHours * 3600)
+		if ageSeconds > pme.config.MaxDataStaleness {
+			return false // Reject stale data
+		}
+	}
+
+	return true
+}
+
+// determineOverallStatus assigns overall status based on scoring and gates
+func (pme *PreMovementEngine) determineOverallStatus(candidate *PreMovementCandidate) string {
+	// BLOCKED: Failed gates or critical issues
+	if candidate.GatesStatus == "BLOCKED" {
+		return "BLOCKED"
+	}
+
+	score := candidate.TotalScore
+	hasConfirmation := candidate.GatesStatus == "CONFIRMED"
+	hasSignificantCVD := candidate.CVDResult != nil && candidate.CVDResult.IsSignificant
+
+	// STRONG: High score + confirmation + strong signals
+	if score >= 85 && hasConfirmation && hasSignificantCVD {
+		return "STRONG"
+	}
+
+	// MODERATE: Good score + confirmation OR high score alone
+	if (score >= 75 && hasConfirmation) || score >= 90 {
+		return "MODERATE"
+	}
+
+	// WEAK: Below thresholds but not blocked
+	return "WEAK"
+}
+
+// rankCandidates sorts candidates by overall strength and assigns ranks
+func (pme *PreMovementEngine) rankCandidates(candidates []*PreMovementCandidate) {
+	sort.Slice(candidates, func(i, j int) bool {
+		candi, candj := candidates[i], candidates[j]
+
+		// First sort by overall status priority
+		statusPriority := map[string]int{
+			"STRONG":   4,
+			"MODERATE": 3,
+			"WEAK":     2,
+			"BLOCKED":  1,
+		}
+
+		priI, priJ := statusPriority[candi.OverallStatus], statusPriority[candj.OverallStatus]
+		if priI != priJ {
+			return priI > priJ
+		}
+
+		// Then by Pre-Movement score
+		if candi.TotalScore != candj.TotalScore {
+			return candi.TotalScore > candj.TotalScore
+		}
+
+		// Then by gate precedence score
+		var precedenceI, precedenceJ float64
+		if candi.GatesResult != nil {
+			precedenceI = candi.GatesResult.PrecedenceScore
+		}
+		if candj.GatesResult != nil {
+			precedenceJ = candj.GatesResult.PrecedenceScore
+		}
+
+		if precedenceI != precedenceJ {
+			return precedenceI > precedenceJ
+		}
+
+		// Finally by CVD significance
+		var cvdI, cvdJ float64
+		if candi.CVDResult != nil {
+			cvdI = candi.CVDResult.SignificanceScore
+		}
+		if candj.CVDResult != nil {
+			cvdJ = candj.CVDResult.SignificanceScore
+		}
+
+		return cvdI > cvdJ
+	})
+
+	// Assign ranks
+	for i, candidate := range candidates {
+		candidate.Rank = i + 1
+	}
+}
+
+// assessDataFreshness evaluates overall data quality
+func (pme *PreMovementEngine) assessDataFreshness(candidates []*PreMovementCandidate) *DataFreshnessReport {
+	report := &DataFreshnessReport{}
+
+	if len(candidates) == 0 {
+		report.FreshnessGrade = "N/A"
+		return report
+	}
+
+	var totalAge, oldestAge int64
+	staleCount := 0
+
+	for _, candidate := range candidates {
+		if candidate.ScoreBreakdown != nil && candidate.ScoreBreakdown.DataFreshness != nil {
+			ageSeconds := int64(candidate.ScoreBreakdown.DataFreshness.OldestFeedHours * 3600)
+			totalAge += ageSeconds
+
+			if ageSeconds > oldestAge {
+				oldestAge = ageSeconds
+			}
+
+			if ageSeconds > pme.config.StaleDataWarning {
+				staleCount++
+			}
+		}
+	}
+
+	report.AverageAgeSeconds = totalAge / int64(len(candidates))
+	report.OldestDataSeconds = oldestAge
+	report.StaleCandidatesCount = staleCount
+	report.StaleCandidatesPct = float64(staleCount) / float64(len(candidates)) * 100.0
+
+	// Assign freshness grade
+	if report.AverageAgeSeconds < 300 { // < 5 min
+		report.FreshnessGrade = "A"
+	} else if report.AverageAgeSeconds < 600 { // < 10 min
+		report.FreshnessGrade = "B"
+	} else if report.AverageAgeSeconds < 1200 { // < 20 min
+		report.FreshnessGrade = "C"
+	} else if report.AverageAgeSeconds < 1800 { // < 30 min
+		report.FreshnessGrade = "D"
+	} else {
+		report.FreshnessGrade = "F"
+	}
+
+	return report
+}
+
+// GetAnalysisSummary returns a concise summary of Pre-Movement analysis
+func (ar *AnalysisResult) GetAnalysisSummary() string {
+	return fmt.Sprintf("Pre-Movement v3.3 Analysis: %d candidates, %d valid, %d strong (freshness: %s, %dms)",
+		ar.TotalCandidates, ar.ValidCandidates, ar.StrongCandidates,
+		ar.DataFreshness.FreshnessGrade, ar.ProcessTimeMs)
+}
+
+// GetTopCandidatesSummary returns a summary of top N candidates
+func (ar *AnalysisResult) GetTopCandidatesSummary(n int) string {
+	if n > len(ar.Candidates) {
+		n = len(ar.Candidates)
+	}
+
+	summary := fmt.Sprintf("Top %d Pre-Movement Candidates:\n", n)
+
+	for i := 0; i < n; i++ {
+		candidate := ar.Candidates[i]
+		status := map[string]string{
+			"STRONG":   "üî•",
+			"MODERATE": "üìà",
+			"WEAK":     "üìä",
+			"BLOCKED":  "‚ùå",
+		}[candidate.OverallStatus]
+
+		gates := "‚ùå"
+		if candidate.GatesStatus == "CONFIRMED" {
+			gates = "‚úÖ"
+		}
+
+		summary += fmt.Sprintf("  %d. %s %s %s | Score: %.1f | Gates: %s\n",
+			candidate.Rank, status, candidate.Symbol, candidate.OverallStatus,
+			candidate.TotalScore, gates)
+	}
+
+	return summary
+}
+
+// GetCandidateDetails returns detailed analysis for a specific candidate
+func (ar *AnalysisResult) GetCandidateDetails(symbol string) string {
+	for _, candidate := range ar.Candidates {
+		if candidate.Symbol == symbol {
+			report := fmt.Sprintf("Pre-Movement v3.3 Analysis: %s (Rank #%d)\n", symbol, candidate.Rank)
+			report += fmt.Sprintf("Status: %s | Score: %.1f | Gates: %s | Time: %dms\n\n",
+				candidate.OverallStatus, candidate.TotalScore, candidate.GatesStatus, candidate.ProcessTimeMs)
+
+			// Key reasons
+			if len(candidate.Reasons) > 0 {
+				report += "Key Reasons:\n"
+				for i, reason := range candidate.Reasons {
+					report += fmt.Sprintf("  %d. %s\n", i+1, reason)
+				}
+				report += "\n"
+			}
+
+			// Score breakdown
+			if candidate.ScoreBreakdown != nil {
+				report += candidate.ScoreBreakdown.GetDetailedBreakdown() + "\n"
+			}
+
+			// Gate details
+			if candidate.GatesResult != nil {
+				report += candidate.GatesResult.GetDetailedReport() + "\n"
+			}
+
+			// CVD analysis
+			if candidate.CVDResult != nil {
+				report += candidate.CVDResult.GetDetailedAnalysis() + "\n"
+			}
+
+			// Warnings
+			if len(candidate.Warnings) > 0 {
+				report += "Warnings:\n"
+				for i, warning := range candidate.Warnings {
+					report += fmt.Sprintf("  %d. %s\n", i+1, warning)
+				}
+			}
+
+			return report
+		}
+	}
+
+	return fmt.Sprintf("Candidate %s not found in analysis results", symbol)
+}
diff --git a/internal/premove/api_test.go b/internal/premove/api_test.go
index 2d5130a..1d8acbe 100644
--- a/internal/premove/api_test.go
+++ b/internal/premove/api_test.go
@@ -1,551 +1,549 @@
-package premove
-
-import (
-	"context"
-	"testing"
-	"time"
-
-	"github.com/stretchr/testify/assert"
-	"github.com/stretchr/testify/require"
-
-	"cryptorun/internal/microstructure"
-)
-
-func TestPreMovementEngine_ListCandidates_FullPipeline(t *testing.T) {
-	// Create mock microstructure evaluator
-	mockMicro := &mockMicroEvaluator{
-		result: &microstructure.EvaluationResult{
-			Symbol:    "BTC-USD",
-			SpreadBps: 20.0,
-			DepthUSD:  300000.0,
-			VADR:      2.2,
-		},
-	}
-
-	engine := NewPreMovementEngine(mockMicro, nil)
-
-	// Create strong candidate inputs
-	inputs := []*CandidateInput{
-		{
-			Symbol:    "BTC-USD",
-			Timestamp: time.Now(),
-			PreMovementData: &PreMovementData{
-				Symbol:    "BTC-USD",
-				Timestamp: time.Now(),
-
-				// Strong structural signals (targeting ~35 points)
-				FundingZScore:      3.2,  // Strong funding divergence
-				OIResidual:         1.2e6, // $1.2M OI residual
-				ETFFlowTint:        0.8,  // 80% bullish flows
-				ReserveChange7d:    -12.0, // -12% exchange reserves
-				WhaleComposite:     0.85, // 85% whale activity
-				MicroDynamics:      0.7,  // 70% L1/L2 stress
-
-				// Strong behavioral signals (targeting ~30 points)
-				SmartMoneyFlow:     0.8,  // 80% institutional flow
-				CVDResidual:        0.6,  // 60% CVD residual
-
-				// Strong catalyst & compression (targeting ~20 points)
-				CatalystHeat:       0.8,  // 80% catalyst significance
-				VolCompressionRank: 0.9,  // 90th percentile compression
-
-				OldestFeedHours: 0.5, // Fresh data
-			},
-			ConfirmationData: &ConfirmationData{
-				Symbol:    "BTC-USD",
-				Timestamp: time.Now(),
-
-				// Strong 2-of-3 confirmations
-				FundingZScore:  3.2, // Strong funding
-				WhaleComposite: 0.85, // Strong whale activity
-				SupplyProxyScore: 0.7, // Strong supply squeeze
-
-				// Strong supply squeeze components
-				ReserveChange7d:     -12.0,
-				LargeWithdrawals24h: 80e6,
-				StakingInflow24h:    15e6,
-				DerivativesOIChange: 20.0,
-
-				// Volume confirmation in supportive regime
-				VolumeRatio24h: 3.5,
-				CurrentRegime:  "risk_off",
-
-				SpreadBps: 20.0,
-				DepthUSD:  300000.0,
-				VADR:      2.2,
-			},
-			CVDDataPoints: generateSyntheticCVDData(80, 0.8), // Good regression data
-		},
-		{
-			Symbol:    "ETH-USD",
-			Timestamp: time.Now(),
-			PreMovementData: &PreMovementData{
-				Symbol:    "ETH-USD",
-				Timestamp: time.Now(),
-
-				// Moderate signals (targeting ~60 points)
-				FundingZScore:      2.1,
-				OIResidual:         600000,
-				ETFFlowTint:        0.5,
-				ReserveChange7d:    -6.0,
-				WhaleComposite:     0.6,
-				MicroDynamics:      0.4,
-				SmartMoneyFlow:     0.5,
-				CVDResidual:        0.3,
-				CatalystHeat:       0.4,
-				VolCompressionRank: 0.6,
-				OldestFeedHours:    1.2,
-			},
-			ConfirmationData: &ConfirmationData{
-				Symbol:    "ETH-USD",
-				Timestamp: time.Now(),
-
-				// Moderate confirmations (only 2-of-3)
-				FundingZScore:       2.1,  // Pass
-				WhaleComposite:      0.75, // Pass
-				SupplyProxyScore:    0.4,  // Fail
-				ReserveChange7d:     -3.0,
-				LargeWithdrawals24h: 30e6,
-				StakingInflow24h:    8e6,
-				DerivativesOIChange: 10.0,
-				VolumeRatio24h:      1.8,
-				CurrentRegime:       "normal",
-			},
-			CVDDataPoints: generateSyntheticCVDData(60, 0.6), // Moderate regression
-		},
-		{
-			Symbol:    "SOL-USD",
-			Timestamp: time.Now(),
-			PreMovementData: &PreMovementData{
-				Symbol:    "SOL-USD", 
-				Timestamp: time.Now(),
-
-				// Weak signals (targeting ~30 points)
-				FundingZScore:      1.2,
-				OIResidual:         200000,
-				ETFFlowTint:        0.2,
-				ReserveChange7d:    -2.0,
-				WhaleComposite:     0.3,
-				MicroDynamics:      0.2,
-				SmartMoneyFlow:     0.2,
-				CVDResidual:        0.1,
-				CatalystHeat:       0.2,
-				VolCompressionRank: 0.3,
-				OldestFeedHours:    0.8,
-			},
-			ConfirmationData: &ConfirmationData{
-				Symbol:    "SOL-USD",
-				Timestamp: time.Now(),
-
-				// Weak confirmations (only 1-of-3)
-				FundingZScore:       1.2, // Fail
-				WhaleComposite:      0.5, // Fail  
-				SupplyProxyScore:    0.3, // Fail
-				ReserveChange7d:     -2.0,
-				LargeWithdrawals24h: 20e6,
-				StakingInflow24h:    5e6,
-				DerivativesOIChange: 8.0,
-				VolumeRatio24h:      1.2,
-				CurrentRegime:       "normal",
-			},
-			CVDDataPoints: generateNoisyCVDData(40), // Should trigger fallback
-		},
-	}
-
-	result, err := engine.ListCandidates(context.Background(), inputs, 10)
-	require.NoError(t, err)
-	assert.NotNil(t, result)
-
-	// Should process all candidates
-	assert.Equal(t, 3, result.TotalCandidates)
-
-	// BTC and ETH should pass (strong and moderate), SOL should be filtered out
-	assert.Equal(t, 2, result.ValidCandidates, "Should have 2 valid candidates")
-	assert.Equal(t, 1, result.StrongCandidates, "Should have 1 strong candidate (BTC)")
-
-	// Check ranking - BTC should be first (strongest)
-	require.Len(t, result.Candidates, 2)
-	assert.Equal(t, "BTC-USD", result.Candidates[0].Symbol, "BTC should rank first")
-	assert.Equal(t, "STRONG", result.Candidates[0].OverallStatus)
-	assert.Equal(t, 1, result.Candidates[0].Rank)
-
-	assert.Equal(t, "ETH-USD", result.Candidates[1].Symbol, "ETH should rank second")
-	assert.Equal(t, "MODERATE", result.Candidates[1].OverallStatus)  
-	assert.Equal(t, 2, result.Candidates[1].Rank)
-
-	// Check data freshness assessment
-	assert.NotNil(t, result.DataFreshness)
-	assert.Greater(t, result.ProcessTimeMs, int64(0), "Should report processing time")
-}
-
-func TestPreMovementEngine_AnalyzeCandidate_ScoreBreakdown(t *testing.T) {
-	mockMicro := &mockMicroEvaluator{
-		result: &microstructure.EvaluationResult{
-			Symbol:    "TEST-USD",
-			SpreadBps: 30.0,
-			DepthUSD:  150000.0,
-			VADR:      1.9,
-		},
-	}
-
-	engine := NewPreMovementEngine(mockMicro, nil)
-
-	input := &CandidateInput{
-		Symbol:    "TEST-USD",
-		Timestamp: time.Now(),
-		PreMovementData: &PreMovementData{
-			Symbol:             "TEST-USD",
-			Timestamp:          time.Now(),
-			FundingZScore:      2.5,
-			OIResidual:         800000,
-			ETFFlowTint:        0.6,
-			ReserveChange7d:    -8.0,
-			WhaleComposite:     0.7,
-			MicroDynamics:      0.5,
-			SmartMoneyFlow:     0.6,
-			CVDResidual:        0.4,
-			CatalystHeat:       0.6,
-			VolCompressionRank: 0.7,
-			OldestFeedHours:    1.0,
-		},
-		ConfirmationData: &ConfirmationData{
-			Symbol:         "TEST-USD",
-			Timestamp:      time.Now(),
-			FundingZScore:  2.5,
-			WhaleComposite: 0.7,
-			SupplyProxyScore: 0.4, // Will calculate from components
-			ReserveChange7d:     -8.0,
-			LargeWithdrawals24h: 60e6,
-			StakingInflow24h:    12e6,
-			DerivativesOIChange: 18.0,
-			VolumeRatio24h:      2.0,
-			CurrentRegime:       "normal",
-		},
-		CVDDataPoints: generateSyntheticCVDData(70, 0.7),
-	}
-
-	candidate, err := engine.analyzeCandidate(context.Background(), input)
-	require.NoError(t, err)
-	assert.NotNil(t, candidate)
-
-	// Should have complete analysis
-	assert.NotNil(t, candidate.ScoreBreakdown, "Should have score breakdown")
-	assert.NotNil(t, candidate.GatesResult, "Should have gates result")
-	assert.NotNil(t, candidate.CVDResult, "Should have CVD result")
-
-	// Score should be reasonable for moderate signals
-	assert.Greater(t, candidate.TotalScore, 50.0, "Moderate signals should yield >50 score")
-	assert.Less(t, candidate.TotalScore, 85.0, "Moderate signals should yield <85 score")
-
-	// Should have reasons for recommendation
-	assert.Greater(t, len(candidate.Reasons), 0, "Should have recommendation reasons")
-
-	// Should determine appropriate overall status
-	assert.Contains(t, []string{"MODERATE", "WEAK"}, candidate.OverallStatus)
-
-	assert.Greater(t, candidate.ProcessTimeMs, int64(0), "Should report processing time")
-}
-
-func TestPreMovementEngine_DetermineOverallStatus(t *testing.T) {
-	engine := NewPreMovementEngine(nil, nil)
-
-	// Test STRONG status (high score + confirmation + significant CVD)
-	strongCandidate := &PreMovementCandidate{
-		TotalScore:  88.0,
-		GatesStatus: "CONFIRMED",
-		CVDResult:   &CVDResidualResult{IsSignificant: true},
-	}
-	assert.Equal(t, "STRONG", engine.determineOverallStatus(strongCandidate))
-
-	// Test MODERATE status (good score + confirmation)
-	moderateCandidate := &PreMovementCandidate{
-		TotalScore:  78.0,
-		GatesStatus: "CONFIRMED",
-		CVDResult:   &CVDResidualResult{IsSignificant: false},
-	}
-	assert.Equal(t, "MODERATE", engine.determineOverallStatus(moderateCandidate))
-
-	// Test MODERATE status (very high score alone)
-	highScoreCandidate := &PreMovementCandidate{
-		TotalScore:  92.0,
-		GatesStatus: "BLOCKED",
-		CVDResult:   nil,
-	}
-	assert.Equal(t, "MODERATE", engine.determineOverallStatus(highScoreCandidate))
-
-	// Test BLOCKED status
-	blockedCandidate := &PreMovementCandidate{
-		TotalScore:  85.0,
-		GatesStatus: "BLOCKED",
-		CVDResult:   &CVDResidualResult{IsSignificant: true},
-	}
-	assert.Equal(t, "BLOCKED", engine.determineOverallStatus(blockedCandidate))
-
-	// Test WEAK status
-	weakCandidate := &PreMovementCandidate{
-		TotalScore:  65.0,
-		GatesStatus: "CONFIRMED",
-		CVDResult:   &CVDResidualResult{IsSignificant: false},
-	}
-	assert.Equal(t, "WEAK", engine.determineOverallStatus(weakCandidate))
-}
-
-func TestPreMovementEngine_RankCandidates(t *testing.T) {
-	engine := NewPreMovementEngine(nil, nil)
-
-	candidates := []*PreMovementCandidate{
-		{
-			Symbol:        "WEAK",
-			TotalScore:    60.0,
-			OverallStatus: "WEAK",
-			GatesResult:   &ConfirmationResult{PrecedenceScore: 2.0},
-			CVDResult:     &CVDResidualResult{SignificanceScore: 0.3},
-		},
-		{
-			Symbol:        "STRONG",
-			TotalScore:    90.0,
-			OverallStatus: "STRONG", 
-			GatesResult:   &ConfirmationResult{PrecedenceScore: 6.0},
-			CVDResult:     &CVDResidualResult{SignificanceScore: 0.8},
-		},
-		{
-			Symbol:        "MODERATE",
-			TotalScore:    75.0,
-			OverallStatus: "MODERATE",
-			GatesResult:   &ConfirmationResult{PrecedenceScore: 5.0},
-			CVDResult:     &CVDResidualResult{SignificanceScore: 0.5},
-		},
-		{
-			Symbol:        "BLOCKED",
-			TotalScore:    85.0,
-			OverallStatus: "BLOCKED",
-			GatesResult:   &ConfirmationResult{PrecedenceScore: 3.0},
-			CVDResult:     nil,
-		},
-	}
-
-	engine.rankCandidates(candidates)
-
-	// Check ranking order
-	assert.Equal(t, "STRONG", candidates[0].Symbol, "STRONG should rank first")
-	assert.Equal(t, 1, candidates[0].Rank)
-
-	assert.Equal(t, "MODERATE", candidates[1].Symbol, "MODERATE should rank second")
-	assert.Equal(t, 2, candidates[1].Rank)
-
-	assert.Equal(t, "WEAK", candidates[2].Symbol, "WEAK should rank third")
-	assert.Equal(t, 3, candidates[2].Rank)
-
-	assert.Equal(t, "BLOCKED", candidates[3].Symbol, "BLOCKED should rank last")
-	assert.Equal(t, 4, candidates[3].Rank)
-}
-
-func TestPreMovementEngine_AssessDataFreshness(t *testing.T) {
-	engine := NewPreMovementEngine(nil, nil)
-
-	candidates := []*PreMovementCandidate{
-		{
-			Symbol: "FRESH",
-			ScoreBreakdown: &ScoreResult{
-				DataFreshness: &FreshnessInfo{OldestFeedHours: 0.2}, // 12 minutes
-			},
-		},
-		{
-			Symbol: "MODERATE",
-			ScoreBreakdown: &ScoreResult{
-				DataFreshness: &FreshnessInfo{OldestFeedHours: 0.8}, // 48 minutes
-			},
-		},
-		{
-			Symbol: "STALE",
-			ScoreBreakdown: &ScoreResult{
-				DataFreshness: &FreshnessInfo{OldestFeedHours: 1.2}, // 72 minutes
-			},
-		},
-	}
-
-	report := engine.assessDataFreshness(candidates)
-	assert.NotNil(t, report)
-
-	// Should calculate average age
-	expectedAvgSeconds := int64((0.2 + 0.8 + 1.2) / 3.0 * 3600) // Average in seconds
-	assert.Equal(t, expectedAvgSeconds, report.AverageAgeSeconds)
-
-	// Should find oldest data
-	assert.Equal(t, int64(1.2*3600), report.OldestDataSeconds)
-
-	// Should count stale candidates (>10 min warning threshold)
-	assert.Equal(t, 2, report.StaleCandidatesCount, "Should count 2 stale candidates")
-	assert.InDelta(t, 66.7, report.StaleCandidatesPct, 1.0, "Should calculate stale percentage")
-
-	// Should assign reasonable freshness grade
-	assert.Contains(t, []string{"A", "B", "C", "D", "F"}, report.FreshnessGrade)
-}
-
-func TestAnalysisResult_GetAnalysisSummary(t *testing.T) {
-	result := &AnalysisResult{
-		TotalCandidates:  5,
-		ValidCandidates:  3,
-		StrongCandidates: 1,
-		ProcessTimeMs:    234,
-		DataFreshness:    &DataFreshnessReport{FreshnessGrade: "B"},
-	}
-
-	summary := result.GetAnalysisSummary()
-	assert.Contains(t, summary, "Pre-Movement v3.3 Analysis")
-	assert.Contains(t, summary, "5 candidates")
-	assert.Contains(t, summary, "3 valid")
-	assert.Contains(t, summary, "1 strong")
-	assert.Contains(t, summary, "freshness: B")
-	assert.Contains(t, summary, "234ms")
-}
-
-func TestAnalysisResult_GetTopCandidatesSummary(t *testing.T) {
-	result := &AnalysisResult{
-		Candidates: []*PreMovementCandidate{
-			{
-				Rank:          1,
-				Symbol:        "BTC-USD",
-				OverallStatus: "STRONG",
-				TotalScore:    88.5,
-				GatesStatus:   "CONFIRMED",
-			},
-			{
-				Rank:          2,
-				Symbol:        "ETH-USD", 
-				OverallStatus: "MODERATE",
-				TotalScore:    76.2,
-				GatesStatus:   "CONFIRMED",
-			},
-			{
-				Rank:          3,
-				Symbol:        "SOL-USD",
-				OverallStatus: "WEAK",
-				TotalScore:    58.1,
-				GatesStatus:   "BLOCKED",
-			},
-		},
-	}
-
-	summary := result.GetTopCandidatesSummary(2)
-	assert.Contains(t, summary, "Top 2 Pre-Movement Candidates")
-	assert.Contains(t, summary, "1. üî• BTC-USD STRONG | Score: 88.5 | Gates: ‚úÖ")
-	assert.Contains(t, summary, "2. üìà ETH-USD MODERATE | Score: 76.2 | Gates: ‚úÖ")
-	assert.NotContains(t, summary, "SOL-USD") // Should not include 3rd candidate
-}
-
-func TestAnalysisResult_GetCandidateDetails(t *testing.T) {
-	result := &AnalysisResult{
-		Candidates: []*PreMovementCandidate{
-			{
-				Rank:          1,
-				Symbol:        "BTC-USD",
-				OverallStatus: "STRONG",
-				TotalScore:    88.5,
-				GatesStatus:   "CONFIRMED",
-				ProcessTimeMs: 145,
-				Reasons:       []string{"High Pre-Movement score (88.5)", "Strong confirmations"},
-				ScoreBreakdown: &ScoreResult{
-					Symbol:     "BTC-USD",
-					TotalScore: 88.5,
-					ComponentScores: map[string]float64{
-						"derivatives": 12.5,
-						"smart_money": 18.2,
-					},
-				},
-			},
-		},
-	}
-
-	details := result.GetCandidateDetails("BTC-USD")
-	assert.Contains(t, details, "BTC-USD (Rank #1)")
-	assert.Contains(t, details, "Status: STRONG")
-	assert.Contains(t, details, "Score: 88.5")
-	assert.Contains(t, details, "Gates: CONFIRMED")
-	assert.Contains(t, details, "Time: 145ms")
-	assert.Contains(t, details, "High Pre-Movement score (88.5)")
-
-	// Test non-existent candidate
-	notFound := result.GetCandidateDetails("UNKNOWN")
-	assert.Contains(t, notFound, "Candidate UNKNOWN not found")
-}
-
-func TestDefaultEngineConfig(t *testing.T) {
-	config := DefaultEngineConfig()
-	require.NotNil(t, config)
-
-	// Should have all sub-configs
-	assert.NotNil(t, config.ScoreConfig)
-	assert.NotNil(t, config.GateConfig)
-	assert.NotNil(t, config.CVDConfig)
-
-	// Check API limits
-	assert.Equal(t, 50, config.MaxCandidates)
-	assert.Equal(t, int64(2000), config.MaxProcessTimeMs)
-	assert.True(t, config.RequireScore)
-	assert.True(t, config.RequireGates)
-
-	// Check data freshness requirements
-	assert.Equal(t, int64(1800), config.MaxDataStaleness)  // 30 minutes
-	assert.Equal(t, int64(600), config.StaleDataWarning)   // 10 minutes
-}
-
-func TestPreMovementEngine_PerformanceRequirements(t *testing.T) {
-	mockMicro := &mockMicroEvaluator{
-		result: &microstructure.EvaluationResult{
-			Symbol:    "PERF-TEST",
-			SpreadBps: 25.0,
-			DepthUSD:  200000.0,
-			VADR:      2.0,
-		},
-	}
-
-	engine := NewPreMovementEngine(mockMicro, nil)
-
-	// Create multiple candidate inputs for performance test
-	inputs := make([]*CandidateInput, 10)
-	for i := 0; i < 10; i++ {
-		inputs[i] = &CandidateInput{
-			Symbol:    fmt.Sprintf("TEST-%d", i),
-			Timestamp: time.Now(),
-			PreMovementData: &PreMovementData{
-				Symbol:             fmt.Sprintf("TEST-%d", i),
-				Timestamp:          time.Now(),
-				FundingZScore:      2.0 + float64(i)*0.1,
-				OIResidual:         500000,
-				ETFFlowTint:        0.5,
-				ReserveChange7d:    -5.0,
-				WhaleComposite:     0.6,
-				MicroDynamics:      0.4,
-				SmartMoneyFlow:     0.5,
-				CVDResidual:        0.3,
-				CatalystHeat:       0.4,
-				VolCompressionRank: 0.5,
-				OldestFeedHours:    1.0,
-			},
-			ConfirmationData: &ConfirmationData{
-				Symbol:         fmt.Sprintf("TEST-%d", i),
-				Timestamp:      time.Now(),
-				FundingZScore:  2.0 + float64(i)*0.1,
-				WhaleComposite: 0.6 + float64(i)*0.01,
-				SupplyProxyScore: 0.5,
-				VolumeRatio24h:  2.0,
-				CurrentRegime:   "normal",
-			},
-			CVDDataPoints: generateSyntheticCVDData(50, 0.6),
-		}
-	}
-
-	start := time.Now()
-	result, err := engine.ListCandidates(context.Background(), inputs, 10)
-	duration := time.Since(start)
-
-	require.NoError(t, err)
-	assert.NotNil(t, result)
-	assert.Less(t, duration.Milliseconds(), int64(3000), "Should complete 10 candidates in <3s")
-	assert.Greater(t, result.ProcessTimeMs, int64(0), "Should report processing time")
-	assert.LessOrEqual(t, len(result.Candidates), 10, "Should respect candidate limit")
-}
-
-// Import the fmt package for string formatting in tests
-import "fmt"
\ No newline at end of file
+package premove
+
+import (
+	"context"
+	"fmt"
+	"testing"
+	"time"
+
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+
+	"cryptorun/internal/microstructure"
+)
+
+func TestPreMovementEngine_ListCandidates_FullPipeline(t *testing.T) {
+	// Create mock microstructure evaluator
+	mockMicro := &mockMicroEvaluator{
+		result: &microstructure.EvaluationResult{
+			Symbol:    "BTC-USD",
+			SpreadBps: 20.0,
+			DepthUSD:  300000.0,
+			VADR:      2.2,
+		},
+	}
+
+	engine := NewPreMovementEngine(mockMicro, nil)
+
+	// Create strong candidate inputs
+	inputs := []*CandidateInput{
+		{
+			Symbol:    "BTC-USD",
+			Timestamp: time.Now(),
+			PreMovementData: &PreMovementData{
+				Symbol:    "BTC-USD",
+				Timestamp: time.Now(),
+
+				// Strong structural signals (targeting ~35 points)
+				FundingZScore:   3.2,   // Strong funding divergence
+				OIResidual:      1.2e6, // $1.2M OI residual
+				ETFFlowTint:     0.8,   // 80% bullish flows
+				ReserveChange7d: -12.0, // -12% exchange reserves
+				WhaleComposite:  0.85,  // 85% whale activity
+				MicroDynamics:   0.7,   // 70% L1/L2 stress
+
+				// Strong behavioral signals (targeting ~30 points)
+				SmartMoneyFlow: 0.8, // 80% institutional flow
+				CVDResidual:    0.6, // 60% CVD residual
+
+				// Strong catalyst & compression (targeting ~20 points)
+				CatalystHeat:       0.8, // 80% catalyst significance
+				VolCompressionRank: 0.9, // 90th percentile compression
+
+				OldestFeedHours: 0.5, // Fresh data
+			},
+			ConfirmationData: &ConfirmationData{
+				Symbol:    "BTC-USD",
+				Timestamp: time.Now(),
+
+				// Strong 2-of-3 confirmations
+				FundingZScore:    3.2,  // Strong funding
+				WhaleComposite:   0.85, // Strong whale activity
+				SupplyProxyScore: 0.7,  // Strong supply squeeze
+
+				// Strong supply squeeze components
+				ReserveChange7d:     -12.0,
+				LargeWithdrawals24h: 80e6,
+				StakingInflow24h:    15e6,
+				DerivativesOIChange: 20.0,
+
+				// Volume confirmation in supportive regime
+				VolumeRatio24h: 3.5,
+				CurrentRegime:  "risk_off",
+
+				SpreadBps: 20.0,
+				DepthUSD:  300000.0,
+				VADR:      2.2,
+			},
+			CVDDataPoints: generateSyntheticCVDData(80, 0.8), // Good regression data
+		},
+		{
+			Symbol:    "ETH-USD",
+			Timestamp: time.Now(),
+			PreMovementData: &PreMovementData{
+				Symbol:    "ETH-USD",
+				Timestamp: time.Now(),
+
+				// Moderate signals (targeting ~60 points)
+				FundingZScore:      2.1,
+				OIResidual:         600000,
+				ETFFlowTint:        0.5,
+				ReserveChange7d:    -6.0,
+				WhaleComposite:     0.6,
+				MicroDynamics:      0.4,
+				SmartMoneyFlow:     0.5,
+				CVDResidual:        0.3,
+				CatalystHeat:       0.4,
+				VolCompressionRank: 0.6,
+				OldestFeedHours:    1.2,
+			},
+			ConfirmationData: &ConfirmationData{
+				Symbol:    "ETH-USD",
+				Timestamp: time.Now(),
+
+				// Moderate confirmations (only 2-of-3)
+				FundingZScore:       2.1,  // Pass
+				WhaleComposite:      0.75, // Pass
+				SupplyProxyScore:    0.4,  // Fail
+				ReserveChange7d:     -3.0,
+				LargeWithdrawals24h: 30e6,
+				StakingInflow24h:    8e6,
+				DerivativesOIChange: 10.0,
+				VolumeRatio24h:      1.8,
+				CurrentRegime:       "normal",
+			},
+			CVDDataPoints: generateSyntheticCVDData(60, 0.6), // Moderate regression
+		},
+		{
+			Symbol:    "SOL-USD",
+			Timestamp: time.Now(),
+			PreMovementData: &PreMovementData{
+				Symbol:    "SOL-USD",
+				Timestamp: time.Now(),
+
+				// Weak signals (targeting ~30 points)
+				FundingZScore:      1.2,
+				OIResidual:         200000,
+				ETFFlowTint:        0.2,
+				ReserveChange7d:    -2.0,
+				WhaleComposite:     0.3,
+				MicroDynamics:      0.2,
+				SmartMoneyFlow:     0.2,
+				CVDResidual:        0.1,
+				CatalystHeat:       0.2,
+				VolCompressionRank: 0.3,
+				OldestFeedHours:    0.8,
+			},
+			ConfirmationData: &ConfirmationData{
+				Symbol:    "SOL-USD",
+				Timestamp: time.Now(),
+
+				// Weak confirmations (only 1-of-3)
+				FundingZScore:       1.2, // Fail
+				WhaleComposite:      0.5, // Fail
+				SupplyProxyScore:    0.3, // Fail
+				ReserveChange7d:     -2.0,
+				LargeWithdrawals24h: 20e6,
+				StakingInflow24h:    5e6,
+				DerivativesOIChange: 8.0,
+				VolumeRatio24h:      1.2,
+				CurrentRegime:       "normal",
+			},
+			CVDDataPoints: generateNoisyCVDData(40), // Should trigger fallback
+		},
+	}
+
+	result, err := engine.ListCandidates(context.Background(), inputs, 10)
+	require.NoError(t, err)
+	assert.NotNil(t, result)
+
+	// Should process all candidates
+	assert.Equal(t, 3, result.TotalCandidates)
+
+	// BTC and ETH should pass (strong and moderate), SOL should be filtered out
+	assert.Equal(t, 2, result.ValidCandidates, "Should have 2 valid candidates")
+	assert.Equal(t, 1, result.StrongCandidates, "Should have 1 strong candidate (BTC)")
+
+	// Check ranking - BTC should be first (strongest)
+	require.Len(t, result.Candidates, 2)
+	assert.Equal(t, "BTC-USD", result.Candidates[0].Symbol, "BTC should rank first")
+	assert.Equal(t, "STRONG", result.Candidates[0].OverallStatus)
+	assert.Equal(t, 1, result.Candidates[0].Rank)
+
+	assert.Equal(t, "ETH-USD", result.Candidates[1].Symbol, "ETH should rank second")
+	assert.Equal(t, "MODERATE", result.Candidates[1].OverallStatus)
+	assert.Equal(t, 2, result.Candidates[1].Rank)
+
+	// Check data freshness assessment
+	assert.NotNil(t, result.DataFreshness)
+	assert.Greater(t, result.ProcessTimeMs, int64(0), "Should report processing time")
+}
+
+func TestPreMovementEngine_AnalyzeCandidate_ScoreBreakdown(t *testing.T) {
+	mockMicro := &mockMicroEvaluator{
+		result: &microstructure.EvaluationResult{
+			Symbol:    "TEST-USD",
+			SpreadBps: 30.0,
+			DepthUSD:  150000.0,
+			VADR:      1.9,
+		},
+	}
+
+	engine := NewPreMovementEngine(mockMicro, nil)
+
+	input := &CandidateInput{
+		Symbol:    "TEST-USD",
+		Timestamp: time.Now(),
+		PreMovementData: &PreMovementData{
+			Symbol:             "TEST-USD",
+			Timestamp:          time.Now(),
+			FundingZScore:      2.5,
+			OIResidual:         800000,
+			ETFFlowTint:        0.6,
+			ReserveChange7d:    -8.0,
+			WhaleComposite:     0.7,
+			MicroDynamics:      0.5,
+			SmartMoneyFlow:     0.6,
+			CVDResidual:        0.4,
+			CatalystHeat:       0.6,
+			VolCompressionRank: 0.7,
+			OldestFeedHours:    1.0,
+		},
+		ConfirmationData: &ConfirmationData{
+			Symbol:              "TEST-USD",
+			Timestamp:           time.Now(),
+			FundingZScore:       2.5,
+			WhaleComposite:      0.7,
+			SupplyProxyScore:    0.4, // Will calculate from components
+			ReserveChange7d:     -8.0,
+			LargeWithdrawals24h: 60e6,
+			StakingInflow24h:    12e6,
+			DerivativesOIChange: 18.0,
+			VolumeRatio24h:      2.0,
+			CurrentRegime:       "normal",
+		},
+		CVDDataPoints: generateSyntheticCVDData(70, 0.7),
+	}
+
+	candidate, err := engine.analyzeCandidate(context.Background(), input)
+	require.NoError(t, err)
+	assert.NotNil(t, candidate)
+
+	// Should have complete analysis
+	assert.NotNil(t, candidate.ScoreBreakdown, "Should have score breakdown")
+	assert.NotNil(t, candidate.GatesResult, "Should have gates result")
+	assert.NotNil(t, candidate.CVDResult, "Should have CVD result")
+
+	// Score should be reasonable for moderate signals
+	assert.Greater(t, candidate.TotalScore, 50.0, "Moderate signals should yield >50 score")
+	assert.Less(t, candidate.TotalScore, 85.0, "Moderate signals should yield <85 score")
+
+	// Should have reasons for recommendation
+	assert.Greater(t, len(candidate.Reasons), 0, "Should have recommendation reasons")
+
+	// Should determine appropriate overall status
+	assert.Contains(t, []string{"MODERATE", "WEAK"}, candidate.OverallStatus)
+
+	assert.Greater(t, candidate.ProcessTimeMs, int64(0), "Should report processing time")
+}
+
+func TestPreMovementEngine_DetermineOverallStatus(t *testing.T) {
+	engine := NewPreMovementEngine(nil, nil)
+
+	// Test STRONG status (high score + confirmation + significant CVD)
+	strongCandidate := &PreMovementCandidate{
+		TotalScore:  88.0,
+		GatesStatus: "CONFIRMED",
+		CVDResult:   &CVDResidualResult{IsSignificant: true},
+	}
+	assert.Equal(t, "STRONG", engine.determineOverallStatus(strongCandidate))
+
+	// Test MODERATE status (good score + confirmation)
+	moderateCandidate := &PreMovementCandidate{
+		TotalScore:  78.0,
+		GatesStatus: "CONFIRMED",
+		CVDResult:   &CVDResidualResult{IsSignificant: false},
+	}
+	assert.Equal(t, "MODERATE", engine.determineOverallStatus(moderateCandidate))
+
+	// Test MODERATE status (very high score alone)
+	highScoreCandidate := &PreMovementCandidate{
+		TotalScore:  92.0,
+		GatesStatus: "BLOCKED",
+		CVDResult:   nil,
+	}
+	assert.Equal(t, "MODERATE", engine.determineOverallStatus(highScoreCandidate))
+
+	// Test BLOCKED status
+	blockedCandidate := &PreMovementCandidate{
+		TotalScore:  85.0,
+		GatesStatus: "BLOCKED",
+		CVDResult:   &CVDResidualResult{IsSignificant: true},
+	}
+	assert.Equal(t, "BLOCKED", engine.determineOverallStatus(blockedCandidate))
+
+	// Test WEAK status
+	weakCandidate := &PreMovementCandidate{
+		TotalScore:  65.0,
+		GatesStatus: "CONFIRMED",
+		CVDResult:   &CVDResidualResult{IsSignificant: false},
+	}
+	assert.Equal(t, "WEAK", engine.determineOverallStatus(weakCandidate))
+}
+
+func TestPreMovementEngine_RankCandidates(t *testing.T) {
+	engine := NewPreMovementEngine(nil, nil)
+
+	candidates := []*PreMovementCandidate{
+		{
+			Symbol:        "WEAK",
+			TotalScore:    60.0,
+			OverallStatus: "WEAK",
+			GatesResult:   &ConfirmationResult{PrecedenceScore: 2.0},
+			CVDResult:     &CVDResidualResult{SignificanceScore: 0.3},
+		},
+		{
+			Symbol:        "STRONG",
+			TotalScore:    90.0,
+			OverallStatus: "STRONG",
+			GatesResult:   &ConfirmationResult{PrecedenceScore: 6.0},
+			CVDResult:     &CVDResidualResult{SignificanceScore: 0.8},
+		},
+		{
+			Symbol:        "MODERATE",
+			TotalScore:    75.0,
+			OverallStatus: "MODERATE",
+			GatesResult:   &ConfirmationResult{PrecedenceScore: 5.0},
+			CVDResult:     &CVDResidualResult{SignificanceScore: 0.5},
+		},
+		{
+			Symbol:        "BLOCKED",
+			TotalScore:    85.0,
+			OverallStatus: "BLOCKED",
+			GatesResult:   &ConfirmationResult{PrecedenceScore: 3.0},
+			CVDResult:     nil,
+		},
+	}
+
+	engine.rankCandidates(candidates)
+
+	// Check ranking order
+	assert.Equal(t, "STRONG", candidates[0].Symbol, "STRONG should rank first")
+	assert.Equal(t, 1, candidates[0].Rank)
+
+	assert.Equal(t, "MODERATE", candidates[1].Symbol, "MODERATE should rank second")
+	assert.Equal(t, 2, candidates[1].Rank)
+
+	assert.Equal(t, "WEAK", candidates[2].Symbol, "WEAK should rank third")
+	assert.Equal(t, 3, candidates[2].Rank)
+
+	assert.Equal(t, "BLOCKED", candidates[3].Symbol, "BLOCKED should rank last")
+	assert.Equal(t, 4, candidates[3].Rank)
+}
+
+func TestPreMovementEngine_AssessDataFreshness(t *testing.T) {
+	engine := NewPreMovementEngine(nil, nil)
+
+	candidates := []*PreMovementCandidate{
+		{
+			Symbol: "FRESH",
+			ScoreBreakdown: &ScoreResult{
+				DataFreshness: &FreshnessInfo{OldestFeedHours: 0.2}, // 12 minutes
+			},
+		},
+		{
+			Symbol: "MODERATE",
+			ScoreBreakdown: &ScoreResult{
+				DataFreshness: &FreshnessInfo{OldestFeedHours: 0.8}, // 48 minutes
+			},
+		},
+		{
+			Symbol: "STALE",
+			ScoreBreakdown: &ScoreResult{
+				DataFreshness: &FreshnessInfo{OldestFeedHours: 1.2}, // 72 minutes
+			},
+		},
+	}
+
+	report := engine.assessDataFreshness(candidates)
+	assert.NotNil(t, report)
+
+	// Should calculate average age
+	expectedAvgSeconds := int64((0.2 + 0.8 + 1.2) / 3.0 * 3600) // Average in seconds
+	assert.Equal(t, expectedAvgSeconds, report.AverageAgeSeconds)
+
+	// Should find oldest data
+	assert.Equal(t, int64(1.2*3600), report.OldestDataSeconds)
+
+	// Should count stale candidates (>10 min warning threshold)
+	assert.Equal(t, 2, report.StaleCandidatesCount, "Should count 2 stale candidates")
+	assert.InDelta(t, 66.7, report.StaleCandidatesPct, 1.0, "Should calculate stale percentage")
+
+	// Should assign reasonable freshness grade
+	assert.Contains(t, []string{"A", "B", "C", "D", "F"}, report.FreshnessGrade)
+}
+
+func TestAnalysisResult_GetAnalysisSummary(t *testing.T) {
+	result := &AnalysisResult{
+		TotalCandidates:  5,
+		ValidCandidates:  3,
+		StrongCandidates: 1,
+		ProcessTimeMs:    234,
+		DataFreshness:    &DataFreshnessReport{FreshnessGrade: "B"},
+	}
+
+	summary := result.GetAnalysisSummary()
+	assert.Contains(t, summary, "Pre-Movement v3.3 Analysis")
+	assert.Contains(t, summary, "5 candidates")
+	assert.Contains(t, summary, "3 valid")
+	assert.Contains(t, summary, "1 strong")
+	assert.Contains(t, summary, "freshness: B")
+	assert.Contains(t, summary, "234ms")
+}
+
+func TestAnalysisResult_GetTopCandidatesSummary(t *testing.T) {
+	result := &AnalysisResult{
+		Candidates: []*PreMovementCandidate{
+			{
+				Rank:          1,
+				Symbol:        "BTC-USD",
+				OverallStatus: "STRONG",
+				TotalScore:    88.5,
+				GatesStatus:   "CONFIRMED",
+			},
+			{
+				Rank:          2,
+				Symbol:        "ETH-USD",
+				OverallStatus: "MODERATE",
+				TotalScore:    76.2,
+				GatesStatus:   "CONFIRMED",
+			},
+			{
+				Rank:          3,
+				Symbol:        "SOL-USD",
+				OverallStatus: "WEAK",
+				TotalScore:    58.1,
+				GatesStatus:   "BLOCKED",
+			},
+		},
+	}
+
+	summary := result.GetTopCandidatesSummary(2)
+	assert.Contains(t, summary, "Top 2 Pre-Movement Candidates")
+	assert.Contains(t, summary, "1. üî• BTC-USD STRONG | Score: 88.5 | Gates: ‚úÖ")
+	assert.Contains(t, summary, "2. üìà ETH-USD MODERATE | Score: 76.2 | Gates: ‚úÖ")
+	assert.NotContains(t, summary, "SOL-USD") // Should not include 3rd candidate
+}
+
+func TestAnalysisResult_GetCandidateDetails(t *testing.T) {
+	result := &AnalysisResult{
+		Candidates: []*PreMovementCandidate{
+			{
+				Rank:          1,
+				Symbol:        "BTC-USD",
+				OverallStatus: "STRONG",
+				TotalScore:    88.5,
+				GatesStatus:   "CONFIRMED",
+				ProcessTimeMs: 145,
+				Reasons:       []string{"High Pre-Movement score (88.5)", "Strong confirmations"},
+				ScoreBreakdown: &ScoreResult{
+					Symbol:     "BTC-USD",
+					TotalScore: 88.5,
+					ComponentScores: map[string]float64{
+						"derivatives": 12.5,
+						"smart_money": 18.2,
+					},
+				},
+			},
+		},
+	}
+
+	details := result.GetCandidateDetails("BTC-USD")
+	assert.Contains(t, details, "BTC-USD (Rank #1)")
+	assert.Contains(t, details, "Status: STRONG")
+	assert.Contains(t, details, "Score: 88.5")
+	assert.Contains(t, details, "Gates: CONFIRMED")
+	assert.Contains(t, details, "Time: 145ms")
+	assert.Contains(t, details, "High Pre-Movement score (88.5)")
+
+	// Test non-existent candidate
+	notFound := result.GetCandidateDetails("UNKNOWN")
+	assert.Contains(t, notFound, "Candidate UNKNOWN not found")
+}
+
+func TestDefaultEngineConfig(t *testing.T) {
+	config := DefaultEngineConfig()
+	require.NotNil(t, config)
+
+	// Should have all sub-configs
+	assert.NotNil(t, config.ScoreConfig)
+	assert.NotNil(t, config.GateConfig)
+	assert.NotNil(t, config.CVDConfig)
+
+	// Check API limits
+	assert.Equal(t, 50, config.MaxCandidates)
+	assert.Equal(t, int64(2000), config.MaxProcessTimeMs)
+	assert.True(t, config.RequireScore)
+	assert.True(t, config.RequireGates)
+
+	// Check data freshness requirements
+	assert.Equal(t, int64(1800), config.MaxDataStaleness) // 30 minutes
+	assert.Equal(t, int64(600), config.StaleDataWarning)  // 10 minutes
+}
+
+func TestPreMovementEngine_PerformanceRequirements(t *testing.T) {
+	mockMicro := &mockMicroEvaluator{
+		result: &microstructure.EvaluationResult{
+			Symbol:    "PERF-TEST",
+			SpreadBps: 25.0,
+			DepthUSD:  200000.0,
+			VADR:      2.0,
+		},
+	}
+
+	engine := NewPreMovementEngine(mockMicro, nil)
+
+	// Create multiple candidate inputs for performance test
+	inputs := make([]*CandidateInput, 10)
+	for i := 0; i < 10; i++ {
+		inputs[i] = &CandidateInput{
+			Symbol:    fmt.Sprintf("TEST-%d", i),
+			Timestamp: time.Now(),
+			PreMovementData: &PreMovementData{
+				Symbol:             fmt.Sprintf("TEST-%d", i),
+				Timestamp:          time.Now(),
+				FundingZScore:      2.0 + float64(i)*0.1,
+				OIResidual:         500000,
+				ETFFlowTint:        0.5,
+				ReserveChange7d:    -5.0,
+				WhaleComposite:     0.6,
+				MicroDynamics:      0.4,
+				SmartMoneyFlow:     0.5,
+				CVDResidual:        0.3,
+				CatalystHeat:       0.4,
+				VolCompressionRank: 0.5,
+				OldestFeedHours:    1.0,
+			},
+			ConfirmationData: &ConfirmationData{
+				Symbol:           fmt.Sprintf("TEST-%d", i),
+				Timestamp:        time.Now(),
+				FundingZScore:    2.0 + float64(i)*0.1,
+				WhaleComposite:   0.6 + float64(i)*0.01,
+				SupplyProxyScore: 0.5,
+				VolumeRatio24h:   2.0,
+				CurrentRegime:    "normal",
+			},
+			CVDDataPoints: generateSyntheticCVDData(50, 0.6),
+		}
+	}
+
+	start := time.Now()
+	result, err := engine.ListCandidates(context.Background(), inputs, 10)
+	duration := time.Since(start)
+
+	require.NoError(t, err)
+	assert.NotNil(t, result)
+	assert.Less(t, duration.Milliseconds(), int64(3000), "Should complete 10 candidates in <3s")
+	assert.Greater(t, result.ProcessTimeMs, int64(0), "Should report processing time")
+	assert.LessOrEqual(t, len(result.Candidates), 10, "Should respect candidate limit")
+}
diff --git a/internal/premove/cvd_resid.go b/internal/premove/cvd_resid.go
index 56d5119..b6bc0b9 100644
--- a/internal/premove/cvd_resid.go
+++ b/internal/premove/cvd_resid.go
@@ -1,540 +1,540 @@
-package premove
-
-import (
-	"context"
-	"fmt"
-	"math"
-	"sort"
-	"time"
-)
-
-// CVDResidualAnalyzer implements robust CVD residualization with R¬≤ fallback
-type CVDResidualAnalyzer struct {
-	config *CVDConfig
-}
-
-// NewCVDResidualAnalyzer creates a CVD residual analyzer with robust regression
-func NewCVDResidualAnalyzer(config *CVDConfig) *CVDResidualAnalyzer {
-	if config == nil {
-		config = DefaultCVDConfig()
-	}
-	return &CVDResidualAnalyzer{config: config}
-}
-
-// CVDConfig contains parameters for CVD residual analysis
-type CVDConfig struct {
-	// Regression parameters
-	MinDataPoints       int     `yaml:"min_data_points"`       // 50 minimum data points
-	WinsorizePctLower   float64 `yaml:"winsorize_pct_lower"`   // 5% lower winsorization
-	WinsorizePctUpper   float64 `yaml:"winsorize_pct_upper"`   // 95% upper winsorization
-	MinRSquared         float64 `yaml:"min_r_squared"`         // 0.30 minimum R¬≤ for regression
-	DailyRefitEnabled   bool    `yaml:"daily_refit_enabled"`   // true - refit model daily
-	
-	// Fallback parameters
-	FallbackMethod      string  `yaml:"fallback_method"`       // "percentile" or "zscore"
-	FallbackLookback    int     `yaml:"fallback_lookback"`     // 20 periods for percentile rank
-	FallbackThreshold   float64 `yaml:"fallback_threshold"`    // 80% percentile threshold
-	
-	// Residual analysis
-	ResidualMinStdDev   float64 `yaml:"residual_min_std_dev"`  // 2.0 std dev significance
-	ResidualMaxAge      int64   `yaml:"residual_max_age"`      // 3600 seconds (1 hour) max age
-	
-	// Performance limits
-	MaxComputeTimeMs    int64   `yaml:"max_compute_time_ms"`   // 200ms compute timeout
-}
-
-// DefaultCVDConfig returns production CVD configuration
-func DefaultCVDConfig() *CVDConfig {
-	return &CVDConfig{
-		// Regression parameters
-		MinDataPoints:     50,
-		WinsorizePctLower: 5.0,
-		WinsorizePctUpper: 95.0,
-		MinRSquared:       0.30,
-		DailyRefitEnabled: true,
-		
-		// Fallback parameters  
-		FallbackMethod:    "percentile",
-		FallbackLookback:  20,
-		FallbackThreshold: 80.0,
-		
-		// Residual analysis
-		ResidualMinStdDev: 2.0,
-		ResidualMaxAge:    3600, // 1 hour
-		
-		// Performance
-		MaxComputeTimeMs: 200,
-	}
-}
-
-// CVDDataPoint represents a single CVD observation
-type CVDDataPoint struct {
-	Timestamp   time.Time `json:"timestamp"`
-	Price       float64   `json:"price"`        // Price at timestamp
-	CVD         float64   `json:"cvd"`          // Cumulative volume delta
-	Volume      float64   `json:"volume"`       // Volume at timestamp  
-	PriceChange float64   `json:"price_change"` // Price change since previous
-}
-
-// CVDRegressionModel contains fitted regression coefficients and statistics
-type CVDRegressionModel struct {
-	Symbol           string    `json:"symbol"`
-	FitTimestamp     time.Time `json:"fit_timestamp"`
-	Intercept        float64   `json:"intercept"`        // Œ≤‚ÇÄ
-	PriceCoefficient float64   `json:"price_coefficient"` // Œ≤‚ÇÅ - price change coefficient
-	RSquared         float64   `json:"r_squared"`        // Model R¬≤
-	StandardError    float64   `json:"standard_error"`   // Residual standard error
-	DataPoints       int       `json:"data_points"`      // Number of points used in fit
-	IsValid          bool      `json:"is_valid"`         // Whether model meets quality thresholds
-	LastRefit        time.Time `json:"last_refit"`       // When model was last refit
-}
-
-// CVDResidualResult contains residual analysis output
-type CVDResidualResult struct {
-	Symbol             string              `json:"symbol"`
-	Timestamp          time.Time           `json:"timestamp"`
-	RawResidual        float64             `json:"raw_residual"`         // Raw CVD residual
-	NormalizedResidual float64             `json:"normalized_residual"`  // Z-score normalized residual
-	PercentileRank     float64             `json:"percentile_rank"`      // 0-100 percentile rank
-	SignificanceScore  float64             `json:"significance_score"`   // 0-1 significance (for scoring)
-	Method             string              `json:"method"`               // "regression" or fallback method
-	Model              *CVDRegressionModel `json:"model"`                // Regression model used (if applicable)
-	FallbackReason     string              `json:"fallback_reason"`      // Why fallback was used
-	ComputeTimeMs      int64               `json:"compute_time_ms"`      // Analysis compute time
-	DataQuality        *CVDDataQuality     `json:"data_quality"`         // Data quality assessment
-	IsSignificant      bool                `json:"is_significant"`       // Whether residual is significant
-	Warnings           []string            `json:"warnings"`             // Analysis warnings
-}
-
-// CVDDataQuality contains data quality metrics
-type CVDDataQuality struct {
-	PointsAvailable    int     `json:"points_available"`    // Total data points available
-	PointsUsed         int     `json:"points_used"`         // Points used after winsorization
-	WinsorizedPoints   int     `json:"winsorized_points"`   // Points removed by winsorization
-	DataSpanHours      float64 `json:"data_span_hours"`     // Time span of data
-	MissingDataPct     float64 `json:"missing_data_pct"`    // % missing data points
-	OutliersPct        float64 `json:"outliers_pct"`        // % outliers detected
-}
-
-// AnalyzeCVDResidual performs comprehensive CVD residual analysis with fallbacks
-func (cvda *CVDResidualAnalyzer) AnalyzeCVDResidual(ctx context.Context, symbol string, dataPoints []*CVDDataPoint) (*CVDResidualResult, error) {
-	startTime := time.Now()
-
-	result := &CVDResidualResult{
-		Symbol:    symbol,
-		Timestamp: time.Now(),
-		Method:    "regression", // Default to regression, may fallback
-		Warnings:  []string{},
-		DataQuality: &CVDDataQuality{
-			PointsAvailable: len(dataPoints),
-		},
-	}
-
-	// Data quality assessment
-	if len(dataPoints) == 0 {
-		return nil, fmt.Errorf("no CVD data points provided for %s", symbol)
-	}
-
-	// Calculate data span and quality metrics
-	cvda.assessDataQuality(dataPoints, result.DataQuality)
-
-	// Check minimum data requirements
-	if len(dataPoints) < cvda.config.MinDataPoints {
-		result.FallbackReason = fmt.Sprintf("Insufficient data points (%d < %d)", len(dataPoints), cvda.config.MinDataPoints)
-		return cvda.performFallbackAnalysis(dataPoints, result), nil
-	}
-
-	// Winsorize data to remove extreme outliers
-	winsorizedData, outlierCount := cvda.winsorizeData(dataPoints)
-	result.DataQuality.PointsUsed = len(winsorizedData)
-	result.DataQuality.WinsorizedPoints = outlierCount
-	result.DataQuality.OutliersPct = float64(outlierCount) / float64(len(dataPoints)) * 100.0
-
-	if len(winsorizedData) < cvda.config.MinDataPoints {
-		result.FallbackReason = fmt.Sprintf("Too many outliers removed (%d < %d after winsorization)", len(winsorizedData), cvda.config.MinDataPoints)
-		return cvda.performFallbackAnalysis(dataPoints, result), nil
-	}
-
-	// Fit regression model: CVD = Œ≤‚ÇÄ + Œ≤‚ÇÅ * PriceChange + Œµ
-	model, err := cvda.fitRegressionModel(symbol, winsorizedData)
-	if err != nil {
-		result.FallbackReason = fmt.Sprintf("Regression fitting failed: %v", err)
-		return cvda.performFallbackAnalysis(dataPoints, result), nil
-	}
-
-	result.Model = model
-
-	// Check model quality (R¬≤ threshold)
-	if model.RSquared < cvda.config.MinRSquared {
-		result.FallbackReason = fmt.Sprintf("Low R¬≤ (%.3f < %.3f)", model.RSquared, cvda.config.MinRSquared)
-		return cvda.performFallbackAnalysis(dataPoints, result), nil
-	}
-
-	// Calculate residual for most recent data point
-	latestPoint := dataPoints[len(dataPoints)-1]
-	predicted := model.Intercept + model.PriceCoefficient*latestPoint.PriceChange
-	result.RawResidual = latestPoint.CVD - predicted
-
-	// Normalize residual using model standard error
-	if model.StandardError > 0 {
-		result.NormalizedResidual = result.RawResidual / model.StandardError
-	}
-
-	// Calculate percentile rank using historical residuals
-	result.PercentileRank = cvda.calculatePercentileRank(result.RawResidual, winsorizedData, model)
-
-	// Calculate significance score (0-1) for use in Pre-Movement scoring
-	result.SignificanceScore = cvda.calculateSignificanceScore(result.NormalizedResidual, result.PercentileRank)
-
-	// Determine significance based on threshold
-	result.IsSignificant = math.Abs(result.NormalizedResidual) >= cvda.config.ResidualMinStdDev
-
-	result.ComputeTimeMs = time.Since(startTime).Milliseconds()
-
-	// Performance warning
-	if result.ComputeTimeMs > cvda.config.MaxComputeTimeMs {
-		result.Warnings = append(result.Warnings, 
-			fmt.Sprintf("CVD analysis took %dms (>%dms threshold)", result.ComputeTimeMs, cvda.config.MaxComputeTimeMs))
-	}
-
-	return result, nil
-}
-
-// assessDataQuality evaluates input data quality
-func (cvda *CVDResidualAnalyzer) assessDataQuality(dataPoints []*CVDDataPoint, quality *CVDDataQuality) {
-	if len(dataPoints) < 2 {
-		return
-	}
-
-	// Calculate data span
-	firstTime := dataPoints[0].Timestamp
-	lastTime := dataPoints[len(dataPoints)-1].Timestamp
-	quality.DataSpanHours = lastTime.Sub(firstTime).Hours()
-
-	// Detect missing data (gaps > 2x median interval)
-	intervals := make([]float64, 0, len(dataPoints)-1)
-	for i := 1; i < len(dataPoints); i++ {
-		interval := dataPoints[i].Timestamp.Sub(dataPoints[i-1].Timestamp).Minutes()
-		intervals = append(intervals, interval)
-	}
-
-	sort.Float64s(intervals)
-	medianInterval := intervals[len(intervals)/2]
-	
-	missingCount := 0
-	for _, interval := range intervals {
-		if interval > 2*medianInterval {
-			missingCount++
-		}
-	}
-	quality.MissingDataPct = float64(missingCount) / float64(len(intervals)) * 100.0
-}
-
-// winsorizeData removes extreme outliers using percentile-based winsorization
-func (cvda *CVDResidualAnalyzer) winsorizeData(dataPoints []*CVDDataPoint) ([]*CVDDataPoint, int) {
-	if len(dataPoints) <= 10 {
-		return dataPoints, 0 // Don't winsorize small datasets
-	}
-
-	// Extract CVD and price change values for percentile calculation
-	cvdValues := make([]float64, len(dataPoints))
-	priceChanges := make([]float64, len(dataPoints))
-	
-	for i, dp := range dataPoints {
-		cvdValues[i] = dp.CVD
-		priceChanges[i] = dp.PriceChange
-	}
-
-	// Calculate winsorization bounds
-	cvdLower, cvdUpper := calculatePercentileBounds(cvdValues, cvda.config.WinsorizePctLower, cvda.config.WinsorizePctUpper)
-	priceLower, priceUpper := calculatePercentileBounds(priceChanges, cvda.config.WinsorizePctLower, cvda.config.WinsorizePctUpper)
-
-	// Filter out extreme outliers
-	filtered := make([]*CVDDataPoint, 0, len(dataPoints))
-	outlierCount := 0
-
-	for _, dp := range dataPoints {
-		if dp.CVD >= cvdLower && dp.CVD <= cvdUpper && 
-		   dp.PriceChange >= priceLower && dp.PriceChange <= priceUpper {
-			filtered = append(filtered, dp)
-		} else {
-			outlierCount++
-		}
-	}
-
-	return filtered, outlierCount
-}
-
-// fitRegressionModel fits CVD = Œ≤‚ÇÄ + Œ≤‚ÇÅ * PriceChange + Œµ using least squares
-func (cvda *CVDResidualAnalyzer) fitRegressionModel(symbol string, dataPoints []*CVDDataPoint) (*CVDRegressionModel, error) {
-	n := len(dataPoints)
-	if n < 3 {
-		return nil, fmt.Errorf("insufficient data for regression (need ‚â•3, got %d)", n)
-	}
-
-	// Extract X (price changes) and Y (CVD) vectors
-	var sumX, sumY, sumXY, sumXX, sumYY float64
-
-	for _, dp := range dataPoints {
-		x := dp.PriceChange
-		y := dp.CVD
-		
-		sumX += x
-		sumY += y
-		sumXY += x * y
-		sumXX += x * x
-		sumYY += y * y
-	}
-
-	// Calculate means
-	meanX := sumX / float64(n)
-	meanY := sumY / float64(n)
-
-	// Calculate regression coefficients
-	// Œ≤‚ÇÅ = (n‚àëXY - ‚àëX‚àëY) / (n‚àëX¬≤ - (‚àëX)¬≤)
-	// Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ
-	
-	numerator := float64(n)*sumXY - sumX*sumY
-	denominator := float64(n)*sumXX - sumX*sumX
-
-	if math.Abs(denominator) < 1e-10 {
-		return nil, fmt.Errorf("singular matrix - no price variation")
-	}
-
-	beta1 := numerator / denominator
-	beta0 := meanY - beta1*meanX
-
-	// Calculate R¬≤ and standard error
-	var ssRes, ssTot float64
-	for _, dp := range dataPoints {
-		predicted := beta0 + beta1*dp.PriceChange
-		residual := dp.CVD - predicted
-		
-		ssRes += residual * residual
-		ssTot += (dp.CVD - meanY) * (dp.CVD - meanY)
-	}
-
-	var rSquared float64
-	if ssTot > 0 {
-		rSquared = 1.0 - ssRes/ssTot
-	}
-
-	standardError := math.Sqrt(ssRes / float64(n-2))
-
-	model := &CVDRegressionModel{
-		Symbol:           symbol,
-		FitTimestamp:     time.Now(),
-		Intercept:        beta0,
-		PriceCoefficient: beta1,
-		RSquared:         rSquared,
-		StandardError:    standardError,
-		DataPoints:       n,
-		IsValid:          rSquared >= cvda.config.MinRSquared,
-		LastRefit:        time.Now(),
-	}
-
-	return model, nil
-}
-
-// performFallbackAnalysis uses percentile ranking when regression fails
-func (cvda *CVDResidualAnalyzer) performFallbackAnalysis(dataPoints []*CVDDataPoint, result *CVDResidualResult) *CVDResidualResult {
-	result.Method = cvda.config.FallbackMethod
-
-	if len(dataPoints) == 0 {
-		result.Warnings = append(result.Warnings, "No data available for fallback analysis")
-		return result
-	}
-
-	latestPoint := dataPoints[len(dataPoints)-1]
-	result.RawResidual = latestPoint.CVD // Use raw CVD as "residual"
-
-	// Percentile-based fallback
-	if cvda.config.FallbackMethod == "percentile" {
-		lookback := cvda.config.FallbackLookback
-		if lookback > len(dataPoints) {
-			lookback = len(dataPoints)
-		}
-
-		// Get recent CVD values for percentile ranking
-		recentValues := make([]float64, 0, lookback)
-		startIdx := len(dataPoints) - lookback
-		
-		for i := startIdx; i < len(dataPoints); i++ {
-			recentValues = append(recentValues, dataPoints[i].CVD)
-		}
-
-		result.PercentileRank = calculatePercentile(latestPoint.CVD, recentValues)
-		result.SignificanceScore = result.PercentileRank / 100.0
-
-		// Significance based on percentile threshold
-		result.IsSignificant = result.PercentileRank >= cvda.config.FallbackThreshold
-
-	} else if cvda.config.FallbackMethod == "zscore" {
-		// Z-score based fallback
-		lookback := cvda.config.FallbackLookback
-		if lookback > len(dataPoints) {
-			lookback = len(dataPoints)
-		}
-
-		// Calculate mean and std dev of recent CVD values
-		var sum, sumSq float64
-		startIdx := len(dataPoints) - lookback
-		
-		for i := startIdx; i < len(dataPoints); i++ {
-			val := dataPoints[i].CVD
-			sum += val
-			sumSq += val * val
-		}
-
-		mean := sum / float64(lookback)
-		variance := sumSq/float64(lookback) - mean*mean
-		stddev := math.Sqrt(variance)
-
-		if stddev > 0 {
-			result.NormalizedResidual = (latestPoint.CVD - mean) / stddev
-		}
-
-		result.SignificanceScore = math.Min(1.0, math.Abs(result.NormalizedResidual)/cvda.config.ResidualMinStdDev)
-		result.IsSignificant = math.Abs(result.NormalizedResidual) >= cvda.config.ResidualMinStdDev
-	}
-
-	return result
-}
-
-// calculatePercentileRank computes percentile rank of residual using regression model
-func (cvda *CVDResidualAnalyzer) calculatePercentileRank(rawResidual float64, dataPoints []*CVDDataPoint, model *CVDRegressionModel) float64 {
-	if len(dataPoints) < 2 {
-		return 50.0 // Default to median
-	}
-
-	// Calculate residuals for all data points
-	residuals := make([]float64, 0, len(dataPoints))
-	for _, dp := range dataPoints {
-		predicted := model.Intercept + model.PriceCoefficient*dp.PriceChange
-		residual := dp.CVD - predicted
-		residuals = append(residuals, residual)
-	}
-
-	return calculatePercentile(rawResidual, residuals)
-}
-
-// calculateSignificanceScore converts residual statistics to 0-1 score for Pre-Movement
-func (cvda *CVDResidualAnalyzer) calculateSignificanceScore(normalizedResidual, percentileRank float64) float64 {
-	// Combine z-score and percentile rank into unified significance score
-	
-	// Z-score component (0-1, capped at 3œÉ)
-	zScore := math.Min(1.0, math.Abs(normalizedResidual)/3.0)
-	
-	// Percentile component (0-1, distance from median)
-	percentileScore := math.Abs(percentileRank-50.0) / 50.0
-	
-	// Weighted average (favor z-score for significance)
-	significance := 0.7*zScore + 0.3*percentileScore
-	
-	return math.Min(1.0, significance)
-}
-
-// Helper function to calculate percentile bounds for winsorization
-func calculatePercentileBounds(values []float64, lowerPct, upperPct float64) (float64, float64) {
-	if len(values) == 0 {
-		return 0, 0
-	}
-
-	sorted := make([]float64, len(values))
-	copy(sorted, values)
-	sort.Float64s(sorted)
-
-	lowerIdx := int(float64(len(sorted)) * lowerPct / 100.0)
-	upperIdx := int(float64(len(sorted)) * upperPct / 100.0)
-
-	if lowerIdx >= len(sorted) {
-		lowerIdx = len(sorted) - 1
-	}
-	if upperIdx >= len(sorted) {
-		upperIdx = len(sorted) - 1
-	}
-
-	return sorted[lowerIdx], sorted[upperIdx]
-}
-
-// Helper function to calculate percentile rank of value in dataset
-func calculatePercentile(value float64, dataset []float64) float64 {
-	if len(dataset) == 0 {
-		return 50.0
-	}
-
-	count := 0
-	for _, v := range dataset {
-		if v <= value {
-			count++
-		}
-	}
-
-	return float64(count) / float64(len(dataset)) * 100.0
-}
-
-// GetResidualSummary returns a concise summary of CVD residual analysis
-func (cvdr *CVDResidualResult) GetResidualSummary() string {
-	method := cvdr.Method
-	if cvdr.FallbackReason != "" {
-		method = fmt.Sprintf("%s (fallback)", method)
-	}
-
-	significance := "üìä NORMAL"
-	if cvdr.IsSignificant {
-		significance = "‚ö†Ô∏è  SIGNIFICANT"
-	}
-
-	return fmt.Sprintf("%s ‚Äî %s CVD residual: %.3f (%.1f%%, %s, %dms)",
-		significance, cvdr.Symbol, cvdr.RawResidual, cvdr.PercentileRank, method, cvdr.ComputeTimeMs)
-}
-
-// GetDetailedAnalysis returns comprehensive CVD residual analysis
-func (cvdr *CVDResidualResult) GetDetailedAnalysis() string {
-	report := fmt.Sprintf("CVD Residual Analysis: %s\n", cvdr.Symbol)
-	report += fmt.Sprintf("Method: %s | Significant: %t | Time: %dms\n\n", cvdr.Method, cvdr.IsSignificant, cvdr.ComputeTimeMs)
-
-	// Residual metrics
-	report += fmt.Sprintf("Raw Residual: %.3f\n", cvdr.RawResidual)
-	if cvdr.NormalizedResidual != 0 {
-		report += fmt.Sprintf("Z-Score: %.2f\n", cvdr.NormalizedResidual)
-	}
-	report += fmt.Sprintf("Percentile Rank: %.1f%%\n", cvdr.PercentileRank)
-	report += fmt.Sprintf("Significance Score: %.3f\n\n", cvdr.SignificanceScore)
-
-	// Model information
-	if cvdr.Model != nil {
-		report += fmt.Sprintf("Regression Model:\n")
-		report += fmt.Sprintf("  R¬≤: %.3f | Std Error: %.3f | Data Points: %d\n", 
-			cvdr.Model.RSquared, cvdr.Model.StandardError, cvdr.Model.DataPoints)
-		report += fmt.Sprintf("  Equation: CVD = %.3f + %.3f √ó PriceChange\n\n", 
-			cvdr.Model.Intercept, cvdr.Model.PriceCoefficient)
-	}
-
-	// Fallback information
-	if cvdr.FallbackReason != "" {
-		report += fmt.Sprintf("Fallback Reason: %s\n\n", cvdr.FallbackReason)
-	}
-
-	// Data quality
-	if cvdr.DataQuality != nil {
-		report += fmt.Sprintf("Data Quality:\n")
-		report += fmt.Sprintf("  Points: %d available, %d used\n", 
-			cvdr.DataQuality.PointsAvailable, cvdr.DataQuality.PointsUsed)
-		if cvdr.DataQuality.WinsorizedPoints > 0 {
-			report += fmt.Sprintf("  Winsorized: %d outliers (%.1f%%)\n", 
-				cvdr.DataQuality.WinsorizedPoints, cvdr.DataQuality.OutliersPct)
-		}
-		report += fmt.Sprintf("  Data Span: %.1f hours\n", cvdr.DataQuality.DataSpanHours)
-	}
-
-	// Warnings
-	if len(cvdr.Warnings) > 0 {
-		report += fmt.Sprintf("\nWarnings:\n")
-		for i, warning := range cvdr.Warnings {
-			report += fmt.Sprintf("  %d. %s\n", i+1, warning)
-		}
-	}
-
-	return report
-}
\ No newline at end of file
+package premove
+
+import (
+	"context"
+	"fmt"
+	"math"
+	"sort"
+	"time"
+)
+
+// CVDResidualAnalyzer implements robust CVD residualization with R¬≤ fallback
+type CVDResidualAnalyzer struct {
+	config *CVDConfig
+}
+
+// NewCVDResidualAnalyzer creates a CVD residual analyzer with robust regression
+func NewCVDResidualAnalyzer(config *CVDConfig) *CVDResidualAnalyzer {
+	if config == nil {
+		config = DefaultCVDConfig()
+	}
+	return &CVDResidualAnalyzer{config: config}
+}
+
+// CVDConfig contains parameters for CVD residual analysis
+type CVDConfig struct {
+	// Regression parameters
+	MinDataPoints     int     `yaml:"min_data_points"`     // 50 minimum data points
+	WinsorizePctLower float64 `yaml:"winsorize_pct_lower"` // 5% lower winsorization
+	WinsorizePctUpper float64 `yaml:"winsorize_pct_upper"` // 95% upper winsorization
+	MinRSquared       float64 `yaml:"min_r_squared"`       // 0.30 minimum R¬≤ for regression
+	DailyRefitEnabled bool    `yaml:"daily_refit_enabled"` // true - refit model daily
+
+	// Fallback parameters
+	FallbackMethod    string  `yaml:"fallback_method"`    // "percentile" or "zscore"
+	FallbackLookback  int     `yaml:"fallback_lookback"`  // 20 periods for percentile rank
+	FallbackThreshold float64 `yaml:"fallback_threshold"` // 80% percentile threshold
+
+	// Residual analysis
+	ResidualMinStdDev float64 `yaml:"residual_min_std_dev"` // 2.0 std dev significance
+	ResidualMaxAge    int64   `yaml:"residual_max_age"`     // 3600 seconds (1 hour) max age
+
+	// Performance limits
+	MaxComputeTimeMs int64 `yaml:"max_compute_time_ms"` // 200ms compute timeout
+}
+
+// DefaultCVDConfig returns production CVD configuration
+func DefaultCVDConfig() *CVDConfig {
+	return &CVDConfig{
+		// Regression parameters
+		MinDataPoints:     50,
+		WinsorizePctLower: 5.0,
+		WinsorizePctUpper: 95.0,
+		MinRSquared:       0.30,
+		DailyRefitEnabled: true,
+
+		// Fallback parameters
+		FallbackMethod:    "percentile",
+		FallbackLookback:  20,
+		FallbackThreshold: 80.0,
+
+		// Residual analysis
+		ResidualMinStdDev: 2.0,
+		ResidualMaxAge:    3600, // 1 hour
+
+		// Performance
+		MaxComputeTimeMs: 200,
+	}
+}
+
+// CVDDataPoint represents a single CVD observation
+type CVDDataPoint struct {
+	Timestamp   time.Time `json:"timestamp"`
+	Price       float64   `json:"price"`        // Price at timestamp
+	CVD         float64   `json:"cvd"`          // Cumulative volume delta
+	Volume      float64   `json:"volume"`       // Volume at timestamp
+	PriceChange float64   `json:"price_change"` // Price change since previous
+}
+
+// CVDRegressionModel contains fitted regression coefficients and statistics
+type CVDRegressionModel struct {
+	Symbol           string    `json:"symbol"`
+	FitTimestamp     time.Time `json:"fit_timestamp"`
+	Intercept        float64   `json:"intercept"`         // Œ≤‚ÇÄ
+	PriceCoefficient float64   `json:"price_coefficient"` // Œ≤‚ÇÅ - price change coefficient
+	RSquared         float64   `json:"r_squared"`         // Model R¬≤
+	StandardError    float64   `json:"standard_error"`    // Residual standard error
+	DataPoints       int       `json:"data_points"`       // Number of points used in fit
+	IsValid          bool      `json:"is_valid"`          // Whether model meets quality thresholds
+	LastRefit        time.Time `json:"last_refit"`        // When model was last refit
+}
+
+// CVDResidualResult contains residual analysis output
+type CVDResidualResult struct {
+	Symbol             string              `json:"symbol"`
+	Timestamp          time.Time           `json:"timestamp"`
+	RawResidual        float64             `json:"raw_residual"`        // Raw CVD residual
+	NormalizedResidual float64             `json:"normalized_residual"` // Z-score normalized residual
+	PercentileRank     float64             `json:"percentile_rank"`     // 0-100 percentile rank
+	SignificanceScore  float64             `json:"significance_score"`  // 0-1 significance (for scoring)
+	Method             string              `json:"method"`              // "regression" or fallback method
+	Model              *CVDRegressionModel `json:"model"`               // Regression model used (if applicable)
+	FallbackReason     string              `json:"fallback_reason"`     // Why fallback was used
+	ComputeTimeMs      int64               `json:"compute_time_ms"`     // Analysis compute time
+	DataQuality        *CVDDataQuality     `json:"data_quality"`        // Data quality assessment
+	IsSignificant      bool                `json:"is_significant"`      // Whether residual is significant
+	Warnings           []string            `json:"warnings"`            // Analysis warnings
+}
+
+// CVDDataQuality contains data quality metrics
+type CVDDataQuality struct {
+	PointsAvailable  int     `json:"points_available"`  // Total data points available
+	PointsUsed       int     `json:"points_used"`       // Points used after winsorization
+	WinsorizedPoints int     `json:"winsorized_points"` // Points removed by winsorization
+	DataSpanHours    float64 `json:"data_span_hours"`   // Time span of data
+	MissingDataPct   float64 `json:"missing_data_pct"`  // % missing data points
+	OutliersPct      float64 `json:"outliers_pct"`      // % outliers detected
+}
+
+// AnalyzeCVDResidual performs comprehensive CVD residual analysis with fallbacks
+func (cvda *CVDResidualAnalyzer) AnalyzeCVDResidual(ctx context.Context, symbol string, dataPoints []*CVDDataPoint) (*CVDResidualResult, error) {
+	startTime := time.Now()
+
+	result := &CVDResidualResult{
+		Symbol:    symbol,
+		Timestamp: time.Now(),
+		Method:    "regression", // Default to regression, may fallback
+		Warnings:  []string{},
+		DataQuality: &CVDDataQuality{
+			PointsAvailable: len(dataPoints),
+		},
+	}
+
+	// Data quality assessment
+	if len(dataPoints) == 0 {
+		return nil, fmt.Errorf("no CVD data points provided for %s", symbol)
+	}
+
+	// Calculate data span and quality metrics
+	cvda.assessDataQuality(dataPoints, result.DataQuality)
+
+	// Check minimum data requirements
+	if len(dataPoints) < cvda.config.MinDataPoints {
+		result.FallbackReason = fmt.Sprintf("Insufficient data points (%d < %d)", len(dataPoints), cvda.config.MinDataPoints)
+		return cvda.performFallbackAnalysis(dataPoints, result), nil
+	}
+
+	// Winsorize data to remove extreme outliers
+	winsorizedData, outlierCount := cvda.winsorizeData(dataPoints)
+	result.DataQuality.PointsUsed = len(winsorizedData)
+	result.DataQuality.WinsorizedPoints = outlierCount
+	result.DataQuality.OutliersPct = float64(outlierCount) / float64(len(dataPoints)) * 100.0
+
+	if len(winsorizedData) < cvda.config.MinDataPoints {
+		result.FallbackReason = fmt.Sprintf("Too many outliers removed (%d < %d after winsorization)", len(winsorizedData), cvda.config.MinDataPoints)
+		return cvda.performFallbackAnalysis(dataPoints, result), nil
+	}
+
+	// Fit regression model: CVD = Œ≤‚ÇÄ + Œ≤‚ÇÅ * PriceChange + Œµ
+	model, err := cvda.fitRegressionModel(symbol, winsorizedData)
+	if err != nil {
+		result.FallbackReason = fmt.Sprintf("Regression fitting failed: %v", err)
+		return cvda.performFallbackAnalysis(dataPoints, result), nil
+	}
+
+	result.Model = model
+
+	// Check model quality (R¬≤ threshold)
+	if model.RSquared < cvda.config.MinRSquared {
+		result.FallbackReason = fmt.Sprintf("Low R¬≤ (%.3f < %.3f)", model.RSquared, cvda.config.MinRSquared)
+		return cvda.performFallbackAnalysis(dataPoints, result), nil
+	}
+
+	// Calculate residual for most recent data point
+	latestPoint := dataPoints[len(dataPoints)-1]
+	predicted := model.Intercept + model.PriceCoefficient*latestPoint.PriceChange
+	result.RawResidual = latestPoint.CVD - predicted
+
+	// Normalize residual using model standard error
+	if model.StandardError > 0 {
+		result.NormalizedResidual = result.RawResidual / model.StandardError
+	}
+
+	// Calculate percentile rank using historical residuals
+	result.PercentileRank = cvda.calculatePercentileRank(result.RawResidual, winsorizedData, model)
+
+	// Calculate significance score (0-1) for use in Pre-Movement scoring
+	result.SignificanceScore = cvda.calculateSignificanceScore(result.NormalizedResidual, result.PercentileRank)
+
+	// Determine significance based on threshold
+	result.IsSignificant = math.Abs(result.NormalizedResidual) >= cvda.config.ResidualMinStdDev
+
+	result.ComputeTimeMs = time.Since(startTime).Milliseconds()
+
+	// Performance warning
+	if result.ComputeTimeMs > cvda.config.MaxComputeTimeMs {
+		result.Warnings = append(result.Warnings,
+			fmt.Sprintf("CVD analysis took %dms (>%dms threshold)", result.ComputeTimeMs, cvda.config.MaxComputeTimeMs))
+	}
+
+	return result, nil
+}
+
+// assessDataQuality evaluates input data quality
+func (cvda *CVDResidualAnalyzer) assessDataQuality(dataPoints []*CVDDataPoint, quality *CVDDataQuality) {
+	if len(dataPoints) < 2 {
+		return
+	}
+
+	// Calculate data span
+	firstTime := dataPoints[0].Timestamp
+	lastTime := dataPoints[len(dataPoints)-1].Timestamp
+	quality.DataSpanHours = lastTime.Sub(firstTime).Hours()
+
+	// Detect missing data (gaps > 2x median interval)
+	intervals := make([]float64, 0, len(dataPoints)-1)
+	for i := 1; i < len(dataPoints); i++ {
+		interval := dataPoints[i].Timestamp.Sub(dataPoints[i-1].Timestamp).Minutes()
+		intervals = append(intervals, interval)
+	}
+
+	sort.Float64s(intervals)
+	medianInterval := intervals[len(intervals)/2]
+
+	missingCount := 0
+	for _, interval := range intervals {
+		if interval > 2*medianInterval {
+			missingCount++
+		}
+	}
+	quality.MissingDataPct = float64(missingCount) / float64(len(intervals)) * 100.0
+}
+
+// winsorizeData removes extreme outliers using percentile-based winsorization
+func (cvda *CVDResidualAnalyzer) winsorizeData(dataPoints []*CVDDataPoint) ([]*CVDDataPoint, int) {
+	if len(dataPoints) <= 10 {
+		return dataPoints, 0 // Don't winsorize small datasets
+	}
+
+	// Extract CVD and price change values for percentile calculation
+	cvdValues := make([]float64, len(dataPoints))
+	priceChanges := make([]float64, len(dataPoints))
+
+	for i, dp := range dataPoints {
+		cvdValues[i] = dp.CVD
+		priceChanges[i] = dp.PriceChange
+	}
+
+	// Calculate winsorization bounds
+	cvdLower, cvdUpper := calculatePercentileBounds(cvdValues, cvda.config.WinsorizePctLower, cvda.config.WinsorizePctUpper)
+	priceLower, priceUpper := calculatePercentileBounds(priceChanges, cvda.config.WinsorizePctLower, cvda.config.WinsorizePctUpper)
+
+	// Filter out extreme outliers
+	filtered := make([]*CVDDataPoint, 0, len(dataPoints))
+	outlierCount := 0
+
+	for _, dp := range dataPoints {
+		if dp.CVD >= cvdLower && dp.CVD <= cvdUpper &&
+			dp.PriceChange >= priceLower && dp.PriceChange <= priceUpper {
+			filtered = append(filtered, dp)
+		} else {
+			outlierCount++
+		}
+	}
+
+	return filtered, outlierCount
+}
+
+// fitRegressionModel fits CVD = Œ≤‚ÇÄ + Œ≤‚ÇÅ * PriceChange + Œµ using least squares
+func (cvda *CVDResidualAnalyzer) fitRegressionModel(symbol string, dataPoints []*CVDDataPoint) (*CVDRegressionModel, error) {
+	n := len(dataPoints)
+	if n < 3 {
+		return nil, fmt.Errorf("insufficient data for regression (need ‚â•3, got %d)", n)
+	}
+
+	// Extract X (price changes) and Y (CVD) vectors
+	var sumX, sumY, sumXY, sumXX, sumYY float64
+
+	for _, dp := range dataPoints {
+		x := dp.PriceChange
+		y := dp.CVD
+
+		sumX += x
+		sumY += y
+		sumXY += x * y
+		sumXX += x * x
+		sumYY += y * y
+	}
+
+	// Calculate means
+	meanX := sumX / float64(n)
+	meanY := sumY / float64(n)
+
+	// Calculate regression coefficients
+	// Œ≤‚ÇÅ = (n‚àëXY - ‚àëX‚àëY) / (n‚àëX¬≤ - (‚àëX)¬≤)
+	// Œ≤‚ÇÄ = »≥ - Œ≤‚ÇÅxÃÑ
+
+	numerator := float64(n)*sumXY - sumX*sumY
+	denominator := float64(n)*sumXX - sumX*sumX
+
+	if math.Abs(denominator) < 1e-10 {
+		return nil, fmt.Errorf("singular matrix - no price variation")
+	}
+
+	beta1 := numerator / denominator
+	beta0 := meanY - beta1*meanX
+
+	// Calculate R¬≤ and standard error
+	var ssRes, ssTot float64
+	for _, dp := range dataPoints {
+		predicted := beta0 + beta1*dp.PriceChange
+		residual := dp.CVD - predicted
+
+		ssRes += residual * residual
+		ssTot += (dp.CVD - meanY) * (dp.CVD - meanY)
+	}
+
+	var rSquared float64
+	if ssTot > 0 {
+		rSquared = 1.0 - ssRes/ssTot
+	}
+
+	standardError := math.Sqrt(ssRes / float64(n-2))
+
+	model := &CVDRegressionModel{
+		Symbol:           symbol,
+		FitTimestamp:     time.Now(),
+		Intercept:        beta0,
+		PriceCoefficient: beta1,
+		RSquared:         rSquared,
+		StandardError:    standardError,
+		DataPoints:       n,
+		IsValid:          rSquared >= cvda.config.MinRSquared,
+		LastRefit:        time.Now(),
+	}
+
+	return model, nil
+}
+
+// performFallbackAnalysis uses percentile ranking when regression fails
+func (cvda *CVDResidualAnalyzer) performFallbackAnalysis(dataPoints []*CVDDataPoint, result *CVDResidualResult) *CVDResidualResult {
+	result.Method = cvda.config.FallbackMethod
+
+	if len(dataPoints) == 0 {
+		result.Warnings = append(result.Warnings, "No data available for fallback analysis")
+		return result
+	}
+
+	latestPoint := dataPoints[len(dataPoints)-1]
+	result.RawResidual = latestPoint.CVD // Use raw CVD as "residual"
+
+	// Percentile-based fallback
+	if cvda.config.FallbackMethod == "percentile" {
+		lookback := cvda.config.FallbackLookback
+		if lookback > len(dataPoints) {
+			lookback = len(dataPoints)
+		}
+
+		// Get recent CVD values for percentile ranking
+		recentValues := make([]float64, 0, lookback)
+		startIdx := len(dataPoints) - lookback
+
+		for i := startIdx; i < len(dataPoints); i++ {
+			recentValues = append(recentValues, dataPoints[i].CVD)
+		}
+
+		result.PercentileRank = calculatePercentile(latestPoint.CVD, recentValues)
+		result.SignificanceScore = result.PercentileRank / 100.0
+
+		// Significance based on percentile threshold
+		result.IsSignificant = result.PercentileRank >= cvda.config.FallbackThreshold
+
+	} else if cvda.config.FallbackMethod == "zscore" {
+		// Z-score based fallback
+		lookback := cvda.config.FallbackLookback
+		if lookback > len(dataPoints) {
+			lookback = len(dataPoints)
+		}
+
+		// Calculate mean and std dev of recent CVD values
+		var sum, sumSq float64
+		startIdx := len(dataPoints) - lookback
+
+		for i := startIdx; i < len(dataPoints); i++ {
+			val := dataPoints[i].CVD
+			sum += val
+			sumSq += val * val
+		}
+
+		mean := sum / float64(lookback)
+		variance := sumSq/float64(lookback) - mean*mean
+		stddev := math.Sqrt(variance)
+
+		if stddev > 0 {
+			result.NormalizedResidual = (latestPoint.CVD - mean) / stddev
+		}
+
+		result.SignificanceScore = math.Min(1.0, math.Abs(result.NormalizedResidual)/cvda.config.ResidualMinStdDev)
+		result.IsSignificant = math.Abs(result.NormalizedResidual) >= cvda.config.ResidualMinStdDev
+	}
+
+	return result
+}
+
+// calculatePercentileRank computes percentile rank of residual using regression model
+func (cvda *CVDResidualAnalyzer) calculatePercentileRank(rawResidual float64, dataPoints []*CVDDataPoint, model *CVDRegressionModel) float64 {
+	if len(dataPoints) < 2 {
+		return 50.0 // Default to median
+	}
+
+	// Calculate residuals for all data points
+	residuals := make([]float64, 0, len(dataPoints))
+	for _, dp := range dataPoints {
+		predicted := model.Intercept + model.PriceCoefficient*dp.PriceChange
+		residual := dp.CVD - predicted
+		residuals = append(residuals, residual)
+	}
+
+	return calculatePercentile(rawResidual, residuals)
+}
+
+// calculateSignificanceScore converts residual statistics to 0-1 score for Pre-Movement
+func (cvda *CVDResidualAnalyzer) calculateSignificanceScore(normalizedResidual, percentileRank float64) float64 {
+	// Combine z-score and percentile rank into unified significance score
+
+	// Z-score component (0-1, capped at 3œÉ)
+	zScore := math.Min(1.0, math.Abs(normalizedResidual)/3.0)
+
+	// Percentile component (0-1, distance from median)
+	percentileScore := math.Abs(percentileRank-50.0) / 50.0
+
+	// Weighted average (favor z-score for significance)
+	significance := 0.7*zScore + 0.3*percentileScore
+
+	return math.Min(1.0, significance)
+}
+
+// Helper function to calculate percentile bounds for winsorization
+func calculatePercentileBounds(values []float64, lowerPct, upperPct float64) (float64, float64) {
+	if len(values) == 0 {
+		return 0, 0
+	}
+
+	sorted := make([]float64, len(values))
+	copy(sorted, values)
+	sort.Float64s(sorted)
+
+	lowerIdx := int(float64(len(sorted)) * lowerPct / 100.0)
+	upperIdx := int(float64(len(sorted)) * upperPct / 100.0)
+
+	if lowerIdx >= len(sorted) {
+		lowerIdx = len(sorted) - 1
+	}
+	if upperIdx >= len(sorted) {
+		upperIdx = len(sorted) - 1
+	}
+
+	return sorted[lowerIdx], sorted[upperIdx]
+}
+
+// Helper function to calculate percentile rank of value in dataset
+func calculatePercentile(value float64, dataset []float64) float64 {
+	if len(dataset) == 0 {
+		return 50.0
+	}
+
+	count := 0
+	for _, v := range dataset {
+		if v <= value {
+			count++
+		}
+	}
+
+	return float64(count) / float64(len(dataset)) * 100.0
+}
+
+// GetResidualSummary returns a concise summary of CVD residual analysis
+func (cvdr *CVDResidualResult) GetResidualSummary() string {
+	method := cvdr.Method
+	if cvdr.FallbackReason != "" {
+		method = fmt.Sprintf("%s (fallback)", method)
+	}
+
+	significance := "üìä NORMAL"
+	if cvdr.IsSignificant {
+		significance = "‚ö†Ô∏è  SIGNIFICANT"
+	}
+
+	return fmt.Sprintf("%s ‚Äî %s CVD residual: %.3f (%.1f%%, %s, %dms)",
+		significance, cvdr.Symbol, cvdr.RawResidual, cvdr.PercentileRank, method, cvdr.ComputeTimeMs)
+}
+
+// GetDetailedAnalysis returns comprehensive CVD residual analysis
+func (cvdr *CVDResidualResult) GetDetailedAnalysis() string {
+	report := fmt.Sprintf("CVD Residual Analysis: %s\n", cvdr.Symbol)
+	report += fmt.Sprintf("Method: %s | Significant: %t | Time: %dms\n\n", cvdr.Method, cvdr.IsSignificant, cvdr.ComputeTimeMs)
+
+	// Residual metrics
+	report += fmt.Sprintf("Raw Residual: %.3f\n", cvdr.RawResidual)
+	if cvdr.NormalizedResidual != 0 {
+		report += fmt.Sprintf("Z-Score: %.2f\n", cvdr.NormalizedResidual)
+	}
+	report += fmt.Sprintf("Percentile Rank: %.1f%%\n", cvdr.PercentileRank)
+	report += fmt.Sprintf("Significance Score: %.3f\n\n", cvdr.SignificanceScore)
+
+	// Model information
+	if cvdr.Model != nil {
+		report += fmt.Sprintf("Regression Model:\n")
+		report += fmt.Sprintf("  R¬≤: %.3f | Std Error: %.3f | Data Points: %d\n",
+			cvdr.Model.RSquared, cvdr.Model.StandardError, cvdr.Model.DataPoints)
+		report += fmt.Sprintf("  Equation: CVD = %.3f + %.3f √ó PriceChange\n\n",
+			cvdr.Model.Intercept, cvdr.Model.PriceCoefficient)
+	}
+
+	// Fallback information
+	if cvdr.FallbackReason != "" {
+		report += fmt.Sprintf("Fallback Reason: %s\n\n", cvdr.FallbackReason)
+	}
+
+	// Data quality
+	if cvdr.DataQuality != nil {
+		report += fmt.Sprintf("Data Quality:\n")
+		report += fmt.Sprintf("  Points: %d available, %d used\n",
+			cvdr.DataQuality.PointsAvailable, cvdr.DataQuality.PointsUsed)
+		if cvdr.DataQuality.WinsorizedPoints > 0 {
+			report += fmt.Sprintf("  Winsorized: %d outliers (%.1f%%)\n",
+				cvdr.DataQuality.WinsorizedPoints, cvdr.DataQuality.OutliersPct)
+		}
+		report += fmt.Sprintf("  Data Span: %.1f hours\n", cvdr.DataQuality.DataSpanHours)
+	}
+
+	// Warnings
+	if len(cvdr.Warnings) > 0 {
+		report += fmt.Sprintf("\nWarnings:\n")
+		for i, warning := range cvdr.Warnings {
+			report += fmt.Sprintf("  %d. %s\n", i+1, warning)
+		}
+	}
+
+	return report
+}
diff --git a/internal/premove/cvd_resid_test.go b/internal/premove/cvd_resid_test.go
index 6b9b384..1bce92b 100644
--- a/internal/premove/cvd_resid_test.go
+++ b/internal/premove/cvd_resid_test.go
@@ -1,427 +1,427 @@
-package premove
-
-import (
-	"context"
-	"math"
-	"testing"
-	"time"
-
-	"github.com/stretchr/testify/assert"
-	"github.com/stretchr/testify/require"
-)
-
-func TestCVDResidualAnalyzer_AnalyzeCVDResidual_RegressionSuccess(t *testing.T) {
-	analyzer := NewCVDResidualAnalyzer(nil) // Use default config
-
-	// Generate synthetic data with clear price-CVD relationship
-	dataPoints := generateSyntheticCVDData(100, 0.8) // 100 points, 80% R¬≤
-
-	result, err := analyzer.AnalyzeCVDResidual(context.Background(), "BTC-USD", dataPoints)
-	require.NoError(t, err)
-	assert.NotNil(t, result)
-
-	// Should use regression method successfully
-	assert.Equal(t, "regression", result.Method)
-	assert.Empty(t, result.FallbackReason, "Should not fallback with good data")
-	assert.NotNil(t, result.Model)
-
-	// Model should meet quality thresholds
-	assert.GreaterOrEqual(t, result.Model.RSquared, 0.30, "R¬≤ should meet minimum threshold")
-	assert.True(t, result.Model.IsValid, "Model should be valid")
-	assert.Equal(t, 100, result.Model.DataPoints)
-
-	// Should calculate residual metrics
-	assert.NotEqual(t, 0.0, result.RawResidual, "Should have non-zero residual")
-	assert.GreaterOrEqual(t, result.PercentileRank, 0.0)
-	assert.LessOrEqual(t, result.PercentileRank, 100.0)
-	assert.GreaterOrEqual(t, result.SignificanceScore, 0.0)
-	assert.LessOrEqual(t, result.SignificanceScore, 1.0)
-
-	// Should assess data quality
-	assert.NotNil(t, result.DataQuality)
-	assert.Equal(t, 100, result.DataQuality.PointsAvailable)
-	assert.GreaterOrEqual(t, result.DataQuality.PointsUsed, 90) // Should retain most points
-}
-
-func TestCVDResidualAnalyzer_AnalyzeCVDResidual_LowRSquaredFallback(t *testing.T) {
-	analyzer := NewCVDResidualAnalyzer(nil)
-
-	// Generate noisy data with low correlation (should trigger fallback)
-	dataPoints := generateNoisyCVDData(60) // Random relationship
-
-	result, err := analyzer.AnalyzeCVDResidual(context.Background(), "ETH-USD", dataPoints)
-	require.NoError(t, err)
-
-	// Should fallback due to low R¬≤
-	assert.Equal(t, "percentile", result.Method)
-	assert.Contains(t, result.FallbackReason, "Low R¬≤")
-	assert.Nil(t, result.Model) // No valid regression model
-
-	// Should still calculate percentile-based significance
-	assert.GreaterOrEqual(t, result.PercentileRank, 0.0)
-	assert.LessOrEqual(t, result.PercentileRank, 100.0)
-	assert.GreaterOrEqual(t, result.SignificanceScore, 0.0)
-	assert.LessOrEqual(t, result.SignificanceScore, 1.0)
-}
-
-func TestCVDResidualAnalyzer_AnalyzeCVDResidual_InsufficientData(t *testing.T) {
-	analyzer := NewCVDResidualAnalyzer(nil)
-
-	// Generate insufficient data points (below minimum threshold)
-	dataPoints := generateSyntheticCVDData(20, 0.7) // Only 20 points
-
-	result, err := analyzer.AnalyzeCVDResidual(context.Background(), "SOL-USD", dataPoints)
-	require.NoError(t, err)
-
-	// Should fallback due to insufficient data
-	assert.NotEqual(t, "regression", result.Method)
-	assert.Contains(t, result.FallbackReason, "Insufficient data points")
-
-	// Should still provide fallback analysis
-	assert.GreaterOrEqual(t, result.SignificanceScore, 0.0)
-	assert.LessOrEqual(t, result.SignificanceScore, 1.0)
-}
-
-func TestCVDResidualAnalyzer_WinsorizeData_OutlierRemoval(t *testing.T) {
-	analyzer := NewCVDResidualAnalyzer(nil)
-
-	// Generate data with extreme outliers
-	dataPoints := make([]*CVDDataPoint, 0, 100)
-	baseTime := time.Now()
-
-	// Normal data points
-	for i := 0; i < 90; i++ {
-		dp := &CVDDataPoint{
-			Timestamp:   baseTime.Add(time.Duration(i) * time.Minute),
-			Price:       100.0 + float64(i)*0.1,
-			CVD:         1000.0 + float64(i)*10.0,
-			PriceChange: 0.1,
-		}
-		dataPoints = append(dataPoints, dp)
-	}
-
-	// Add extreme outliers
-	for i := 90; i < 100; i++ {
-		dp := &CVDDataPoint{
-			Timestamp:   baseTime.Add(time.Duration(i) * time.Minute),
-			Price:       1000.0,  // Extreme price
-			CVD:         100000.0, // Extreme CVD
-			PriceChange: 50.0,    // Extreme change
-		}
-		dataPoints = append(dataPoints, dp)
-	}
-
-	// Test winsorization
-	filtered, outlierCount := analyzer.winsorizeData(dataPoints)
-
-	assert.Greater(t, outlierCount, 0, "Should remove some outliers")
-	assert.Less(t, len(filtered), len(dataPoints), "Should filter out outliers")
-	assert.GreaterOrEqual(t, len(filtered), 80, "Should retain most normal data")
-
-	// Test with small dataset (should not winsorize)
-	smallData := dataPoints[:5]
-	filteredSmall, outlierCountSmall := analyzer.winsorizeData(smallData)
-	assert.Equal(t, 0, outlierCountSmall, "Should not winsorize small datasets")
-	assert.Equal(t, len(smallData), len(filteredSmall), "Should retain all small dataset points")
-}
-
-func TestCVDResidualAnalyzer_FitRegressionModel(t *testing.T) {
-	analyzer := NewCVDResidualAnalyzer(nil)
-
-	// Generate perfect linear relationship: CVD = 1000 + 100 * PriceChange
-	dataPoints := make([]*CVDDataPoint, 0, 50)
-	baseTime := time.Now()
-
-	for i := 0; i < 50; i++ {
-		priceChange := float64(i-25) * 0.1 // Price changes from -2.5 to +2.4
-		cvd := 1000.0 + 100.0*priceChange  // Perfect linear relationship
-		
-		dp := &CVDDataPoint{
-			Timestamp:   baseTime.Add(time.Duration(i) * time.Minute),
-			PriceChange: priceChange,
-			CVD:         cvd,
-		}
-		dataPoints = append(dataPoints, dp)
-	}
-
-	model, err := analyzer.fitRegressionModel("TEST-USD", dataPoints)
-	require.NoError(t, err)
-	assert.NotNil(t, model)
-
-	// Should recover correct coefficients (approximately)
-	assert.InDelta(t, 1000.0, model.Intercept, 1.0, "Should recover correct intercept")
-	assert.InDelta(t, 100.0, model.PriceCoefficient, 1.0, "Should recover correct slope")
-	assert.Greater(t, model.RSquared, 0.99, "Perfect linear relationship should have R¬≤ > 0.99")
-	assert.True(t, model.IsValid)
-}
-
-func TestCVDResidualAnalyzer_PerformFallbackAnalysis_Percentile(t *testing.T) {
-	config := DefaultCVDConfig()
-	config.FallbackMethod = "percentile"
-	config.FallbackLookback = 10
-	config.FallbackThreshold = 80.0
-
-	analyzer := NewCVDResidualAnalyzer(config)
-
-	// Generate data where latest point is extreme
-	dataPoints := make([]*CVDDataPoint, 0, 15)
-	baseTime := time.Now()
-
-	// Normal CVD values
-	for i := 0; i < 14; i++ {
-		dp := &CVDDataPoint{
-			Timestamp: baseTime.Add(time.Duration(i) * time.Minute),
-			CVD:       1000.0 + float64(i)*10.0, // Values 1000-1130
-		}
-		dataPoints = append(dataPoints, dp)
-	}
-
-	// Extreme latest value
-	extremeDP := &CVDDataPoint{
-		Timestamp: baseTime.Add(15 * time.Minute),
-		CVD:       2000.0, // Much higher than others
-	}
-	dataPoints = append(dataPoints, extremeDP)
-
-	result := &CVDResidualResult{
-		Symbol:    "TEST-USD",
-		Timestamp: time.Now(),
-	}
-
-	finalResult := analyzer.performFallbackAnalysis(dataPoints, result)
-
-	assert.Equal(t, "percentile", finalResult.Method)
-	assert.Equal(t, 2000.0, finalResult.RawResidual) // Should use raw CVD
-	assert.GreaterOrEqual(t, finalResult.PercentileRank, 90.0, "Extreme value should have high percentile rank")
-	assert.True(t, finalResult.IsSignificant, "Should be significant above threshold")
-}
-
-func TestCVDResidualAnalyzer_PerformFallbackAnalysis_ZScore(t *testing.T) {
-	config := DefaultCVDConfig()
-	config.FallbackMethod = "zscore"
-	config.FallbackLookback = 10
-	config.ResidualMinStdDev = 2.0
-
-	analyzer := NewCVDResidualAnalyzer(config)
-
-	// Generate data with known mean and std dev
-	dataPoints := make([]*CVDDataPoint, 0, 15)
-	baseTime := time.Now()
-
-	// CVD values with mean=1000, std dev ‚âà 10
-	cvdValues := []float64{980, 985, 990, 995, 1000, 1005, 1010, 1015, 1020}
-	for i, cvd := range cvdValues {
-		dp := &CVDDataPoint{
-			Timestamp: baseTime.Add(time.Duration(i) * time.Minute),
-			CVD:       cvd,
-		}
-		dataPoints = append(dataPoints, dp)
-	}
-
-	// Add extreme latest value (3 std devs from mean)
-	extremeDP := &CVDDataPoint{
-		Timestamp: baseTime.Add(10 * time.Minute),
-		CVD:       1030.0, // ~3 std devs above mean
-	}
-	dataPoints = append(dataPoints, extremeDP)
-
-	result := &CVDResidualResult{
-		Symbol:    "TEST-USD", 
-		Timestamp: time.Now(),
-	}
-
-	finalResult := analyzer.performFallbackAnalysis(dataPoints, result)
-
-	assert.Equal(t, "zscore", finalResult.Method)
-	assert.Greater(t, math.Abs(finalResult.NormalizedResidual), 2.0, "Should have >2œÉ z-score")
-	assert.True(t, finalResult.IsSignificant, "Should be significant above threshold")
-}
-
-func TestCVDResidualAnalyzer_CalculateSignificanceScore(t *testing.T) {
-	analyzer := NewCVDResidualAnalyzer(nil)
-
-	// Test extreme significance (high z-score and percentile)
-	score1 := analyzer.calculateSignificanceScore(3.0, 95.0) // 3œÉ, 95th percentile
-	assert.GreaterOrEqual(t, score1, 0.8, "Extreme values should have high significance")
-
-	// Test moderate significance
-	score2 := analyzer.calculateSignificanceScore(1.5, 70.0) // 1.5œÉ, 70th percentile
-	assert.GreaterOrEqual(t, score2, 0.3)
-	assert.LessOrEqual(t, score2, 0.7)
-
-	// Test low significance
-	score3 := analyzer.calculateSignificanceScore(0.5, 55.0) // 0.5œÉ, near median
-	assert.LessOrEqual(t, score3, 0.4, "Low values should have low significance")
-
-	// All scores should be in valid range
-	scores := []float64{score1, score2, score3}
-	for _, score := range scores {
-		assert.GreaterOrEqual(t, score, 0.0, "Significance score should be ‚â• 0")
-		assert.LessOrEqual(t, score, 1.0, "Significance score should be ‚â§ 1")
-	}
-}
-
-func TestCVDResidualAnalyzer_PerformanceRequirements(t *testing.T) {
-	config := DefaultCVDConfig()
-	config.MaxComputeTimeMs = 100 // 100ms limit for testing
-
-	analyzer := NewCVDResidualAnalyzer(config)
-	dataPoints := generateSyntheticCVDData(200, 0.7) // Larger dataset
-
-	start := time.Now()
-	result, err := analyzer.AnalyzeCVDResidual(context.Background(), "PERF-TEST", dataPoints)
-	duration := time.Since(start)
-
-	require.NoError(t, err)
-	assert.NotNil(t, result)
-	assert.Less(t, duration.Milliseconds(), int64(500), "Analysis should complete in <500ms")
-	assert.Greater(t, result.ComputeTimeMs, int64(0), "Should report compute time")
-}
-
-func TestCVDResidualAnalyzer_GetResidualSummary(t *testing.T) {
-	result := &CVDResidualResult{
-		Symbol:            "BTC-USD",
-		RawResidual:       1234.5,
-		PercentileRank:    87.3,
-		Method:            "regression",
-		ComputeTimeMs:     45,
-		IsSignificant:     true,
-	}
-
-	summary := result.GetResidualSummary()
-	assert.Contains(t, summary, "‚ö†Ô∏è  SIGNIFICANT")
-	assert.Contains(t, summary, "BTC-USD")
-	assert.Contains(t, summary, "1234.5")
-	assert.Contains(t, summary, "87.3%")
-	assert.Contains(t, summary, "regression")
-	assert.Contains(t, summary, "45ms")
-}
-
-func TestCVDResidualAnalyzer_GetDetailedAnalysis(t *testing.T) {
-	result := &CVDResidualResult{
-		Symbol:             "ETH-USD",
-		Method:             "regression",
-		IsSignificant:      true,
-		ComputeTimeMs:      67,
-		RawResidual:        -567.8,
-		NormalizedResidual: -2.3,
-		PercentileRank:     15.2,
-		SignificanceScore:  0.82,
-		Model: &CVDRegressionModel{
-			RSquared:         0.78,
-			StandardError:    245.6,
-			DataPoints:       89,
-			Intercept:        1234.5,
-			PriceCoefficient: 123.4,
-		},
-		DataQuality: &CVDDataQuality{
-			PointsAvailable:  100,
-			PointsUsed:       89,
-			WinsorizedPoints: 11,
-			OutliersPct:      11.0,
-			DataSpanHours:    6.5,
-		},
-	}
-
-	analysis := result.GetDetailedAnalysis()
-	assert.Contains(t, analysis, "ETH-USD")
-	assert.Contains(t, analysis, "regression")
-	assert.Contains(t, analysis, "true")
-	assert.Contains(t, analysis, "-567.8")
-	assert.Contains(t, analysis, "-2.3")
-	assert.Contains(t, analysis, "15.2%")
-	assert.Contains(t, analysis, "R¬≤: 0.78")
-	assert.Contains(t, analysis, "CVD = 1234.5 + 123.4 √ó PriceChange")
-	assert.Contains(t, analysis, "100 available, 89 used")
-	assert.Contains(t, analysis, "Winsorized: 11 outliers")
-}
-
-func TestDefaultCVDConfig(t *testing.T) {
-	config := DefaultCVDConfig()
-	require.NotNil(t, config)
-
-	// Check regression parameters
-	assert.Equal(t, 50, config.MinDataPoints)
-	assert.Equal(t, 5.0, config.WinsorizePctLower)
-	assert.Equal(t, 95.0, config.WinsorizePctUpper)
-	assert.Equal(t, 0.30, config.MinRSquared)
-	assert.True(t, config.DailyRefitEnabled)
-
-	// Check fallback parameters
-	assert.Equal(t, "percentile", config.FallbackMethod)
-	assert.Equal(t, 20, config.FallbackLookback)
-	assert.Equal(t, 80.0, config.FallbackThreshold)
-
-	// Check residual analysis parameters
-	assert.Equal(t, 2.0, config.ResidualMinStdDev)
-	assert.Equal(t, int64(3600), config.ResidualMaxAge)
-
-	// Check performance limits
-	assert.Equal(t, int64(200), config.MaxComputeTimeMs)
-}
-
-// Helper functions for test data generation
-
-func generateSyntheticCVDData(count int, rSquared float64) []*CVDDataPoint {
-	dataPoints := make([]*CVDDataPoint, 0, count)
-	baseTime := time.Now()
-	
-	// Generate data with controlled R¬≤
-	noiseLevel := math.Sqrt(1.0 - rSquared) // Adjust noise to achieve target R¬≤
-
-	for i := 0; i < count; i++ {
-		priceChange := (float64(i) - float64(count)/2.0) / 10.0 // Price changes around 0
-		
-		// True relationship: CVD = 1000 + 50 * priceChange
-		trueCVD := 1000.0 + 50.0*priceChange
-		
-		// Add noise to achieve target R¬≤
-		noise := (rand.Float64() - 0.5) * noiseLevel * 200.0 // ¬±100 noise range
-		observedCVD := trueCVD + noise
-
-		dp := &CVDDataPoint{
-			Timestamp:   baseTime.Add(time.Duration(i) * time.Minute),
-			Price:       100.0 + priceChange,
-			CVD:         observedCVD,
-			PriceChange: priceChange,
-			Volume:      1000000.0, // 1M volume baseline
-		}
-		dataPoints = append(dataPoints, dp)
-	}
-
-	return dataPoints
-}
-
-func generateNoisyCVDData(count int) []*CVDDataPoint {
-	dataPoints := make([]*CVDDataPoint, 0, count)
-	baseTime := time.Now()
-
-	for i := 0; i < count; i++ {
-		// Completely random relationship (no correlation)
-		priceChange := (rand.Float64() - 0.5) * 4.0 // ¬±2.0 price change
-		cvd := 800.0 + rand.Float64()*400.0        // Random CVD 800-1200
-
-		dp := &CVDDataPoint{
-			Timestamp:   baseTime.Add(time.Duration(i) * time.Minute),
-			Price:       100.0 + priceChange,
-			CVD:         cvd,
-			PriceChange: priceChange,
-			Volume:      1000000.0,
-		}
-		dataPoints = append(dataPoints, dp)
-	}
-
-	return dataPoints
-}
-
-// Simple random number generator for reproducible tests
-var rand = &simpleRand{seed: 12345}
-
-type simpleRand struct {
-	seed int64
-}
-
-func (r *simpleRand) Float64() float64 {
-	r.seed = (r.seed*1103515245 + 12345) & 0x7fffffff
-	return float64(r.seed) / float64(0x7fffffff)
-}
\ No newline at end of file
+package premove
+
+import (
+	"context"
+	"math"
+	"testing"
+	"time"
+
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+func TestCVDResidualAnalyzer_AnalyzeCVDResidual_RegressionSuccess(t *testing.T) {
+	analyzer := NewCVDResidualAnalyzer(nil) // Use default config
+
+	// Generate synthetic data with clear price-CVD relationship
+	dataPoints := generateSyntheticCVDData(100, 0.8) // 100 points, 80% R¬≤
+
+	result, err := analyzer.AnalyzeCVDResidual(context.Background(), "BTC-USD", dataPoints)
+	require.NoError(t, err)
+	assert.NotNil(t, result)
+
+	// Should use regression method successfully
+	assert.Equal(t, "regression", result.Method)
+	assert.Empty(t, result.FallbackReason, "Should not fallback with good data")
+	assert.NotNil(t, result.Model)
+
+	// Model should meet quality thresholds
+	assert.GreaterOrEqual(t, result.Model.RSquared, 0.30, "R¬≤ should meet minimum threshold")
+	assert.True(t, result.Model.IsValid, "Model should be valid")
+	assert.Equal(t, 100, result.Model.DataPoints)
+
+	// Should calculate residual metrics
+	assert.NotEqual(t, 0.0, result.RawResidual, "Should have non-zero residual")
+	assert.GreaterOrEqual(t, result.PercentileRank, 0.0)
+	assert.LessOrEqual(t, result.PercentileRank, 100.0)
+	assert.GreaterOrEqual(t, result.SignificanceScore, 0.0)
+	assert.LessOrEqual(t, result.SignificanceScore, 1.0)
+
+	// Should assess data quality
+	assert.NotNil(t, result.DataQuality)
+	assert.Equal(t, 100, result.DataQuality.PointsAvailable)
+	assert.GreaterOrEqual(t, result.DataQuality.PointsUsed, 90) // Should retain most points
+}
+
+func TestCVDResidualAnalyzer_AnalyzeCVDResidual_LowRSquaredFallback(t *testing.T) {
+	analyzer := NewCVDResidualAnalyzer(nil)
+
+	// Generate noisy data with low correlation (should trigger fallback)
+	dataPoints := generateNoisyCVDData(60) // Random relationship
+
+	result, err := analyzer.AnalyzeCVDResidual(context.Background(), "ETH-USD", dataPoints)
+	require.NoError(t, err)
+
+	// Should fallback due to low R¬≤
+	assert.Equal(t, "percentile", result.Method)
+	assert.Contains(t, result.FallbackReason, "Low R¬≤")
+	assert.Nil(t, result.Model) // No valid regression model
+
+	// Should still calculate percentile-based significance
+	assert.GreaterOrEqual(t, result.PercentileRank, 0.0)
+	assert.LessOrEqual(t, result.PercentileRank, 100.0)
+	assert.GreaterOrEqual(t, result.SignificanceScore, 0.0)
+	assert.LessOrEqual(t, result.SignificanceScore, 1.0)
+}
+
+func TestCVDResidualAnalyzer_AnalyzeCVDResidual_InsufficientData(t *testing.T) {
+	analyzer := NewCVDResidualAnalyzer(nil)
+
+	// Generate insufficient data points (below minimum threshold)
+	dataPoints := generateSyntheticCVDData(20, 0.7) // Only 20 points
+
+	result, err := analyzer.AnalyzeCVDResidual(context.Background(), "SOL-USD", dataPoints)
+	require.NoError(t, err)
+
+	// Should fallback due to insufficient data
+	assert.NotEqual(t, "regression", result.Method)
+	assert.Contains(t, result.FallbackReason, "Insufficient data points")
+
+	// Should still provide fallback analysis
+	assert.GreaterOrEqual(t, result.SignificanceScore, 0.0)
+	assert.LessOrEqual(t, result.SignificanceScore, 1.0)
+}
+
+func TestCVDResidualAnalyzer_WinsorizeData_OutlierRemoval(t *testing.T) {
+	analyzer := NewCVDResidualAnalyzer(nil)
+
+	// Generate data with extreme outliers
+	dataPoints := make([]*CVDDataPoint, 0, 100)
+	baseTime := time.Now()
+
+	// Normal data points
+	for i := 0; i < 90; i++ {
+		dp := &CVDDataPoint{
+			Timestamp:   baseTime.Add(time.Duration(i) * time.Minute),
+			Price:       100.0 + float64(i)*0.1,
+			CVD:         1000.0 + float64(i)*10.0,
+			PriceChange: 0.1,
+		}
+		dataPoints = append(dataPoints, dp)
+	}
+
+	// Add extreme outliers
+	for i := 90; i < 100; i++ {
+		dp := &CVDDataPoint{
+			Timestamp:   baseTime.Add(time.Duration(i) * time.Minute),
+			Price:       1000.0,   // Extreme price
+			CVD:         100000.0, // Extreme CVD
+			PriceChange: 50.0,     // Extreme change
+		}
+		dataPoints = append(dataPoints, dp)
+	}
+
+	// Test winsorization
+	filtered, outlierCount := analyzer.winsorizeData(dataPoints)
+
+	assert.Greater(t, outlierCount, 0, "Should remove some outliers")
+	assert.Less(t, len(filtered), len(dataPoints), "Should filter out outliers")
+	assert.GreaterOrEqual(t, len(filtered), 80, "Should retain most normal data")
+
+	// Test with small dataset (should not winsorize)
+	smallData := dataPoints[:5]
+	filteredSmall, outlierCountSmall := analyzer.winsorizeData(smallData)
+	assert.Equal(t, 0, outlierCountSmall, "Should not winsorize small datasets")
+	assert.Equal(t, len(smallData), len(filteredSmall), "Should retain all small dataset points")
+}
+
+func TestCVDResidualAnalyzer_FitRegressionModel(t *testing.T) {
+	analyzer := NewCVDResidualAnalyzer(nil)
+
+	// Generate perfect linear relationship: CVD = 1000 + 100 * PriceChange
+	dataPoints := make([]*CVDDataPoint, 0, 50)
+	baseTime := time.Now()
+
+	for i := 0; i < 50; i++ {
+		priceChange := float64(i-25) * 0.1 // Price changes from -2.5 to +2.4
+		cvd := 1000.0 + 100.0*priceChange  // Perfect linear relationship
+
+		dp := &CVDDataPoint{
+			Timestamp:   baseTime.Add(time.Duration(i) * time.Minute),
+			PriceChange: priceChange,
+			CVD:         cvd,
+		}
+		dataPoints = append(dataPoints, dp)
+	}
+
+	model, err := analyzer.fitRegressionModel("TEST-USD", dataPoints)
+	require.NoError(t, err)
+	assert.NotNil(t, model)
+
+	// Should recover correct coefficients (approximately)
+	assert.InDelta(t, 1000.0, model.Intercept, 1.0, "Should recover correct intercept")
+	assert.InDelta(t, 100.0, model.PriceCoefficient, 1.0, "Should recover correct slope")
+	assert.Greater(t, model.RSquared, 0.99, "Perfect linear relationship should have R¬≤ > 0.99")
+	assert.True(t, model.IsValid)
+}
+
+func TestCVDResidualAnalyzer_PerformFallbackAnalysis_Percentile(t *testing.T) {
+	config := DefaultCVDConfig()
+	config.FallbackMethod = "percentile"
+	config.FallbackLookback = 10
+	config.FallbackThreshold = 80.0
+
+	analyzer := NewCVDResidualAnalyzer(config)
+
+	// Generate data where latest point is extreme
+	dataPoints := make([]*CVDDataPoint, 0, 15)
+	baseTime := time.Now()
+
+	// Normal CVD values
+	for i := 0; i < 14; i++ {
+		dp := &CVDDataPoint{
+			Timestamp: baseTime.Add(time.Duration(i) * time.Minute),
+			CVD:       1000.0 + float64(i)*10.0, // Values 1000-1130
+		}
+		dataPoints = append(dataPoints, dp)
+	}
+
+	// Extreme latest value
+	extremeDP := &CVDDataPoint{
+		Timestamp: baseTime.Add(15 * time.Minute),
+		CVD:       2000.0, // Much higher than others
+	}
+	dataPoints = append(dataPoints, extremeDP)
+
+	result := &CVDResidualResult{
+		Symbol:    "TEST-USD",
+		Timestamp: time.Now(),
+	}
+
+	finalResult := analyzer.performFallbackAnalysis(dataPoints, result)
+
+	assert.Equal(t, "percentile", finalResult.Method)
+	assert.Equal(t, 2000.0, finalResult.RawResidual) // Should use raw CVD
+	assert.GreaterOrEqual(t, finalResult.PercentileRank, 90.0, "Extreme value should have high percentile rank")
+	assert.True(t, finalResult.IsSignificant, "Should be significant above threshold")
+}
+
+func TestCVDResidualAnalyzer_PerformFallbackAnalysis_ZScore(t *testing.T) {
+	config := DefaultCVDConfig()
+	config.FallbackMethod = "zscore"
+	config.FallbackLookback = 10
+	config.ResidualMinStdDev = 2.0
+
+	analyzer := NewCVDResidualAnalyzer(config)
+
+	// Generate data with known mean and std dev
+	dataPoints := make([]*CVDDataPoint, 0, 15)
+	baseTime := time.Now()
+
+	// CVD values with mean=1000, std dev ‚âà 10
+	cvdValues := []float64{980, 985, 990, 995, 1000, 1005, 1010, 1015, 1020}
+	for i, cvd := range cvdValues {
+		dp := &CVDDataPoint{
+			Timestamp: baseTime.Add(time.Duration(i) * time.Minute),
+			CVD:       cvd,
+		}
+		dataPoints = append(dataPoints, dp)
+	}
+
+	// Add extreme latest value (3 std devs from mean)
+	extremeDP := &CVDDataPoint{
+		Timestamp: baseTime.Add(10 * time.Minute),
+		CVD:       1030.0, // ~3 std devs above mean
+	}
+	dataPoints = append(dataPoints, extremeDP)
+
+	result := &CVDResidualResult{
+		Symbol:    "TEST-USD",
+		Timestamp: time.Now(),
+	}
+
+	finalResult := analyzer.performFallbackAnalysis(dataPoints, result)
+
+	assert.Equal(t, "zscore", finalResult.Method)
+	assert.Greater(t, math.Abs(finalResult.NormalizedResidual), 2.0, "Should have >2œÉ z-score")
+	assert.True(t, finalResult.IsSignificant, "Should be significant above threshold")
+}
+
+func TestCVDResidualAnalyzer_CalculateSignificanceScore(t *testing.T) {
+	analyzer := NewCVDResidualAnalyzer(nil)
+
+	// Test extreme significance (high z-score and percentile)
+	score1 := analyzer.calculateSignificanceScore(3.0, 95.0) // 3œÉ, 95th percentile
+	assert.GreaterOrEqual(t, score1, 0.8, "Extreme values should have high significance")
+
+	// Test moderate significance
+	score2 := analyzer.calculateSignificanceScore(1.5, 70.0) // 1.5œÉ, 70th percentile
+	assert.GreaterOrEqual(t, score2, 0.3)
+	assert.LessOrEqual(t, score2, 0.7)
+
+	// Test low significance
+	score3 := analyzer.calculateSignificanceScore(0.5, 55.0) // 0.5œÉ, near median
+	assert.LessOrEqual(t, score3, 0.4, "Low values should have low significance")
+
+	// All scores should be in valid range
+	scores := []float64{score1, score2, score3}
+	for _, score := range scores {
+		assert.GreaterOrEqual(t, score, 0.0, "Significance score should be ‚â• 0")
+		assert.LessOrEqual(t, score, 1.0, "Significance score should be ‚â§ 1")
+	}
+}
+
+func TestCVDResidualAnalyzer_PerformanceRequirements(t *testing.T) {
+	config := DefaultCVDConfig()
+	config.MaxComputeTimeMs = 100 // 100ms limit for testing
+
+	analyzer := NewCVDResidualAnalyzer(config)
+	dataPoints := generateSyntheticCVDData(200, 0.7) // Larger dataset
+
+	start := time.Now()
+	result, err := analyzer.AnalyzeCVDResidual(context.Background(), "PERF-TEST", dataPoints)
+	duration := time.Since(start)
+
+	require.NoError(t, err)
+	assert.NotNil(t, result)
+	assert.Less(t, duration.Milliseconds(), int64(500), "Analysis should complete in <500ms")
+	assert.Greater(t, result.ComputeTimeMs, int64(0), "Should report compute time")
+}
+
+func TestCVDResidualAnalyzer_GetResidualSummary(t *testing.T) {
+	result := &CVDResidualResult{
+		Symbol:         "BTC-USD",
+		RawResidual:    1234.5,
+		PercentileRank: 87.3,
+		Method:         "regression",
+		ComputeTimeMs:  45,
+		IsSignificant:  true,
+	}
+
+	summary := result.GetResidualSummary()
+	assert.Contains(t, summary, "‚ö†Ô∏è  SIGNIFICANT")
+	assert.Contains(t, summary, "BTC-USD")
+	assert.Contains(t, summary, "1234.5")
+	assert.Contains(t, summary, "87.3%")
+	assert.Contains(t, summary, "regression")
+	assert.Contains(t, summary, "45ms")
+}
+
+func TestCVDResidualAnalyzer_GetDetailedAnalysis(t *testing.T) {
+	result := &CVDResidualResult{
+		Symbol:             "ETH-USD",
+		Method:             "regression",
+		IsSignificant:      true,
+		ComputeTimeMs:      67,
+		RawResidual:        -567.8,
+		NormalizedResidual: -2.3,
+		PercentileRank:     15.2,
+		SignificanceScore:  0.82,
+		Model: &CVDRegressionModel{
+			RSquared:         0.78,
+			StandardError:    245.6,
+			DataPoints:       89,
+			Intercept:        1234.5,
+			PriceCoefficient: 123.4,
+		},
+		DataQuality: &CVDDataQuality{
+			PointsAvailable:  100,
+			PointsUsed:       89,
+			WinsorizedPoints: 11,
+			OutliersPct:      11.0,
+			DataSpanHours:    6.5,
+		},
+	}
+
+	analysis := result.GetDetailedAnalysis()
+	assert.Contains(t, analysis, "ETH-USD")
+	assert.Contains(t, analysis, "regression")
+	assert.Contains(t, analysis, "true")
+	assert.Contains(t, analysis, "-567.8")
+	assert.Contains(t, analysis, "-2.3")
+	assert.Contains(t, analysis, "15.2%")
+	assert.Contains(t, analysis, "R¬≤: 0.78")
+	assert.Contains(t, analysis, "CVD = 1234.5 + 123.4 √ó PriceChange")
+	assert.Contains(t, analysis, "100 available, 89 used")
+	assert.Contains(t, analysis, "Winsorized: 11 outliers")
+}
+
+func TestDefaultCVDConfig(t *testing.T) {
+	config := DefaultCVDConfig()
+	require.NotNil(t, config)
+
+	// Check regression parameters
+	assert.Equal(t, 50, config.MinDataPoints)
+	assert.Equal(t, 5.0, config.WinsorizePctLower)
+	assert.Equal(t, 95.0, config.WinsorizePctUpper)
+	assert.Equal(t, 0.30, config.MinRSquared)
+	assert.True(t, config.DailyRefitEnabled)
+
+	// Check fallback parameters
+	assert.Equal(t, "percentile", config.FallbackMethod)
+	assert.Equal(t, 20, config.FallbackLookback)
+	assert.Equal(t, 80.0, config.FallbackThreshold)
+
+	// Check residual analysis parameters
+	assert.Equal(t, 2.0, config.ResidualMinStdDev)
+	assert.Equal(t, int64(3600), config.ResidualMaxAge)
+
+	// Check performance limits
+	assert.Equal(t, int64(200), config.MaxComputeTimeMs)
+}
+
+// Helper functions for test data generation
+
+func generateSyntheticCVDData(count int, rSquared float64) []*CVDDataPoint {
+	dataPoints := make([]*CVDDataPoint, 0, count)
+	baseTime := time.Now()
+
+	// Generate data with controlled R¬≤
+	noiseLevel := math.Sqrt(1.0 - rSquared) // Adjust noise to achieve target R¬≤
+
+	for i := 0; i < count; i++ {
+		priceChange := (float64(i) - float64(count)/2.0) / 10.0 // Price changes around 0
+
+		// True relationship: CVD = 1000 + 50 * priceChange
+		trueCVD := 1000.0 + 50.0*priceChange
+
+		// Add noise to achieve target R¬≤
+		noise := (rand.Float64() - 0.5) * noiseLevel * 200.0 // ¬±100 noise range
+		observedCVD := trueCVD + noise
+
+		dp := &CVDDataPoint{
+			Timestamp:   baseTime.Add(time.Duration(i) * time.Minute),
+			Price:       100.0 + priceChange,
+			CVD:         observedCVD,
+			PriceChange: priceChange,
+			Volume:      1000000.0, // 1M volume baseline
+		}
+		dataPoints = append(dataPoints, dp)
+	}
+
+	return dataPoints
+}
+
+func generateNoisyCVDData(count int) []*CVDDataPoint {
+	dataPoints := make([]*CVDDataPoint, 0, count)
+	baseTime := time.Now()
+
+	for i := 0; i < count; i++ {
+		// Completely random relationship (no correlation)
+		priceChange := (rand.Float64() - 0.5) * 4.0 // ¬±2.0 price change
+		cvd := 800.0 + rand.Float64()*400.0         // Random CVD 800-1200
+
+		dp := &CVDDataPoint{
+			Timestamp:   baseTime.Add(time.Duration(i) * time.Minute),
+			Price:       100.0 + priceChange,
+			CVD:         cvd,
+			PriceChange: priceChange,
+			Volume:      1000000.0,
+		}
+		dataPoints = append(dataPoints, dp)
+	}
+
+	return dataPoints
+}
+
+// Simple random number generator for reproducible tests
+var rand = &simpleRand{seed: 12345}
+
+type simpleRand struct {
+	seed int64
+}
+
+func (r *simpleRand) Float64() float64 {
+	r.seed = (r.seed*1103515245 + 12345) & 0x7fffffff
+	return float64(r.seed) / float64(0x7fffffff)
+}
diff --git a/internal/premove/gates.go b/internal/premove/gates.go
index a94161d..bdaeeb5 100644
--- a/internal/premove/gates.go
+++ b/internal/premove/gates.go
@@ -1,499 +1,499 @@
-package premove
-
-import (
-	"context"
-	"fmt"
-	"math"
-	"sort"
-	"time"
-
-	"cryptorun/internal/microstructure"
-)
-
-// GateEvaluator implements Pre-Movement v3.3 2-of-3 confirmation logic
-type GateEvaluator struct {
-	microEvaluator *microstructure.Evaluator
-	config         *GateConfig
-}
-
-// NewGateEvaluator creates a Pre-Movement v3.3 gate evaluator
-func NewGateEvaluator(microEvaluator *microstructure.Evaluator, config *GateConfig) *GateEvaluator {
-	if config == nil {
-		config = DefaultGateConfig()
-	}
-	return &GateEvaluator{
-		microEvaluator: microEvaluator,
-		config:         config,
-	}
-}
-
-// GateConfig contains thresholds for Pre-Movement v3.3 gate evaluation
-type GateConfig struct {
-	// Core 2-of-3 confirmation gates
-	FundingDivergenceThreshold float64 `yaml:"funding_divergence_threshold"` // ‚â•2.0œÉ z-score
-	SupplySqueezeThreshold     float64 `yaml:"supply_squeeze_threshold"`     // ‚â•0.6 proxy score
-	WhaleCompositeThreshold    float64 `yaml:"whale_composite_threshold"`    // ‚â•0.7 composite
-
-	// Supply squeeze proxy components (2-of-4 required)
-	ReserveDepletionThreshold  float64 `yaml:"reserve_depletion_threshold"`  // ‚â§-5% cross-venue
-	LargeWithdrawalsThreshold  float64 `yaml:"large_withdrawals_threshold"`  // ‚â•$50M/24h
-	StakingInflowThreshold     float64 `yaml:"staking_inflow_threshold"`     // ‚â•$10M/24h
-	DerivativesLeverageThreshold float64 `yaml:"derivatives_leverage_threshold"` // ‚â•15% OI increase
-
-	// Volume confirmation additive (risk_off/btc_driven regimes)
-	VolumeConfirmationEnabled  bool    `yaml:"volume_confirmation_enabled"`  // Enable additive volume gate
-	VolumeConfirmationThreshold float64 `yaml:"volume_confirmation_threshold"` // ‚â•2.5√ó average
-
-	// Precedence weights (for tie-breaking when >2 gates pass)
-	FundingPrecedence float64 `yaml:"funding_precedence"` // 3.0 (highest priority)
-	WhalePrecedence   float64 `yaml:"whale_precedence"`   // 2.0 (medium priority)
-	SupplyPrecedence  float64 `yaml:"supply_precedence"`  // 1.0 (lowest priority)
-
-	// Gate evaluation timeouts
-	MaxEvaluationTimeMs int64 `yaml:"max_evaluation_time_ms"` // 500ms timeout
-}
-
-// DefaultGateConfig returns Pre-Movement v3.3 production gate configuration
-func DefaultGateConfig() *GateConfig {
-	return &GateConfig{
-		// Core 2-of-3 thresholds
-		FundingDivergenceThreshold: 2.0, // 2.0œÉ
-		SupplySqueezeThreshold:     0.6, // 60% proxy confidence
-		WhaleCompositeThreshold:    0.7, // 70% whale activity
-
-		// Supply squeeze components (2-of-4)
-		ReserveDepletionThreshold:    -5.0, // -5% reserves
-		LargeWithdrawalsThreshold:    50e6, // $50M withdrawals
-		StakingInflowThreshold:       10e6, // $10M staking
-		DerivativesLeverageThreshold: 15.0, // 15% OI increase
-
-		// Volume confirmation
-		VolumeConfirmationEnabled:   true,
-		VolumeConfirmationThreshold: 2.5, // 2.5√ó volume
-
-		// Precedence weights
-		FundingPrecedence: 3.0, // Funding has highest precedence
-		WhalePrecedence:   2.0, // Whale activity medium precedence  
-		SupplyPrecedence:  1.0, // Supply squeeze lowest precedence
-
-		// Performance limits
-		MaxEvaluationTimeMs: 500, // 500ms timeout
-	}
-}
-
-// ConfirmationData contains all inputs for Pre-Movement v3.3 gate evaluation
-type ConfirmationData struct {
-	Symbol    string    `json:"symbol"`
-	Timestamp time.Time `json:"timestamp"`
-
-	// Core confirmation signals
-	FundingZScore    float64 `json:"funding_z_score"`    // Cross-venue funding z-score
-	WhaleComposite   float64 `json:"whale_composite"`    // Whale activity composite 0-1
-	SupplyProxyScore float64 `json:"supply_proxy_score"` // Supply squeeze proxy 0-1
-
-	// Supply squeeze components
-	ReserveChange7d      float64 `json:"reserve_change_7d"`      // % change in exchange reserves
-	LargeWithdrawals24h  float64 `json:"large_withdrawals_24h"`  // $ large withdrawals 24h
-	StakingInflow24h     float64 `json:"staking_inflow_24h"`     // $ staking inflows 24h  
-	DerivativesOIChange  float64 `json:"derivatives_oi_change"`  // % OI change 24h
-
-	// Volume confirmation (regime-dependent)
-	VolumeRatio24h float64 `json:"volume_ratio_24h"` // Current/average volume ratio
-	CurrentRegime  string  `json:"current_regime"`   // "risk_off", "btc_driven", "normal"
-
-	// Microstructure context
-	SpreadBps   float64 `json:"spread_bps"`   // Current spread basis points
-	DepthUSD    float64 `json:"depth_usd"`    // Available depth USD
-	VADR        float64 `json:"vadr"`         // Volume-adjusted daily range
-}
-
-// ConfirmationResult contains the complete gate evaluation result
-type ConfirmationResult struct {
-	Symbol           string                 `json:"symbol"`
-	Timestamp        time.Time              `json:"timestamp"`
-	Passed           bool                   `json:"passed"`           // 2-of-3 + microstructure passed
-	ConfirmationCount int                   `json:"confirmation_count"` // Number of core gates passed
-	RequiredCount     int                   `json:"required_count"`   // Required gates (2 or 3)
-	PassedGates       []string              `json:"passed_gates"`     // Names of passed gates
-	FailedGates       []string              `json:"failed_gates"`     // Names of failed gates
-	GateResults       map[string]*GateCheck `json:"gate_results"`     // Detailed gate results
-	PrecedenceScore   float64               `json:"precedence_score"` // Weighted precedence for ranking
-	VolumeBoost       bool                  `json:"volume_boost"`     // Volume confirmation applied
-	SupplyBreakdown   *SupplySqueezeBreakdown `json:"supply_breakdown"` // Supply proxy component details
-	MicroReport       *microstructure.EvaluationResult `json:"micro_report"` // Microstructure evaluation
-	EvaluationTimeMs  int64                 `json:"evaluation_time_ms"`
-	Warnings          []string              `json:"warnings"`
-}
-
-// SupplySqueezeBreakdown shows which supply components triggered
-type SupplySqueezeBreakdown struct {
-	ProxyScore         float64             `json:"proxy_score"`         // Final 0-1 proxy score
-	ComponentResults   map[string]*GateCheck `json:"component_results"`   // Individual 2-of-4 results
-	PassedComponents   []string            `json:"passed_components"`   // Names of passed components
-	ComponentCount     int                 `json:"component_count"`     // Number passed (need ‚â•2)
-	RequiredComponents int                 `json:"required_components"` // Required components (2)
-}
-
-// GateCheck represents individual gate evaluation result (reused from existing gates package)
-type GateCheck struct {
-	Name        string      `json:"name"`
-	Passed      bool        `json:"passed"`
-	Value       interface{} `json:"value"`
-	Threshold   interface{} `json:"threshold"`
-	Description string      `json:"description"`
-}
-
-// EvaluateConfirmation performs comprehensive Pre-Movement v3.3 gate evaluation
-func (ge *GateEvaluator) EvaluateConfirmation(ctx context.Context, data *ConfirmationData) (*ConfirmationResult, error) {
-	startTime := time.Now()
-
-	result := &ConfirmationResult{
-		Symbol:          data.Symbol,
-		Timestamp:       time.Now(),
-		RequiredCount:   2, // Base requirement: 2-of-3 
-		GateResults:     make(map[string]*GateCheck),
-		PassedGates:     []string{},
-		FailedGates:     []string{},
-		Warnings:        []string{},
-	}
-
-	// Evaluate core 2-of-3 confirmation gates
-	ge.evaluateFundingDivergence(data, result)
-	ge.evaluateWhaleComposite(data, result)
-	ge.evaluateSupplySqueezeProxy(data, result)
-
-	// Check volume confirmation boost for specific regimes
-	if ge.config.VolumeConfirmationEnabled {
-		ge.evaluateVolumeConfirmation(data, result)
-	}
-
-	// Calculate final confirmation status
-	result.ConfirmationCount = len(result.PassedGates)
-	
-	// Adjust required count for volume boost in specific regimes
-	requiredCount := 2
-	if result.VolumeBoost && (data.CurrentRegime == "risk_off" || data.CurrentRegime == "btc_driven") {
-		requiredCount = 1 // Volume boost reduces requirement to 1-of-3 + volume
-		result.RequiredCount = requiredCount
-	}
-
-	coreConfirmationPassed := result.ConfirmationCount >= requiredCount
-
-	// Evaluate microstructure as consultation (not blocking for Pre-Movement)
-	microResult, err := ge.microEvaluator.EvaluateSnapshot(ctx, data.Symbol)
-	if err != nil {
-		result.Warnings = append(result.Warnings, fmt.Sprintf("Microstructure evaluation failed: %v", err))
-	} else {
-		result.MicroReport = microResult
-	}
-
-	// Overall pass/fail: 2-of-3 confirmations (microstructure is consultative)
-	result.Passed = coreConfirmationPassed
-	
-	// Calculate precedence score for ranking multiple candidates
-	result.PrecedenceScore = ge.calculatePrecedenceScore(result)
-
-	result.EvaluationTimeMs = time.Since(startTime).Milliseconds()
-
-	// Performance warning if evaluation took too long
-	if result.EvaluationTimeMs > ge.config.MaxEvaluationTimeMs {
-		result.Warnings = append(result.Warnings, 
-			fmt.Sprintf("Gate evaluation took %dms (>%dms threshold)", 
-				result.EvaluationTimeMs, ge.config.MaxEvaluationTimeMs))
-	}
-
-	return result, nil
-}
-
-// evaluateFundingDivergence checks cross-venue funding rate divergence
-func (ge *GateEvaluator) evaluateFundingDivergence(data *ConfirmationData, result *ConfirmationResult) {
-	gate := &GateCheck{
-		Name:        "funding_divergence",
-		Value:       data.FundingZScore,
-		Threshold:   ge.config.FundingDivergenceThreshold,
-		Description: fmt.Sprintf("Funding z-score %.2f ‚â• %.2f", data.FundingZScore, ge.config.FundingDivergenceThreshold),
-	}
-	
-	gate.Passed = data.FundingZScore >= ge.config.FundingDivergenceThreshold
-	result.GateResults["funding_divergence"] = gate
-
-	if gate.Passed {
-		result.PassedGates = append(result.PassedGates, "funding_divergence")
-	} else {
-		result.FailedGates = append(result.FailedGates, "funding_divergence")
-	}
-}
-
-// evaluateWhaleComposite checks whale activity patterns
-func (ge *GateEvaluator) evaluateWhaleComposite(data *ConfirmationData, result *ConfirmationResult) {
-	gate := &GateCheck{
-		Name:        "whale_composite",
-		Value:       data.WhaleComposite,
-		Threshold:   ge.config.WhaleCompositeThreshold,
-		Description: fmt.Sprintf("Whale composite %.2f ‚â• %.2f", data.WhaleComposite, ge.config.WhaleCompositeThreshold),
-	}
-	
-	gate.Passed = data.WhaleComposite >= ge.config.WhaleCompositeThreshold
-	result.GateResults["whale_composite"] = gate
-
-	if gate.Passed {
-		result.PassedGates = append(result.PassedGates, "whale_composite")
-	} else {
-		result.FailedGates = append(result.FailedGates, "whale_composite")
-	}
-}
-
-// evaluateSupplySqueezeProxy checks supply squeeze using 2-of-4 component logic
-func (ge *GateEvaluator) evaluateSupplySqueezeProxy(data *ConfirmationData, result *ConfirmationResult) {
-	breakdown := &SupplySqueezeBreakdown{
-		ComponentResults:   make(map[string]*GateCheck),
-		PassedComponents:   []string{},
-		RequiredComponents: 2, // Need 2-of-4 components
-	}
-
-	// Component 1: Reserve depletion
-	reserveGate := &GateCheck{
-		Name:        "reserve_depletion",
-		Value:       data.ReserveChange7d,
-		Threshold:   ge.config.ReserveDepletionThreshold,
-		Description: fmt.Sprintf("Reserve change %.1f%% ‚â§ %.1f%%", data.ReserveChange7d, ge.config.ReserveDepletionThreshold),
-		Passed:      data.ReserveChange7d <= ge.config.ReserveDepletionThreshold,
-	}
-	breakdown.ComponentResults["reserve_depletion"] = reserveGate
-
-	// Component 2: Large withdrawals
-	withdrawalGate := &GateCheck{
-		Name:        "large_withdrawals",
-		Value:       data.LargeWithdrawals24h,
-		Threshold:   ge.config.LargeWithdrawalsThreshold,
-		Description: fmt.Sprintf("Withdrawals $%.1fM ‚â• $%.1fM", data.LargeWithdrawals24h/1e6, ge.config.LargeWithdrawalsThreshold/1e6),
-		Passed:      data.LargeWithdrawals24h >= ge.config.LargeWithdrawalsThreshold,
-	}
-	breakdown.ComponentResults["large_withdrawals"] = withdrawalGate
-
-	// Component 3: Staking inflows
-	stakingGate := &GateCheck{
-		Name:        "staking_inflow",
-		Value:       data.StakingInflow24h,
-		Threshold:   ge.config.StakingInflowThreshold,
-		Description: fmt.Sprintf("Staking $%.1fM ‚â• $%.1fM", data.StakingInflow24h/1e6, ge.config.StakingInflowThreshold/1e6),
-		Passed:      data.StakingInflow24h >= ge.config.StakingInflowThreshold,
-	}
-	breakdown.ComponentResults["staking_inflow"] = stakingGate
-
-	// Component 4: Derivatives leverage
-	derivsGate := &GateCheck{
-		Name:        "derivatives_oi",
-		Value:       data.DerivativesOIChange,
-		Threshold:   ge.config.DerivativesLeverageThreshold,
-		Description: fmt.Sprintf("OI change %.1f%% ‚â• %.1f%%", data.DerivativesOIChange, ge.config.DerivativesLeverageThreshold),
-		Passed:      data.DerivativesOIChange >= ge.config.DerivativesLeverageThreshold,
-	}
-	breakdown.ComponentResults["derivatives_oi"] = derivsGate
-
-	// Count passed components
-	for name, gate := range breakdown.ComponentResults {
-		if gate.Passed {
-			breakdown.PassedComponents = append(breakdown.PassedComponents, name)
-		}
-	}
-	breakdown.ComponentCount = len(breakdown.PassedComponents)
-
-	// Calculate proxy score based on component strength
-	breakdown.ProxyScore = ge.calculateSupplyProxyScore(data, breakdown)
-
-	// Main supply squeeze gate passes if proxy score meets threshold
-	supplyGate := &GateCheck{
-		Name:        "supply_squeeze",
-		Value:       breakdown.ProxyScore,
-		Threshold:   ge.config.SupplySqueezeThreshold,
-		Description: fmt.Sprintf("Supply proxy %.2f ‚â• %.2f (%d/4 components)", 
-			breakdown.ProxyScore, ge.config.SupplySqueezeThreshold, breakdown.ComponentCount),
-		Passed:      breakdown.ProxyScore >= ge.config.SupplySqueezeThreshold,
-	}
-
-	result.GateResults["supply_squeeze"] = supplyGate
-	result.SupplyBreakdown = breakdown
-
-	if supplyGate.Passed {
-		result.PassedGates = append(result.PassedGates, "supply_squeeze")
-	} else {
-		result.FailedGates = append(result.FailedGates, "supply_squeeze")
-	}
-}
-
-// evaluateVolumeConfirmation checks additive volume confirmation for specific regimes
-func (ge *GateEvaluator) evaluateVolumeConfirmation(data *ConfirmationData, result *ConfirmationResult) {
-	// Only apply volume boost in risk_off or btc_driven regimes
-	if data.CurrentRegime != "risk_off" && data.CurrentRegime != "btc_driven" {
-		return
-	}
-
-	volumeGate := &GateCheck{
-		Name:        "volume_confirmation",
-		Value:       data.VolumeRatio24h,
-		Threshold:   ge.config.VolumeConfirmationThreshold,
-		Description: fmt.Sprintf("Volume ratio %.2f√ó ‚â• %.2f√ó (regime: %s)", 
-			data.VolumeRatio24h, ge.config.VolumeConfirmationThreshold, data.CurrentRegime),
-		Passed:      data.VolumeRatio24h >= ge.config.VolumeConfirmationThreshold,
-	}
-
-	result.GateResults["volume_confirmation"] = volumeGate
-	result.VolumeBoost = volumeGate.Passed
-
-	if volumeGate.Passed {
-		result.PassedGates = append(result.PassedGates, "volume_confirmation")
-	}
-}
-
-// calculateSupplyProxyScore computes weighted supply squeeze proxy score from components
-func (ge *GateEvaluator) calculateSupplyProxyScore(data *ConfirmationData, breakdown *SupplySqueezeBreakdown) float64 {
-	var score float64
-	components := 0
-
-	// Reserve depletion contribution (0-0.3)
-	if breakdown.ComponentResults["reserve_depletion"].Passed {
-		reserveStrength := math.Min(1.0, math.Abs(data.ReserveChange7d)/20.0) // -20% = max strength
-		score += reserveStrength * 0.3
-		components++
-	}
-
-	// Large withdrawals contribution (0-0.25)
-	if breakdown.ComponentResults["large_withdrawals"].Passed {
-		withdrawalStrength := math.Min(1.0, data.LargeWithdrawals24h/100e6) // $100M = max strength
-		score += withdrawalStrength * 0.25
-		components++
-	}
-
-	// Staking inflows contribution (0-0.2)
-	if breakdown.ComponentResults["staking_inflow"].Passed {
-		stakingStrength := math.Min(1.0, data.StakingInflow24h/25e6) // $25M = max strength
-		score += stakingStrength * 0.2
-		components++
-	}
-
-	// Derivatives leverage contribution (0-0.25)
-	if breakdown.ComponentResults["derivatives_oi"].Passed {
-		oiStrength := math.Min(1.0, data.DerivativesOIChange/50.0) // 50% = max strength
-		score += oiStrength * 0.25
-		components++
-	}
-
-	// Require at least 2 components for valid proxy score
-	if components < 2 {
-		return 0.0
-	}
-
-	return score
-}
-
-// calculatePrecedenceScore computes weighted precedence for ranking candidates
-func (ge *GateEvaluator) calculatePrecedenceScore(result *ConfirmationResult) float64 {
-	var score float64
-
-	// Weight passed gates by precedence
-	for _, gateName := range result.PassedGates {
-		switch gateName {
-		case "funding_divergence":
-			score += ge.config.FundingPrecedence
-		case "whale_composite":
-			score += ge.config.WhalePrecedence  
-		case "supply_squeeze":
-			score += ge.config.SupplyPrecedence
-		case "volume_confirmation":
-			score += 0.5 // Additive boost precedence
-		}
-	}
-
-	return score
-}
-
-// GetConfirmationSummary returns a concise summary of gate evaluation
-func (cr *ConfirmationResult) GetConfirmationSummary() string {
-	status := "‚ùå BLOCKED"
-	if cr.Passed {
-		status = "‚úÖ CONFIRMED"
-	}
-
-	volumeNote := ""
-	if cr.VolumeBoost {
-		volumeNote = " +VOL"
-	}
-
-	return fmt.Sprintf("%s ‚Äî %s (%d/%d gates%s, %.1f precedence, %dms)",
-		status, cr.Symbol, cr.ConfirmationCount, cr.RequiredCount, 
-		volumeNote, cr.PrecedenceScore, cr.EvaluationTimeMs)
-}
-
-// GetDetailedReport returns comprehensive confirmation gate report
-func (cr *ConfirmationResult) GetDetailedReport() string {
-	report := fmt.Sprintf("Pre-Movement v3.3 Confirmation: %s\n", cr.Symbol)
-	report += fmt.Sprintf("Status: %s | Gates: %d/%d | Precedence: %.1f | Time: %dms\n\n",
-		map[bool]string{true: "CONFIRMED ‚úÖ", false: "BLOCKED ‚ùå"}[cr.Passed],
-		cr.ConfirmationCount, cr.RequiredCount, cr.PrecedenceScore, cr.EvaluationTimeMs)
-
-	// Core gate results
-	report += "Core Gates (2-of-3 required):\n"
-	coreGates := []string{"funding_divergence", "whale_composite", "supply_squeeze"}
-	
-	for _, gateName := range coreGates {
-		if gate, exists := cr.GateResults[gateName]; exists {
-			status := map[bool]string{true: "‚úÖ", false: "‚ùå"}[gate.Passed]
-			report += fmt.Sprintf("  %s %s: %s\n", status, gateName, gate.Description)
-		}
-	}
-
-	// Supply squeeze breakdown
-	if cr.SupplyBreakdown != nil {
-		report += fmt.Sprintf("\nSupply Squeeze Components (%d/4 passed, need ‚â•2):\n", cr.SupplyBreakdown.ComponentCount)
-		componentOrder := []string{"reserve_depletion", "large_withdrawals", "staking_inflow", "derivatives_oi"}
-		
-		for _, compName := range componentOrder {
-			if comp, exists := cr.SupplyBreakdown.ComponentResults[compName]; exists {
-				status := map[bool]string{true: "‚úÖ", false: "‚ùå"}[comp.Passed]
-				report += fmt.Sprintf("    %s %s: %s\n", status, compName, comp.Description)
-			}
-		}
-	}
-
-	// Volume confirmation
-	if cr.VolumeBoost {
-		if gate, exists := cr.GateResults["volume_confirmation"]; exists {
-			report += fmt.Sprintf("\nVolume Confirmation: ‚úÖ %s\n", gate.Description)
-		}
-	}
-
-	// Microstructure consultation
-	if cr.MicroReport != nil {
-		report += fmt.Sprintf("\nMicrostructure Consultation:\n")
-		report += fmt.Sprintf("  Spread: %.1f bps | Depth: $%.0f | VADR: %.2f√ó\n",
-			cr.MicroReport.SpreadBps, cr.MicroReport.DepthUSD, cr.MicroReport.VADR)
-	}
-
-	// Warnings
-	if len(cr.Warnings) > 0 {
-		report += fmt.Sprintf("\nWarnings:\n")
-		for i, warning := range cr.Warnings {
-			report += fmt.Sprintf("  %d. %s\n", i+1, warning)
-		}
-	}
-
-	return report
-}
-
-// RankCandidates sorts confirmation results by precedence score (highest first)
-func RankCandidates(results []*ConfirmationResult) []*ConfirmationResult {
-	ranked := make([]*ConfirmationResult, len(results))
-	copy(ranked, results)
-	
-	sort.Slice(ranked, func(i, j int) bool {
-		// First sort by pass/fail status
-		if ranked[i].Passed != ranked[j].Passed {
-			return ranked[i].Passed // Passed results come first
-		}
-		// Then sort by precedence score (higher first)
-		return ranked[i].PrecedenceScore > ranked[j].PrecedenceScore
-	})
-	
-	return ranked
-}
\ No newline at end of file
+package premove
+
+import (
+	"context"
+	"fmt"
+	"math"
+	"sort"
+	"time"
+
+	"cryptorun/internal/microstructure"
+)
+
+// GateEvaluator implements Pre-Movement v3.3 2-of-3 confirmation logic
+type GateEvaluator struct {
+	microEvaluator *microstructure.Evaluator
+	config         *GateConfig
+}
+
+// NewGateEvaluator creates a Pre-Movement v3.3 gate evaluator
+func NewGateEvaluator(microEvaluator *microstructure.Evaluator, config *GateConfig) *GateEvaluator {
+	if config == nil {
+		config = DefaultGateConfig()
+	}
+	return &GateEvaluator{
+		microEvaluator: microEvaluator,
+		config:         config,
+	}
+}
+
+// GateConfig contains thresholds for Pre-Movement v3.3 gate evaluation
+type GateConfig struct {
+	// Core 2-of-3 confirmation gates
+	FundingDivergenceThreshold float64 `yaml:"funding_divergence_threshold"` // ‚â•2.0œÉ z-score
+	SupplySqueezeThreshold     float64 `yaml:"supply_squeeze_threshold"`     // ‚â•0.6 proxy score
+	WhaleCompositeThreshold    float64 `yaml:"whale_composite_threshold"`    // ‚â•0.7 composite
+
+	// Supply squeeze proxy components (2-of-4 required)
+	ReserveDepletionThreshold    float64 `yaml:"reserve_depletion_threshold"`    // ‚â§-5% cross-venue
+	LargeWithdrawalsThreshold    float64 `yaml:"large_withdrawals_threshold"`    // ‚â•$50M/24h
+	StakingInflowThreshold       float64 `yaml:"staking_inflow_threshold"`       // ‚â•$10M/24h
+	DerivativesLeverageThreshold float64 `yaml:"derivatives_leverage_threshold"` // ‚â•15% OI increase
+
+	// Volume confirmation additive (risk_off/btc_driven regimes)
+	VolumeConfirmationEnabled   bool    `yaml:"volume_confirmation_enabled"`   // Enable additive volume gate
+	VolumeConfirmationThreshold float64 `yaml:"volume_confirmation_threshold"` // ‚â•2.5√ó average
+
+	// Precedence weights (for tie-breaking when >2 gates pass)
+	FundingPrecedence float64 `yaml:"funding_precedence"` // 3.0 (highest priority)
+	WhalePrecedence   float64 `yaml:"whale_precedence"`   // 2.0 (medium priority)
+	SupplyPrecedence  float64 `yaml:"supply_precedence"`  // 1.0 (lowest priority)
+
+	// Gate evaluation timeouts
+	MaxEvaluationTimeMs int64 `yaml:"max_evaluation_time_ms"` // 500ms timeout
+}
+
+// DefaultGateConfig returns Pre-Movement v3.3 production gate configuration
+func DefaultGateConfig() *GateConfig {
+	return &GateConfig{
+		// Core 2-of-3 thresholds
+		FundingDivergenceThreshold: 2.0, // 2.0œÉ
+		SupplySqueezeThreshold:     0.6, // 60% proxy confidence
+		WhaleCompositeThreshold:    0.7, // 70% whale activity
+
+		// Supply squeeze components (2-of-4)
+		ReserveDepletionThreshold:    -5.0, // -5% reserves
+		LargeWithdrawalsThreshold:    50e6, // $50M withdrawals
+		StakingInflowThreshold:       10e6, // $10M staking
+		DerivativesLeverageThreshold: 15.0, // 15% OI increase
+
+		// Volume confirmation
+		VolumeConfirmationEnabled:   true,
+		VolumeConfirmationThreshold: 2.5, // 2.5√ó volume
+
+		// Precedence weights
+		FundingPrecedence: 3.0, // Funding has highest precedence
+		WhalePrecedence:   2.0, // Whale activity medium precedence
+		SupplyPrecedence:  1.0, // Supply squeeze lowest precedence
+
+		// Performance limits
+		MaxEvaluationTimeMs: 500, // 500ms timeout
+	}
+}
+
+// ConfirmationData contains all inputs for Pre-Movement v3.3 gate evaluation
+type ConfirmationData struct {
+	Symbol    string    `json:"symbol"`
+	Timestamp time.Time `json:"timestamp"`
+
+	// Core confirmation signals
+	FundingZScore    float64 `json:"funding_z_score"`    // Cross-venue funding z-score
+	WhaleComposite   float64 `json:"whale_composite"`    // Whale activity composite 0-1
+	SupplyProxyScore float64 `json:"supply_proxy_score"` // Supply squeeze proxy 0-1
+
+	// Supply squeeze components
+	ReserveChange7d     float64 `json:"reserve_change_7d"`     // % change in exchange reserves
+	LargeWithdrawals24h float64 `json:"large_withdrawals_24h"` // $ large withdrawals 24h
+	StakingInflow24h    float64 `json:"staking_inflow_24h"`    // $ staking inflows 24h
+	DerivativesOIChange float64 `json:"derivatives_oi_change"` // % OI change 24h
+
+	// Volume confirmation (regime-dependent)
+	VolumeRatio24h float64 `json:"volume_ratio_24h"` // Current/average volume ratio
+	CurrentRegime  string  `json:"current_regime"`   // "risk_off", "btc_driven", "normal"
+
+	// Microstructure context
+	SpreadBps float64 `json:"spread_bps"` // Current spread basis points
+	DepthUSD  float64 `json:"depth_usd"`  // Available depth USD
+	VADR      float64 `json:"vadr"`       // Volume-adjusted daily range
+}
+
+// ConfirmationResult contains the complete gate evaluation result
+type ConfirmationResult struct {
+	Symbol            string                           `json:"symbol"`
+	Timestamp         time.Time                        `json:"timestamp"`
+	Passed            bool                             `json:"passed"`             // 2-of-3 + microstructure passed
+	ConfirmationCount int                              `json:"confirmation_count"` // Number of core gates passed
+	RequiredCount     int                              `json:"required_count"`     // Required gates (2 or 3)
+	PassedGates       []string                         `json:"passed_gates"`       // Names of passed gates
+	FailedGates       []string                         `json:"failed_gates"`       // Names of failed gates
+	GateResults       map[string]*GateCheck            `json:"gate_results"`       // Detailed gate results
+	PrecedenceScore   float64                          `json:"precedence_score"`   // Weighted precedence for ranking
+	VolumeBoost       bool                             `json:"volume_boost"`       // Volume confirmation applied
+	SupplyBreakdown   *SupplySqueezeBreakdown          `json:"supply_breakdown"`   // Supply proxy component details
+	MicroReport       *microstructure.EvaluationResult `json:"micro_report"`       // Microstructure evaluation
+	EvaluationTimeMs  int64                            `json:"evaluation_time_ms"`
+	Warnings          []string                         `json:"warnings"`
+}
+
+// SupplySqueezeBreakdown shows which supply components triggered
+type SupplySqueezeBreakdown struct {
+	ProxyScore         float64               `json:"proxy_score"`         // Final 0-1 proxy score
+	ComponentResults   map[string]*GateCheck `json:"component_results"`   // Individual 2-of-4 results
+	PassedComponents   []string              `json:"passed_components"`   // Names of passed components
+	ComponentCount     int                   `json:"component_count"`     // Number passed (need ‚â•2)
+	RequiredComponents int                   `json:"required_components"` // Required components (2)
+}
+
+// GateCheck represents individual gate evaluation result (reused from existing gates package)
+type GateCheck struct {
+	Name        string      `json:"name"`
+	Passed      bool        `json:"passed"`
+	Value       interface{} `json:"value"`
+	Threshold   interface{} `json:"threshold"`
+	Description string      `json:"description"`
+}
+
+// EvaluateConfirmation performs comprehensive Pre-Movement v3.3 gate evaluation
+func (ge *GateEvaluator) EvaluateConfirmation(ctx context.Context, data *ConfirmationData) (*ConfirmationResult, error) {
+	startTime := time.Now()
+
+	result := &ConfirmationResult{
+		Symbol:        data.Symbol,
+		Timestamp:     time.Now(),
+		RequiredCount: 2, // Base requirement: 2-of-3
+		GateResults:   make(map[string]*GateCheck),
+		PassedGates:   []string{},
+		FailedGates:   []string{},
+		Warnings:      []string{},
+	}
+
+	// Evaluate core 2-of-3 confirmation gates
+	ge.evaluateFundingDivergence(data, result)
+	ge.evaluateWhaleComposite(data, result)
+	ge.evaluateSupplySqueezeProxy(data, result)
+
+	// Check volume confirmation boost for specific regimes
+	if ge.config.VolumeConfirmationEnabled {
+		ge.evaluateVolumeConfirmation(data, result)
+	}
+
+	// Calculate final confirmation status
+	result.ConfirmationCount = len(result.PassedGates)
+
+	// Adjust required count for volume boost in specific regimes
+	requiredCount := 2
+	if result.VolumeBoost && (data.CurrentRegime == "risk_off" || data.CurrentRegime == "btc_driven") {
+		requiredCount = 1 // Volume boost reduces requirement to 1-of-3 + volume
+		result.RequiredCount = requiredCount
+	}
+
+	coreConfirmationPassed := result.ConfirmationCount >= requiredCount
+
+	// Evaluate microstructure as consultation (not blocking for Pre-Movement)
+	microResult, err := ge.microEvaluator.EvaluateSnapshot(ctx, data.Symbol)
+	if err != nil {
+		result.Warnings = append(result.Warnings, fmt.Sprintf("Microstructure evaluation failed: %v", err))
+	} else {
+		result.MicroReport = microResult
+	}
+
+	// Overall pass/fail: 2-of-3 confirmations (microstructure is consultative)
+	result.Passed = coreConfirmationPassed
+
+	// Calculate precedence score for ranking multiple candidates
+	result.PrecedenceScore = ge.calculatePrecedenceScore(result)
+
+	result.EvaluationTimeMs = time.Since(startTime).Milliseconds()
+
+	// Performance warning if evaluation took too long
+	if result.EvaluationTimeMs > ge.config.MaxEvaluationTimeMs {
+		result.Warnings = append(result.Warnings,
+			fmt.Sprintf("Gate evaluation took %dms (>%dms threshold)",
+				result.EvaluationTimeMs, ge.config.MaxEvaluationTimeMs))
+	}
+
+	return result, nil
+}
+
+// evaluateFundingDivergence checks cross-venue funding rate divergence
+func (ge *GateEvaluator) evaluateFundingDivergence(data *ConfirmationData, result *ConfirmationResult) {
+	gate := &GateCheck{
+		Name:        "funding_divergence",
+		Value:       data.FundingZScore,
+		Threshold:   ge.config.FundingDivergenceThreshold,
+		Description: fmt.Sprintf("Funding z-score %.2f ‚â• %.2f", data.FundingZScore, ge.config.FundingDivergenceThreshold),
+	}
+
+	gate.Passed = data.FundingZScore >= ge.config.FundingDivergenceThreshold
+	result.GateResults["funding_divergence"] = gate
+
+	if gate.Passed {
+		result.PassedGates = append(result.PassedGates, "funding_divergence")
+	} else {
+		result.FailedGates = append(result.FailedGates, "funding_divergence")
+	}
+}
+
+// evaluateWhaleComposite checks whale activity patterns
+func (ge *GateEvaluator) evaluateWhaleComposite(data *ConfirmationData, result *ConfirmationResult) {
+	gate := &GateCheck{
+		Name:        "whale_composite",
+		Value:       data.WhaleComposite,
+		Threshold:   ge.config.WhaleCompositeThreshold,
+		Description: fmt.Sprintf("Whale composite %.2f ‚â• %.2f", data.WhaleComposite, ge.config.WhaleCompositeThreshold),
+	}
+
+	gate.Passed = data.WhaleComposite >= ge.config.WhaleCompositeThreshold
+	result.GateResults["whale_composite"] = gate
+
+	if gate.Passed {
+		result.PassedGates = append(result.PassedGates, "whale_composite")
+	} else {
+		result.FailedGates = append(result.FailedGates, "whale_composite")
+	}
+}
+
+// evaluateSupplySqueezeProxy checks supply squeeze using 2-of-4 component logic
+func (ge *GateEvaluator) evaluateSupplySqueezeProxy(data *ConfirmationData, result *ConfirmationResult) {
+	breakdown := &SupplySqueezeBreakdown{
+		ComponentResults:   make(map[string]*GateCheck),
+		PassedComponents:   []string{},
+		RequiredComponents: 2, // Need 2-of-4 components
+	}
+
+	// Component 1: Reserve depletion
+	reserveGate := &GateCheck{
+		Name:        "reserve_depletion",
+		Value:       data.ReserveChange7d,
+		Threshold:   ge.config.ReserveDepletionThreshold,
+		Description: fmt.Sprintf("Reserve change %.1f%% ‚â§ %.1f%%", data.ReserveChange7d, ge.config.ReserveDepletionThreshold),
+		Passed:      data.ReserveChange7d <= ge.config.ReserveDepletionThreshold,
+	}
+	breakdown.ComponentResults["reserve_depletion"] = reserveGate
+
+	// Component 2: Large withdrawals
+	withdrawalGate := &GateCheck{
+		Name:        "large_withdrawals",
+		Value:       data.LargeWithdrawals24h,
+		Threshold:   ge.config.LargeWithdrawalsThreshold,
+		Description: fmt.Sprintf("Withdrawals $%.1fM ‚â• $%.1fM", data.LargeWithdrawals24h/1e6, ge.config.LargeWithdrawalsThreshold/1e6),
+		Passed:      data.LargeWithdrawals24h >= ge.config.LargeWithdrawalsThreshold,
+	}
+	breakdown.ComponentResults["large_withdrawals"] = withdrawalGate
+
+	// Component 3: Staking inflows
+	stakingGate := &GateCheck{
+		Name:        "staking_inflow",
+		Value:       data.StakingInflow24h,
+		Threshold:   ge.config.StakingInflowThreshold,
+		Description: fmt.Sprintf("Staking $%.1fM ‚â• $%.1fM", data.StakingInflow24h/1e6, ge.config.StakingInflowThreshold/1e6),
+		Passed:      data.StakingInflow24h >= ge.config.StakingInflowThreshold,
+	}
+	breakdown.ComponentResults["staking_inflow"] = stakingGate
+
+	// Component 4: Derivatives leverage
+	derivsGate := &GateCheck{
+		Name:        "derivatives_oi",
+		Value:       data.DerivativesOIChange,
+		Threshold:   ge.config.DerivativesLeverageThreshold,
+		Description: fmt.Sprintf("OI change %.1f%% ‚â• %.1f%%", data.DerivativesOIChange, ge.config.DerivativesLeverageThreshold),
+		Passed:      data.DerivativesOIChange >= ge.config.DerivativesLeverageThreshold,
+	}
+	breakdown.ComponentResults["derivatives_oi"] = derivsGate
+
+	// Count passed components
+	for name, gate := range breakdown.ComponentResults {
+		if gate.Passed {
+			breakdown.PassedComponents = append(breakdown.PassedComponents, name)
+		}
+	}
+	breakdown.ComponentCount = len(breakdown.PassedComponents)
+
+	// Calculate proxy score based on component strength
+	breakdown.ProxyScore = ge.calculateSupplyProxyScore(data, breakdown)
+
+	// Main supply squeeze gate passes if proxy score meets threshold
+	supplyGate := &GateCheck{
+		Name:      "supply_squeeze",
+		Value:     breakdown.ProxyScore,
+		Threshold: ge.config.SupplySqueezeThreshold,
+		Description: fmt.Sprintf("Supply proxy %.2f ‚â• %.2f (%d/4 components)",
+			breakdown.ProxyScore, ge.config.SupplySqueezeThreshold, breakdown.ComponentCount),
+		Passed: breakdown.ProxyScore >= ge.config.SupplySqueezeThreshold,
+	}
+
+	result.GateResults["supply_squeeze"] = supplyGate
+	result.SupplyBreakdown = breakdown
+
+	if supplyGate.Passed {
+		result.PassedGates = append(result.PassedGates, "supply_squeeze")
+	} else {
+		result.FailedGates = append(result.FailedGates, "supply_squeeze")
+	}
+}
+
+// evaluateVolumeConfirmation checks additive volume confirmation for specific regimes
+func (ge *GateEvaluator) evaluateVolumeConfirmation(data *ConfirmationData, result *ConfirmationResult) {
+	// Only apply volume boost in risk_off or btc_driven regimes
+	if data.CurrentRegime != "risk_off" && data.CurrentRegime != "btc_driven" {
+		return
+	}
+
+	volumeGate := &GateCheck{
+		Name:      "volume_confirmation",
+		Value:     data.VolumeRatio24h,
+		Threshold: ge.config.VolumeConfirmationThreshold,
+		Description: fmt.Sprintf("Volume ratio %.2f√ó ‚â• %.2f√ó (regime: %s)",
+			data.VolumeRatio24h, ge.config.VolumeConfirmationThreshold, data.CurrentRegime),
+		Passed: data.VolumeRatio24h >= ge.config.VolumeConfirmationThreshold,
+	}
+
+	result.GateResults["volume_confirmation"] = volumeGate
+	result.VolumeBoost = volumeGate.Passed
+
+	if volumeGate.Passed {
+		result.PassedGates = append(result.PassedGates, "volume_confirmation")
+	}
+}
+
+// calculateSupplyProxyScore computes weighted supply squeeze proxy score from components
+func (ge *GateEvaluator) calculateSupplyProxyScore(data *ConfirmationData, breakdown *SupplySqueezeBreakdown) float64 {
+	var score float64
+	components := 0
+
+	// Reserve depletion contribution (0-0.3)
+	if breakdown.ComponentResults["reserve_depletion"].Passed {
+		reserveStrength := math.Min(1.0, math.Abs(data.ReserveChange7d)/20.0) // -20% = max strength
+		score += reserveStrength * 0.3
+		components++
+	}
+
+	// Large withdrawals contribution (0-0.25)
+	if breakdown.ComponentResults["large_withdrawals"].Passed {
+		withdrawalStrength := math.Min(1.0, data.LargeWithdrawals24h/100e6) // $100M = max strength
+		score += withdrawalStrength * 0.25
+		components++
+	}
+
+	// Staking inflows contribution (0-0.2)
+	if breakdown.ComponentResults["staking_inflow"].Passed {
+		stakingStrength := math.Min(1.0, data.StakingInflow24h/25e6) // $25M = max strength
+		score += stakingStrength * 0.2
+		components++
+	}
+
+	// Derivatives leverage contribution (0-0.25)
+	if breakdown.ComponentResults["derivatives_oi"].Passed {
+		oiStrength := math.Min(1.0, data.DerivativesOIChange/50.0) // 50% = max strength
+		score += oiStrength * 0.25
+		components++
+	}
+
+	// Require at least 2 components for valid proxy score
+	if components < 2 {
+		return 0.0
+	}
+
+	return score
+}
+
+// calculatePrecedenceScore computes weighted precedence for ranking candidates
+func (ge *GateEvaluator) calculatePrecedenceScore(result *ConfirmationResult) float64 {
+	var score float64
+
+	// Weight passed gates by precedence
+	for _, gateName := range result.PassedGates {
+		switch gateName {
+		case "funding_divergence":
+			score += ge.config.FundingPrecedence
+		case "whale_composite":
+			score += ge.config.WhalePrecedence
+		case "supply_squeeze":
+			score += ge.config.SupplyPrecedence
+		case "volume_confirmation":
+			score += 0.5 // Additive boost precedence
+		}
+	}
+
+	return score
+}
+
+// GetConfirmationSummary returns a concise summary of gate evaluation
+func (cr *ConfirmationResult) GetConfirmationSummary() string {
+	status := "‚ùå BLOCKED"
+	if cr.Passed {
+		status = "‚úÖ CONFIRMED"
+	}
+
+	volumeNote := ""
+	if cr.VolumeBoost {
+		volumeNote = " +VOL"
+	}
+
+	return fmt.Sprintf("%s ‚Äî %s (%d/%d gates%s, %.1f precedence, %dms)",
+		status, cr.Symbol, cr.ConfirmationCount, cr.RequiredCount,
+		volumeNote, cr.PrecedenceScore, cr.EvaluationTimeMs)
+}
+
+// GetDetailedReport returns comprehensive confirmation gate report
+func (cr *ConfirmationResult) GetDetailedReport() string {
+	report := fmt.Sprintf("Pre-Movement v3.3 Confirmation: %s\n", cr.Symbol)
+	report += fmt.Sprintf("Status: %s | Gates: %d/%d | Precedence: %.1f | Time: %dms\n\n",
+		map[bool]string{true: "CONFIRMED ‚úÖ", false: "BLOCKED ‚ùå"}[cr.Passed],
+		cr.ConfirmationCount, cr.RequiredCount, cr.PrecedenceScore, cr.EvaluationTimeMs)
+
+	// Core gate results
+	report += "Core Gates (2-of-3 required):\n"
+	coreGates := []string{"funding_divergence", "whale_composite", "supply_squeeze"}
+
+	for _, gateName := range coreGates {
+		if gate, exists := cr.GateResults[gateName]; exists {
+			status := map[bool]string{true: "‚úÖ", false: "‚ùå"}[gate.Passed]
+			report += fmt.Sprintf("  %s %s: %s\n", status, gateName, gate.Description)
+		}
+	}
+
+	// Supply squeeze breakdown
+	if cr.SupplyBreakdown != nil {
+		report += fmt.Sprintf("\nSupply Squeeze Components (%d/4 passed, need ‚â•2):\n", cr.SupplyBreakdown.ComponentCount)
+		componentOrder := []string{"reserve_depletion", "large_withdrawals", "staking_inflow", "derivatives_oi"}
+
+		for _, compName := range componentOrder {
+			if comp, exists := cr.SupplyBreakdown.ComponentResults[compName]; exists {
+				status := map[bool]string{true: "‚úÖ", false: "‚ùå"}[comp.Passed]
+				report += fmt.Sprintf("    %s %s: %s\n", status, compName, comp.Description)
+			}
+		}
+	}
+
+	// Volume confirmation
+	if cr.VolumeBoost {
+		if gate, exists := cr.GateResults["volume_confirmation"]; exists {
+			report += fmt.Sprintf("\nVolume Confirmation: ‚úÖ %s\n", gate.Description)
+		}
+	}
+
+	// Microstructure consultation
+	if cr.MicroReport != nil {
+		report += fmt.Sprintf("\nMicrostructure Consultation:\n")
+		report += fmt.Sprintf("  Spread: %.1f bps | Depth: $%.0f | VADR: %.2f√ó\n",
+			cr.MicroReport.SpreadBps, cr.MicroReport.DepthUSD, cr.MicroReport.VADR)
+	}
+
+	// Warnings
+	if len(cr.Warnings) > 0 {
+		report += fmt.Sprintf("\nWarnings:\n")
+		for i, warning := range cr.Warnings {
+			report += fmt.Sprintf("  %d. %s\n", i+1, warning)
+		}
+	}
+
+	return report
+}
+
+// RankCandidates sorts confirmation results by precedence score (highest first)
+func RankCandidates(results []*ConfirmationResult) []*ConfirmationResult {
+	ranked := make([]*ConfirmationResult, len(results))
+	copy(ranked, results)
+
+	sort.Slice(ranked, func(i, j int) bool {
+		// First sort by pass/fail status
+		if ranked[i].Passed != ranked[j].Passed {
+			return ranked[i].Passed // Passed results come first
+		}
+		// Then sort by precedence score (higher first)
+		return ranked[i].PrecedenceScore > ranked[j].PrecedenceScore
+	})
+
+	return ranked
+}
diff --git a/internal/premove/gates_test.go b/internal/premove/gates_test.go
index d83346d..9e0616b 100644
--- a/internal/premove/gates_test.go
+++ b/internal/premove/gates_test.go
@@ -1,401 +1,401 @@
-package premove
-
-import (
-	"context"
-	"testing"
-	"time"
-
-	"github.com/stretchr/testify/assert"
-	"github.com/stretchr/testify/require"
-
-	"cryptorun/internal/microstructure"
-)
-
-// Mock microstructure evaluator for testing
-type mockMicroEvaluator struct {
-	result *microstructure.EvaluationResult
-	err    error
-}
-
-func (m *mockMicroEvaluator) EvaluateSnapshot(ctx context.Context, symbol string) (*microstructure.EvaluationResult, error) {
-	if m.err != nil {
-		return nil, m.err
-	}
-	return m.result, nil
-}
-
-func TestGateEvaluator_EvaluateConfirmation_2of3Pass(t *testing.T) {
-	mockMicro := &mockMicroEvaluator{
-		result: &microstructure.EvaluationResult{
-			Symbol:    "BTC-USD",
-			SpreadBps: 25.0,
-			DepthUSD:  250000.0,
-			VADR:      2.1,
-		},
-	}
-
-	evaluator := NewGateEvaluator(mockMicro, nil)
-
-	data := &ConfirmationData{
-		Symbol:    "BTC-USD",
-		Timestamp: time.Now(),
-
-		// Strong funding divergence (PASS)
-		FundingZScore: 3.2,
-
-		// Strong whale activity (PASS)
-		WhaleComposite: 0.8,
-
-		// Weak supply squeeze proxy (FAIL)
-		SupplyProxyScore:     0.4, // Below 0.6 threshold
-		ReserveChange7d:      -2.0, // Not sufficient for depletion
-		LargeWithdrawals24h:  20e6, // Below $50M threshold
-		StakingInflow24h:     5e6,  // Below $10M threshold
-		DerivativesOIChange:  8.0,  // Below 15% threshold
-
-		// Volume confirmation
-		VolumeRatio24h: 3.0,
-		CurrentRegime:  "risk_off", // Enables volume confirmation
-
-		// Microstructure context
-		SpreadBps: 25.0,
-		DepthUSD:  250000.0,
-		VADR:      2.1,
-	}
-
-	result, err := evaluator.EvaluateConfirmation(context.Background(), data)
-	require.NoError(t, err)
-	assert.NotNil(t, result)
-
-	// Should pass with 2-of-3 + volume boost
-	assert.True(t, result.Passed, "Should pass with 2 strong confirmations")
-	assert.Equal(t, 2, result.ConfirmationCount, "Should have 2 confirmations")
-	assert.True(t, result.VolumeBoost, "Should have volume boost in risk_off regime")
-	
-	// Check individual gate results
-	assert.Contains(t, result.PassedGates, "funding_divergence")
-	assert.Contains(t, result.PassedGates, "whale_composite")
-	assert.Contains(t, result.PassedGates, "volume_confirmation")
-	assert.Contains(t, result.FailedGates, "supply_squeeze")
-
-	// Should have precedence score
-	assert.Greater(t, result.PrecedenceScore, 0.0, "Should calculate precedence score")
-
-	// Should have supply breakdown
-	assert.NotNil(t, result.SupplyBreakdown)
-	assert.Less(t, result.SupplyBreakdown.ComponentCount, 2, "Should have <2 supply components")
-}
-
-func TestGateEvaluator_EvaluateConfirmation_SupplySqueezeProxy(t *testing.T) {
-	mockMicro := &mockMicroEvaluator{
-		result: &microstructure.EvaluationResult{
-			Symbol:    "ETH-USD",
-			SpreadBps: 30.0,
-			DepthUSD:  180000.0,
-			VADR:      1.9,
-		},
-	}
-
-	evaluator := NewGateEvaluator(mockMicro, nil)
-
-	data := &ConfirmationData{
-		Symbol:    "ETH-USD",
-		Timestamp: time.Now(),
-
-		// Weak funding and whale
-		FundingZScore:  1.5, // Below 2.0 threshold
-		WhaleComposite: 0.5, // Below 0.7 threshold
-
-		// Strong supply squeeze (4-of-4 components passing)
-		ReserveChange7d:     -12.0, // Strong depletion ‚úÖ
-		LargeWithdrawals24h: 80e6,  // Large withdrawals ‚úÖ
-		StakingInflow24h:    15e6,  // Strong staking ‚úÖ
-		DerivativesOIChange: 25.0,  // Strong derivatives ‚úÖ
-
-		// Should generate high proxy score
-		SupplyProxyScore: 0.0, // Will be calculated
-
-		VolumeRatio24h: 1.5,
-		CurrentRegime:  "normal",
-	}
-
-	result, err := evaluator.EvaluateConfirmation(context.Background(), data)
-	require.NoError(t, err)
-
-	// Should fail overall (only 1-of-3 core confirmations)
-	assert.False(t, result.Passed, "Should fail with only supply squeeze passing")
-	assert.Equal(t, 1, result.ConfirmationCount, "Should have 1 confirmation")
-
-	// But supply squeeze should pass with strong proxy score
-	assert.Contains(t, result.PassedGates, "supply_squeeze")
-	assert.NotNil(t, result.SupplyBreakdown)
-	assert.Equal(t, 4, result.SupplyBreakdown.ComponentCount, "Should have all 4 supply components")
-	assert.GreaterOrEqual(t, result.SupplyBreakdown.ProxyScore, 0.6, "Strong components should yield high proxy score")
-
-	// Check individual supply components
-	supplyBreakdown := result.SupplyBreakdown
-	assert.True(t, supplyBreakdown.ComponentResults["reserve_depletion"].Passed)
-	assert.True(t, supplyBreakdown.ComponentResults["large_withdrawals"].Passed)
-	assert.True(t, supplyBreakdown.ComponentResults["staking_inflow"].Passed)
-	assert.True(t, supplyBreakdown.ComponentResults["derivatives_oi"].Passed)
-}
-
-func TestGateEvaluator_EvaluateConfirmation_VolumeBoostRegimes(t *testing.T) {
-	mockMicro := &mockMicroEvaluator{
-		result: &microstructure.EvaluationResult{
-			Symbol:    "SOL-USD", 
-			SpreadBps: 35.0,
-			DepthUSD:  120000.0,
-			VADR:      1.8,
-		},
-	}
-
-	evaluator := NewGateEvaluator(mockMicro, nil)
-
-	// Test volume boost in btc_driven regime
-	data := &ConfirmationData{
-		Symbol:         "SOL-USD",
-		Timestamp:      time.Now(),
-		FundingZScore:  2.5,  // Only 1 strong confirmation
-		WhaleComposite: 0.5,  // Weak
-		SupplyProxyScore: 0.4, // Weak
-
-		// Strong volume confirmation
-		VolumeRatio24h: 4.0,
-		CurrentRegime:  "btc_driven", // Should enable volume boost
-	}
-
-	result, err := evaluator.EvaluateConfirmation(context.Background(), data)
-	require.NoError(t, err)
-
-	// Should pass with 1-of-3 + volume boost in btc_driven regime
-	assert.True(t, result.Passed, "Should pass with volume boost in btc_driven regime")
-	assert.Equal(t, 1, result.ConfirmationCount, "Should have 1 core confirmation")
-	assert.Equal(t, 1, result.RequiredCount, "Volume boost should reduce requirement to 1")
-	assert.True(t, result.VolumeBoost, "Should have volume boost")
-
-	// Test no volume boost in normal regime
-	data.CurrentRegime = "normal"
-	result, err = evaluator.EvaluateConfirmation(context.Background(), data)
-	require.NoError(t, err)
-
-	assert.False(t, result.Passed, "Should not pass without volume boost in normal regime")
-	assert.Equal(t, 2, result.RequiredCount, "Should require 2-of-3 in normal regime")
-	assert.False(t, result.VolumeBoost, "Should not have volume boost in normal regime")
-}
-
-func TestGateEvaluator_EvaluateConfirmation_PrecedenceRanking(t *testing.T) {
-	mockMicro := &mockMicroEvaluator{
-		result: &microstructure.EvaluationResult{
-			Symbol:    "ADA-USD",
-			SpreadBps: 40.0,
-			DepthUSD:  90000.0,
-			VADR:      1.7,
-		},
-	}
-
-	evaluator := NewGateEvaluator(mockMicro, nil)
-
-	// Test all 3 confirmations passing (should get max precedence)
-	data := &ConfirmationData{
-		Symbol:    "ADA-USD",
-		Timestamp: time.Now(),
-
-		// All strong confirmations
-		FundingZScore:        3.0, // Funding (precedence 3.0)
-		WhaleComposite:       0.8, // Whale (precedence 2.0)  
-		SupplyProxyScore:     0.7, // Supply (precedence 1.0)
-		ReserveChange7d:      -10.0,
-		LargeWithdrawals24h:  60e6,
-		StakingInflow24h:     12e6,
-		DerivativesOIChange:  18.0,
-
-		VolumeRatio24h: 2.0,
-		CurrentRegime:  "normal",
-	}
-
-	result, err := evaluator.EvaluateConfirmation(context.Background(), data)
-	require.NoError(t, err)
-
-	assert.True(t, result.Passed, "Should pass with all confirmations")
-	assert.Equal(t, 3, result.ConfirmationCount, "Should have all 3 confirmations")
-	
-	// Should have maximum precedence (3.0 + 2.0 + 1.0 = 6.0)
-	assert.Equal(t, 6.0, result.PrecedenceScore, "Should have max precedence with all gates")
-}
-
-func TestGateEvaluator_RankCandidates(t *testing.T) {
-	// Create mock results with different strengths
-	results := []*ConfirmationResult{
-		{
-			Symbol:            "WEAK",
-			Passed:            false,
-			ConfirmationCount: 0,
-			PrecedenceScore:   0.0,
-		},
-		{
-			Symbol:            "STRONG",
-			Passed:            true,
-			ConfirmationCount: 3,
-			PrecedenceScore:   6.0, // All gates
-		},
-		{
-			Symbol:            "MODERATE",
-			Passed:            true,
-			ConfirmationCount: 2,
-			PrecedenceScore:   5.0, // Funding + whale
-		},
-		{
-			Symbol:            "BLOCKED",
-			Passed:            false,
-			ConfirmationCount: 1,
-			PrecedenceScore:   3.0,
-		},
-	}
-
-	ranked := RankCandidates(results)
-
-	// Should sort passed first, then by precedence
-	assert.Equal(t, "STRONG", ranked[0].Symbol, "Strongest should rank first")
-	assert.Equal(t, "MODERATE", ranked[1].Symbol, "Moderate should rank second")
-	assert.True(t, ranked[0].Passed && ranked[1].Passed, "Passed results should rank first")
-	assert.False(t, ranked[2].Passed || ranked[3].Passed, "Failed results should rank last")
-}
-
-func TestGateEvaluator_PerformanceTimeout(t *testing.T) {
-	// Mock slow microstructure evaluator
-	slowMicro := &mockMicroEvaluator{
-		result: &microstructure.EvaluationResult{
-			Symbol:    "SLOW-USD",
-			SpreadBps: 50.0,
-			DepthUSD:  75000.0,
-			VADR:      1.5,
-		},
-	}
-
-	config := DefaultGateConfig()
-	config.MaxEvaluationTimeMs = 10 // Very low timeout for testing
-
-	evaluator := NewGateEvaluator(slowMicro, config)
-
-	data := &ConfirmationData{
-		Symbol:         "SLOW-USD",
-		Timestamp:      time.Now(),
-		FundingZScore:  2.5,
-		WhaleComposite: 0.8,
-		SupplyProxyScore: 0.7,
-		VolumeRatio24h: 2.0,
-		CurrentRegime:  "normal",
-	}
-
-	result, err := evaluator.EvaluateConfirmation(context.Background(), data)
-	require.NoError(t, err)
-
-	// Should complete but may have performance warning
-	// (In practice, we'd add artificial delay to test timeout warning)
-	assert.NotNil(t, result)
-	assert.GreaterOrEqual(t, result.EvaluationTimeMs, int64(0))
-}
-
-func TestGateEvaluator_GetConfirmationSummary(t *testing.T) {
-	result := &ConfirmationResult{
-		Symbol:            "BTC-USD",
-		Passed:            true,
-		ConfirmationCount: 2,
-		RequiredCount:     2,
-		VolumeBoost:       true,
-		PrecedenceScore:   5.0,
-		EvaluationTimeMs:  123,
-	}
-
-	summary := result.GetConfirmationSummary()
-	assert.Contains(t, summary, "‚úÖ CONFIRMED")
-	assert.Contains(t, summary, "BTC-USD")
-	assert.Contains(t, summary, "2/2 gates")
-	assert.Contains(t, summary, "+VOL")
-	assert.Contains(t, summary, "5.0 precedence")
-	assert.Contains(t, summary, "123ms")
-}
-
-func TestGateEvaluator_GetDetailedReport(t *testing.T) {
-	result := &ConfirmationResult{
-		Symbol:            "ETH-USD",
-		Passed:            true,
-		ConfirmationCount: 2,
-		RequiredCount:     2,
-		PrecedenceScore:   5.0,
-		EvaluationTimeMs:  89,
-		GateResults: map[string]*GateCheck{
-			"funding_divergence": {
-				Name:        "funding_divergence",
-				Passed:      true,
-				Description: "Funding z-score 2.50 ‚â• 2.00",
-			},
-			"whale_composite": {
-				Name:        "whale_composite", 
-				Passed:      true,
-				Description: "Whale composite 0.75 ‚â• 0.70",
-			},
-			"supply_squeeze": {
-				Name:        "supply_squeeze",
-				Passed:      false,
-				Description: "Supply proxy 0.45 ‚â• 0.60 (1/4 components)",
-			},
-		},
-		SupplyBreakdown: &SupplySqueezeBreakdown{
-			ComponentCount: 1,
-			ComponentResults: map[string]*GateCheck{
-				"reserve_depletion": {
-					Name:        "reserve_depletion",
-					Passed:      true,
-					Description: "Reserve change -8.0% ‚â§ -5.0%",
-				},
-			},
-		},
-		VolumeBoost: false,
-		MicroReport: &microstructure.EvaluationResult{
-			SpreadBps: 28.0,
-			DepthUSD:  180000.0,
-			VADR:      2.0,
-		},
-	}
-
-	report := result.GetDetailedReport()
-	assert.Contains(t, report, "ETH-USD")
-	assert.Contains(t, report, "CONFIRMED ‚úÖ")
-	assert.Contains(t, report, "2/2")
-	assert.Contains(t, report, "5.0 precedence")
-	assert.Contains(t, report, "‚úÖ funding_divergence")
-	assert.Contains(t, report, "‚úÖ whale_composite") 
-	assert.Contains(t, report, "‚ùå supply_squeeze")
-	assert.Contains(t, report, "Supply Squeeze Components (1/4 passed")
-	assert.Contains(t, report, "Spread: 28.0 bps")
-}
-
-func TestDefaultGateConfig(t *testing.T) {
-	config := DefaultGateConfig()
-	require.NotNil(t, config)
-
-	// Check core thresholds
-	assert.Equal(t, 2.0, config.FundingDivergenceThreshold)
-	assert.Equal(t, 0.6, config.SupplySqueezeThreshold)
-	assert.Equal(t, 0.7, config.WhaleCompositeThreshold)
-
-	// Check supply squeeze component thresholds
-	assert.Equal(t, -5.0, config.ReserveDepletionThreshold)
-	assert.Equal(t, 50e6, config.LargeWithdrawalsThreshold)
-	assert.Equal(t, 10e6, config.StakingInflowThreshold)
-	assert.Equal(t, 15.0, config.DerivativesLeverageThreshold)
-
-	// Check precedence weights
-	assert.Equal(t, 3.0, config.FundingPrecedence, "Funding should have highest precedence")
-	assert.Equal(t, 2.0, config.WhalePrecedence, "Whale should have medium precedence")
-	assert.Equal(t, 1.0, config.SupplyPrecedence, "Supply should have lowest precedence")
-
-	// Check volume confirmation
-	assert.True(t, config.VolumeConfirmationEnabled)
-	assert.Equal(t, 2.5, config.VolumeConfirmationThreshold)
-
-	// Check performance limits
-	assert.Equal(t, int64(500), config.MaxEvaluationTimeMs)
-}
\ No newline at end of file
+package premove
+
+import (
+	"context"
+	"testing"
+	"time"
+
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+
+	"cryptorun/internal/microstructure"
+)
+
+// Mock microstructure evaluator for testing
+type mockMicroEvaluator struct {
+	result *microstructure.EvaluationResult
+	err    error
+}
+
+func (m *mockMicroEvaluator) EvaluateSnapshot(ctx context.Context, symbol string) (*microstructure.EvaluationResult, error) {
+	if m.err != nil {
+		return nil, m.err
+	}
+	return m.result, nil
+}
+
+func TestGateEvaluator_EvaluateConfirmation_2of3Pass(t *testing.T) {
+	mockMicro := &mockMicroEvaluator{
+		result: &microstructure.EvaluationResult{
+			Symbol:    "BTC-USD",
+			SpreadBps: 25.0,
+			DepthUSD:  250000.0,
+			VADR:      2.1,
+		},
+	}
+
+	evaluator := NewGateEvaluator(mockMicro, nil)
+
+	data := &ConfirmationData{
+		Symbol:    "BTC-USD",
+		Timestamp: time.Now(),
+
+		// Strong funding divergence (PASS)
+		FundingZScore: 3.2,
+
+		// Strong whale activity (PASS)
+		WhaleComposite: 0.8,
+
+		// Weak supply squeeze proxy (FAIL)
+		SupplyProxyScore:    0.4,  // Below 0.6 threshold
+		ReserveChange7d:     -2.0, // Not sufficient for depletion
+		LargeWithdrawals24h: 20e6, // Below $50M threshold
+		StakingInflow24h:    5e6,  // Below $10M threshold
+		DerivativesOIChange: 8.0,  // Below 15% threshold
+
+		// Volume confirmation
+		VolumeRatio24h: 3.0,
+		CurrentRegime:  "risk_off", // Enables volume confirmation
+
+		// Microstructure context
+		SpreadBps: 25.0,
+		DepthUSD:  250000.0,
+		VADR:      2.1,
+	}
+
+	result, err := evaluator.EvaluateConfirmation(context.Background(), data)
+	require.NoError(t, err)
+	assert.NotNil(t, result)
+
+	// Should pass with 2-of-3 + volume boost
+	assert.True(t, result.Passed, "Should pass with 2 strong confirmations")
+	assert.Equal(t, 2, result.ConfirmationCount, "Should have 2 confirmations")
+	assert.True(t, result.VolumeBoost, "Should have volume boost in risk_off regime")
+
+	// Check individual gate results
+	assert.Contains(t, result.PassedGates, "funding_divergence")
+	assert.Contains(t, result.PassedGates, "whale_composite")
+	assert.Contains(t, result.PassedGates, "volume_confirmation")
+	assert.Contains(t, result.FailedGates, "supply_squeeze")
+
+	// Should have precedence score
+	assert.Greater(t, result.PrecedenceScore, 0.0, "Should calculate precedence score")
+
+	// Should have supply breakdown
+	assert.NotNil(t, result.SupplyBreakdown)
+	assert.Less(t, result.SupplyBreakdown.ComponentCount, 2, "Should have <2 supply components")
+}
+
+func TestGateEvaluator_EvaluateConfirmation_SupplySqueezeProxy(t *testing.T) {
+	mockMicro := &mockMicroEvaluator{
+		result: &microstructure.EvaluationResult{
+			Symbol:    "ETH-USD",
+			SpreadBps: 30.0,
+			DepthUSD:  180000.0,
+			VADR:      1.9,
+		},
+	}
+
+	evaluator := NewGateEvaluator(mockMicro, nil)
+
+	data := &ConfirmationData{
+		Symbol:    "ETH-USD",
+		Timestamp: time.Now(),
+
+		// Weak funding and whale
+		FundingZScore:  1.5, // Below 2.0 threshold
+		WhaleComposite: 0.5, // Below 0.7 threshold
+
+		// Strong supply squeeze (4-of-4 components passing)
+		ReserveChange7d:     -12.0, // Strong depletion ‚úÖ
+		LargeWithdrawals24h: 80e6,  // Large withdrawals ‚úÖ
+		StakingInflow24h:    15e6,  // Strong staking ‚úÖ
+		DerivativesOIChange: 25.0,  // Strong derivatives ‚úÖ
+
+		// Should generate high proxy score
+		SupplyProxyScore: 0.0, // Will be calculated
+
+		VolumeRatio24h: 1.5,
+		CurrentRegime:  "normal",
+	}
+
+	result, err := evaluator.EvaluateConfirmation(context.Background(), data)
+	require.NoError(t, err)
+
+	// Should fail overall (only 1-of-3 core confirmations)
+	assert.False(t, result.Passed, "Should fail with only supply squeeze passing")
+	assert.Equal(t, 1, result.ConfirmationCount, "Should have 1 confirmation")
+
+	// But supply squeeze should pass with strong proxy score
+	assert.Contains(t, result.PassedGates, "supply_squeeze")
+	assert.NotNil(t, result.SupplyBreakdown)
+	assert.Equal(t, 4, result.SupplyBreakdown.ComponentCount, "Should have all 4 supply components")
+	assert.GreaterOrEqual(t, result.SupplyBreakdown.ProxyScore, 0.6, "Strong components should yield high proxy score")
+
+	// Check individual supply components
+	supplyBreakdown := result.SupplyBreakdown
+	assert.True(t, supplyBreakdown.ComponentResults["reserve_depletion"].Passed)
+	assert.True(t, supplyBreakdown.ComponentResults["large_withdrawals"].Passed)
+	assert.True(t, supplyBreakdown.ComponentResults["staking_inflow"].Passed)
+	assert.True(t, supplyBreakdown.ComponentResults["derivatives_oi"].Passed)
+}
+
+func TestGateEvaluator_EvaluateConfirmation_VolumeBoostRegimes(t *testing.T) {
+	mockMicro := &mockMicroEvaluator{
+		result: &microstructure.EvaluationResult{
+			Symbol:    "SOL-USD",
+			SpreadBps: 35.0,
+			DepthUSD:  120000.0,
+			VADR:      1.8,
+		},
+	}
+
+	evaluator := NewGateEvaluator(mockMicro, nil)
+
+	// Test volume boost in btc_driven regime
+	data := &ConfirmationData{
+		Symbol:           "SOL-USD",
+		Timestamp:        time.Now(),
+		FundingZScore:    2.5, // Only 1 strong confirmation
+		WhaleComposite:   0.5, // Weak
+		SupplyProxyScore: 0.4, // Weak
+
+		// Strong volume confirmation
+		VolumeRatio24h: 4.0,
+		CurrentRegime:  "btc_driven", // Should enable volume boost
+	}
+
+	result, err := evaluator.EvaluateConfirmation(context.Background(), data)
+	require.NoError(t, err)
+
+	// Should pass with 1-of-3 + volume boost in btc_driven regime
+	assert.True(t, result.Passed, "Should pass with volume boost in btc_driven regime")
+	assert.Equal(t, 1, result.ConfirmationCount, "Should have 1 core confirmation")
+	assert.Equal(t, 1, result.RequiredCount, "Volume boost should reduce requirement to 1")
+	assert.True(t, result.VolumeBoost, "Should have volume boost")
+
+	// Test no volume boost in normal regime
+	data.CurrentRegime = "normal"
+	result, err = evaluator.EvaluateConfirmation(context.Background(), data)
+	require.NoError(t, err)
+
+	assert.False(t, result.Passed, "Should not pass without volume boost in normal regime")
+	assert.Equal(t, 2, result.RequiredCount, "Should require 2-of-3 in normal regime")
+	assert.False(t, result.VolumeBoost, "Should not have volume boost in normal regime")
+}
+
+func TestGateEvaluator_EvaluateConfirmation_PrecedenceRanking(t *testing.T) {
+	mockMicro := &mockMicroEvaluator{
+		result: &microstructure.EvaluationResult{
+			Symbol:    "ADA-USD",
+			SpreadBps: 40.0,
+			DepthUSD:  90000.0,
+			VADR:      1.7,
+		},
+	}
+
+	evaluator := NewGateEvaluator(mockMicro, nil)
+
+	// Test all 3 confirmations passing (should get max precedence)
+	data := &ConfirmationData{
+		Symbol:    "ADA-USD",
+		Timestamp: time.Now(),
+
+		// All strong confirmations
+		FundingZScore:       3.0, // Funding (precedence 3.0)
+		WhaleComposite:      0.8, // Whale (precedence 2.0)
+		SupplyProxyScore:    0.7, // Supply (precedence 1.0)
+		ReserveChange7d:     -10.0,
+		LargeWithdrawals24h: 60e6,
+		StakingInflow24h:    12e6,
+		DerivativesOIChange: 18.0,
+
+		VolumeRatio24h: 2.0,
+		CurrentRegime:  "normal",
+	}
+
+	result, err := evaluator.EvaluateConfirmation(context.Background(), data)
+	require.NoError(t, err)
+
+	assert.True(t, result.Passed, "Should pass with all confirmations")
+	assert.Equal(t, 3, result.ConfirmationCount, "Should have all 3 confirmations")
+
+	// Should have maximum precedence (3.0 + 2.0 + 1.0 = 6.0)
+	assert.Equal(t, 6.0, result.PrecedenceScore, "Should have max precedence with all gates")
+}
+
+func TestGateEvaluator_RankCandidates(t *testing.T) {
+	// Create mock results with different strengths
+	results := []*ConfirmationResult{
+		{
+			Symbol:            "WEAK",
+			Passed:            false,
+			ConfirmationCount: 0,
+			PrecedenceScore:   0.0,
+		},
+		{
+			Symbol:            "STRONG",
+			Passed:            true,
+			ConfirmationCount: 3,
+			PrecedenceScore:   6.0, // All gates
+		},
+		{
+			Symbol:            "MODERATE",
+			Passed:            true,
+			ConfirmationCount: 2,
+			PrecedenceScore:   5.0, // Funding + whale
+		},
+		{
+			Symbol:            "BLOCKED",
+			Passed:            false,
+			ConfirmationCount: 1,
+			PrecedenceScore:   3.0,
+		},
+	}
+
+	ranked := RankCandidates(results)
+
+	// Should sort passed first, then by precedence
+	assert.Equal(t, "STRONG", ranked[0].Symbol, "Strongest should rank first")
+	assert.Equal(t, "MODERATE", ranked[1].Symbol, "Moderate should rank second")
+	assert.True(t, ranked[0].Passed && ranked[1].Passed, "Passed results should rank first")
+	assert.False(t, ranked[2].Passed || ranked[3].Passed, "Failed results should rank last")
+}
+
+func TestGateEvaluator_PerformanceTimeout(t *testing.T) {
+	// Mock slow microstructure evaluator
+	slowMicro := &mockMicroEvaluator{
+		result: &microstructure.EvaluationResult{
+			Symbol:    "SLOW-USD",
+			SpreadBps: 50.0,
+			DepthUSD:  75000.0,
+			VADR:      1.5,
+		},
+	}
+
+	config := DefaultGateConfig()
+	config.MaxEvaluationTimeMs = 10 // Very low timeout for testing
+
+	evaluator := NewGateEvaluator(slowMicro, config)
+
+	data := &ConfirmationData{
+		Symbol:           "SLOW-USD",
+		Timestamp:        time.Now(),
+		FundingZScore:    2.5,
+		WhaleComposite:   0.8,
+		SupplyProxyScore: 0.7,
+		VolumeRatio24h:   2.0,
+		CurrentRegime:    "normal",
+	}
+
+	result, err := evaluator.EvaluateConfirmation(context.Background(), data)
+	require.NoError(t, err)
+
+	// Should complete but may have performance warning
+	// (In practice, we'd add artificial delay to test timeout warning)
+	assert.NotNil(t, result)
+	assert.GreaterOrEqual(t, result.EvaluationTimeMs, int64(0))
+}
+
+func TestGateEvaluator_GetConfirmationSummary(t *testing.T) {
+	result := &ConfirmationResult{
+		Symbol:            "BTC-USD",
+		Passed:            true,
+		ConfirmationCount: 2,
+		RequiredCount:     2,
+		VolumeBoost:       true,
+		PrecedenceScore:   5.0,
+		EvaluationTimeMs:  123,
+	}
+
+	summary := result.GetConfirmationSummary()
+	assert.Contains(t, summary, "‚úÖ CONFIRMED")
+	assert.Contains(t, summary, "BTC-USD")
+	assert.Contains(t, summary, "2/2 gates")
+	assert.Contains(t, summary, "+VOL")
+	assert.Contains(t, summary, "5.0 precedence")
+	assert.Contains(t, summary, "123ms")
+}
+
+func TestGateEvaluator_GetDetailedReport(t *testing.T) {
+	result := &ConfirmationResult{
+		Symbol:            "ETH-USD",
+		Passed:            true,
+		ConfirmationCount: 2,
+		RequiredCount:     2,
+		PrecedenceScore:   5.0,
+		EvaluationTimeMs:  89,
+		GateResults: map[string]*GateCheck{
+			"funding_divergence": {
+				Name:        "funding_divergence",
+				Passed:      true,
+				Description: "Funding z-score 2.50 ‚â• 2.00",
+			},
+			"whale_composite": {
+				Name:        "whale_composite",
+				Passed:      true,
+				Description: "Whale composite 0.75 ‚â• 0.70",
+			},
+			"supply_squeeze": {
+				Name:        "supply_squeeze",
+				Passed:      false,
+				Description: "Supply proxy 0.45 ‚â• 0.60 (1/4 components)",
+			},
+		},
+		SupplyBreakdown: &SupplySqueezeBreakdown{
+			ComponentCount: 1,
+			ComponentResults: map[string]*GateCheck{
+				"reserve_depletion": {
+					Name:        "reserve_depletion",
+					Passed:      true,
+					Description: "Reserve change -8.0% ‚â§ -5.0%",
+				},
+			},
+		},
+		VolumeBoost: false,
+		MicroReport: &microstructure.EvaluationResult{
+			SpreadBps: 28.0,
+			DepthUSD:  180000.0,
+			VADR:      2.0,
+		},
+	}
+
+	report := result.GetDetailedReport()
+	assert.Contains(t, report, "ETH-USD")
+	assert.Contains(t, report, "CONFIRMED ‚úÖ")
+	assert.Contains(t, report, "2/2")
+	assert.Contains(t, report, "5.0 precedence")
+	assert.Contains(t, report, "‚úÖ funding_divergence")
+	assert.Contains(t, report, "‚úÖ whale_composite")
+	assert.Contains(t, report, "‚ùå supply_squeeze")
+	assert.Contains(t, report, "Supply Squeeze Components (1/4 passed")
+	assert.Contains(t, report, "Spread: 28.0 bps")
+}
+
+func TestDefaultGateConfig(t *testing.T) {
+	config := DefaultGateConfig()
+	require.NotNil(t, config)
+
+	// Check core thresholds
+	assert.Equal(t, 2.0, config.FundingDivergenceThreshold)
+	assert.Equal(t, 0.6, config.SupplySqueezeThreshold)
+	assert.Equal(t, 0.7, config.WhaleCompositeThreshold)
+
+	// Check supply squeeze component thresholds
+	assert.Equal(t, -5.0, config.ReserveDepletionThreshold)
+	assert.Equal(t, 50e6, config.LargeWithdrawalsThreshold)
+	assert.Equal(t, 10e6, config.StakingInflowThreshold)
+	assert.Equal(t, 15.0, config.DerivativesLeverageThreshold)
+
+	// Check precedence weights
+	assert.Equal(t, 3.0, config.FundingPrecedence, "Funding should have highest precedence")
+	assert.Equal(t, 2.0, config.WhalePrecedence, "Whale should have medium precedence")
+	assert.Equal(t, 1.0, config.SupplyPrecedence, "Supply should have lowest precedence")
+
+	// Check volume confirmation
+	assert.True(t, config.VolumeConfirmationEnabled)
+	assert.Equal(t, 2.5, config.VolumeConfirmationThreshold)
+
+	// Check performance limits
+	assert.Equal(t, int64(500), config.MaxEvaluationTimeMs)
+}
diff --git a/internal/premove/score.go b/internal/premove/score.go
index b26cc8d..c698258 100644
--- a/internal/premove/score.go
+++ b/internal/premove/score.go
@@ -1,370 +1,370 @@
-package premove
-
-import (
-	"context"
-	"fmt"
-	"math"
-	"time"
-)
-
-// ScoreEngine implements Pre-Movement v3.3 100-point scoring system
-type ScoreEngine struct {
-	config *ScoreConfig
-}
-
-// NewScoreEngine creates a Pre-Movement v3.3 scoring engine
-func NewScoreEngine(config *ScoreConfig) *ScoreEngine {
-	if config == nil {
-		config = DefaultScoreConfig()
-	}
-	return &ScoreEngine{config: config}
-}
-
-// ScoreConfig contains thresholds and weights for Pre-Movement v3.3 scoring
-type ScoreConfig struct {
-	// Structural Components (0-40 points)
-	DerivativesWeight    float64 `yaml:"derivatives_weight"`     // 15 pts: funding, OI, ETF
-	SupplyDemandWeight   float64 `yaml:"supply_demand_weight"`   // 15 pts: reserves, whale moves
-	MicrostructureWeight float64 `yaml:"microstructure_weight"` // 10 pts: L1/L2 dynamics
-
-	// Behavioral Components (0-35 points) 
-	SmartMoneyWeight float64 `yaml:"smart_money_weight"` // 20 pts: large tx patterns
-	CVDResidualWeight float64 `yaml:"cvd_residual_weight"` // 15 pts: cumulative volume delta
-
-	// Catalyst & Compression (0-25 points)
-	CatalystWeight    float64 `yaml:"catalyst_weight"`    // 15 pts: news/events
-	CompressionWeight float64 `yaml:"compression_weight"` // 10 pts: volatility compression
-
-	// Freshness penalty parameters
-	MaxFreshnessHours   float64 `yaml:"max_freshness_hours"`   // 2.0 hours
-	FreshnessPenaltyPct float64 `yaml:"freshness_penalty_pct"` // 20% max penalty
-
-	// Score normalization
-	MinScore float64 `yaml:"min_score"` // 0.0
-	MaxScore float64 `yaml:"max_score"` // 100.0
-}
-
-// DefaultScoreConfig returns Pre-Movement v3.3 production configuration
-func DefaultScoreConfig() *ScoreConfig {
-	return &ScoreConfig{
-		// Structural (40 points total)
-		DerivativesWeight:    15.0,
-		SupplyDemandWeight:   15.0, 
-		MicrostructureWeight: 10.0,
-
-		// Behavioral (35 points total)
-		SmartMoneyWeight:  20.0,
-		CVDResidualWeight: 15.0,
-
-		// Catalyst & Compression (25 points total)
-		CatalystWeight:    15.0,
-		CompressionWeight: 10.0,
-
-		// Freshness parameters
-		MaxFreshnessHours:   2.0,  // "worst feed wins" rule
-		FreshnessPenaltyPct: 20.0, // max 20% penalty
-
-		// Score bounds
-		MinScore: 0.0,
-		MaxScore: 100.0,
-	}
-}
-
-// PreMovementData contains all inputs for Pre-Movement v3.3 scoring
-type PreMovementData struct {
-	Symbol    string    `json:"symbol"`
-	Timestamp time.Time `json:"timestamp"`
-
-	// Structural factors
-	FundingZScore      float64 `json:"funding_z_score"`       // Cross-venue funding divergence
-	OIResidual         float64 `json:"oi_residual"`           // Open interest anomaly
-	ETFFlowTint        float64 `json:"etf_flow_tint"`         // ETF net flow direction
-	ReserveChange7d    float64 `json:"reserve_change_7d"`     // Exchange reserves change
-	WhaleComposite     float64 `json:"whale_composite"`       // Large transaction patterns
-	MicroDynamics      float64 `json:"micro_dynamics"`        // L1/L2 order book stress
-
-	// Behavioral factors
-	SmartMoneyFlow     float64 `json:"smart_money_flow"`      // Institutional flow patterns
-	CVDResidual        float64 `json:"cvd_residual"`          // Volume-price residual
-
-	// Catalyst & compression
-	CatalystHeat       float64 `json:"catalyst_heat"`         // News/event significance
-	VolCompressionRank float64 `json:"vol_compression_rank"`  // Volatility compression percentile
-
-	// Data freshness tracking (worst feed wins)
-	OldestFeedHours float64 `json:"oldest_feed_hours"` // Hours since oldest data point
-}
-
-// ScoreResult contains the complete Pre-Movement v3.3 score breakdown
-type ScoreResult struct {
-	Symbol           string                 `json:"symbol"`
-	Timestamp        time.Time              `json:"timestamp"`
-	TotalScore       float64                `json:"total_score"`       // 0-100 final score
-	ComponentScores  map[string]float64     `json:"component_scores"`  // Individual component contributions
-	Attribution      map[string]interface{} `json:"attribution"`       // Detailed score attribution
-	DataFreshness    *FreshnessInfo         `json:"data_freshness"`    // Freshness penalty details
-	EvaluationTimeMs int64                  `json:"evaluation_time_ms"`
-	IsValid          bool                   `json:"is_valid"`          // Whether score is actionable
-	Warnings         []string               `json:"warnings"`          // Data quality warnings
-}
-
-// FreshnessInfo tracks data staleness and penalties applied
-type FreshnessInfo struct {
-	OldestFeedHours    float64 `json:"oldest_feed_hours"`
-	FreshnessPenalty   float64 `json:"freshness_penalty"`   // 0-20% penalty applied
-	AffectedComponents []string `json:"affected_components"` // Components penalized
-	WorstFeed          string  `json:"worst_feed"`          // Name of stalest feed
-}
-
-// CalculateScore computes the complete Pre-Movement v3.3 score
-func (se *ScoreEngine) CalculateScore(ctx context.Context, data *PreMovementData) (*ScoreResult, error) {
-	startTime := time.Now()
-
-	result := &ScoreResult{
-		Symbol:          data.Symbol,
-		Timestamp:       time.Now(),
-		ComponentScores: make(map[string]float64),
-		Attribution:     make(map[string]interface{}),
-		Warnings:        []string{},
-	}
-
-	// Calculate individual component scores
-	structuralScore := se.calculateStructuralScore(data, result)
-	behavioralScore := se.calculateBehavioralScore(data, result)
-	catalystScore := se.calculateCatalystScore(data, result)
-
-	// Base score before freshness penalty
-	baseScore := structuralScore + behavioralScore + catalystScore
-
-	// Apply freshness penalty ("worst feed wins" rule)
-	freshnessInfo, finalScore := se.applyFreshnessPenalty(baseScore, data.OldestFeedHours)
-	result.DataFreshness = freshnessInfo
-
-	// Normalize and bound the score
-	result.TotalScore = se.normalizeScore(finalScore)
-	result.IsValid = result.TotalScore >= 0.0 && len(result.Warnings) == 0
-	result.EvaluationTimeMs = time.Since(startTime).Milliseconds()
-
-	return result, nil
-}
-
-// calculateStructuralScore computes structural factors (40 points max)
-func (se *ScoreEngine) calculateStructuralScore(data *PreMovementData, result *ScoreResult) float64 {
-	var score float64
-
-	// Derivatives component (15 points max)
-	derivScore := se.scoreDerivatives(data.FundingZScore, data.OIResidual, data.ETFFlowTint)
-	result.ComponentScores["derivatives"] = derivScore
-	result.Attribution["derivatives"] = map[string]interface{}{
-		"funding_z_score": data.FundingZScore,
-		"oi_residual":     data.OIResidual,
-		"etf_flow_tint":   data.ETFFlowTint,
-		"contribution":    derivScore,
-	}
-	score += derivScore
-
-	// Supply/demand component (15 points max)
-	supplyScore := se.scoreSupplyDemand(data.ReserveChange7d, data.WhaleComposite)
-	result.ComponentScores["supply_demand"] = supplyScore
-	result.Attribution["supply_demand"] = map[string]interface{}{
-		"reserve_change_7d": data.ReserveChange7d,
-		"whale_composite":   data.WhaleComposite,
-		"contribution":      supplyScore,
-	}
-	score += supplyScore
-
-	// Microstructure component (10 points max)
-	microScore := se.scoreMicrostructure(data.MicroDynamics)
-	result.ComponentScores["microstructure"] = microScore
-	result.Attribution["microstructure"] = map[string]interface{}{
-		"micro_dynamics": data.MicroDynamics,
-		"contribution":   microScore,
-	}
-	score += microScore
-
-	return score
-}
-
-// calculateBehavioralScore computes behavioral factors (35 points max)
-func (se *ScoreEngine) calculateBehavioralScore(data *PreMovementData, result *ScoreResult) float64 {
-	var score float64
-
-	// Smart money component (20 points max)
-	smartScore := se.scoreSmartMoney(data.SmartMoneyFlow)
-	result.ComponentScores["smart_money"] = smartScore
-	result.Attribution["smart_money"] = map[string]interface{}{
-		"smart_money_flow": data.SmartMoneyFlow,
-		"contribution":     smartScore,
-	}
-	score += smartScore
-
-	// CVD residual component (15 points max)
-	cvdScore := se.scoreCVDResidual(data.CVDResidual)
-	result.ComponentScores["cvd_residual"] = cvdScore
-	result.Attribution["cvd_residual"] = map[string]interface{}{
-		"cvd_residual": data.CVDResidual,
-		"contribution": cvdScore,
-	}
-	score += cvdScore
-
-	return score
-}
-
-// calculateCatalystScore computes catalyst and compression factors (25 points max)
-func (se *ScoreEngine) calculateCatalystScore(data *PreMovementData, result *ScoreResult) float64 {
-	var score float64
-
-	// Catalyst component (15 points max)
-	catalystScore := se.scoreCatalyst(data.CatalystHeat)
-	result.ComponentScores["catalyst"] = catalystScore
-	result.Attribution["catalyst"] = map[string]interface{}{
-		"catalyst_heat": data.CatalystHeat,
-		"contribution":  catalystScore,
-	}
-	score += catalystScore
-
-	// Compression component (10 points max)
-	compressionScore := se.scoreCompression(data.VolCompressionRank)
-	result.ComponentScores["compression"] = compressionScore
-	result.Attribution["compression"] = map[string]interface{}{
-		"vol_compression_rank": data.VolCompressionRank,
-		"contribution":         compressionScore,
-	}
-	score += compressionScore
-
-	return score
-}
-
-// Individual scoring functions implementing Pre-Movement v3.3 logic
-
-func (se *ScoreEngine) scoreDerivatives(fundingZ, oiResidual, etfTint float64) float64 {
-	// Funding z-score contribution (0-7 points)
-	fundingScore := math.Min(7.0, math.Max(0.0, fundingZ*1.5)) // Scale z-score to 0-7 range
-
-	// OI residual contribution (0-4 points) 
-	oiScore := math.Min(4.0, math.Max(0.0, oiResidual/250000.0)) // Scale $1M OI residual = 4 points
-
-	// ETF flow tint contribution (0-4 points)
-	etfScore := math.Min(4.0, math.Max(0.0, etfTint*4.0)) // Scale 0-1 tint to 0-4 points
-
-	return fundingScore + oiScore + etfScore
-}
-
-func (se *ScoreEngine) scoreSupplyDemand(reserveChange, whaleComposite float64) float64 {
-	// Reserve depletion contribution (0-8 points)
-	reserveScore := math.Min(8.0, math.Max(0.0, -reserveChange*0.4)) // -20% reserves = 8 points
-
-	// Whale composite contribution (0-7 points)
-	whaleScore := math.Min(7.0, math.Max(0.0, whaleComposite*7.0)) // Scale 0-1 composite to 0-7 points
-
-	return reserveScore + whaleScore
-}
-
-func (se *ScoreEngine) scoreMicrostructure(microDynamics float64) float64 {
-	// L1/L2 stress contribution (0-10 points)
-	return math.Min(10.0, math.Max(0.0, microDynamics*10.0)) // Scale 0-1 dynamics to 0-10 points
-}
-
-func (se *ScoreEngine) scoreSmartMoney(smartFlow float64) float64 {
-	// Institutional flow patterns (0-20 points)
-	return math.Min(20.0, math.Max(0.0, smartFlow*20.0)) // Scale 0-1 flow to 0-20 points
-}
-
-func (se *ScoreEngine) scoreCVDResidual(cvdResidual float64) float64 {
-	// CVD residual strength (0-15 points)
-	return math.Min(15.0, math.Max(0.0, math.Abs(cvdResidual)*15.0)) // Scale 0-1 residual to 0-15 points
-}
-
-func (se *ScoreEngine) scoreCatalyst(catalystHeat float64) float64 {
-	// News/event significance (0-15 points)
-	return math.Min(15.0, math.Max(0.0, catalystHeat*15.0)) // Scale 0-1 heat to 0-15 points
-}
-
-func (se *ScoreEngine) scoreCompression(compressionRank float64) float64 {
-	// Volatility compression percentile (0-10 points)
-	return math.Min(10.0, math.Max(0.0, compressionRank*10.0)) // Scale 0-1 percentile to 0-10 points
-}
-
-// applyFreshnessPenalty implements "worst feed wins" freshness penalty
-func (se *ScoreEngine) applyFreshnessPenalty(baseScore, oldestFeedHours float64) (*FreshnessInfo, float64) {
-	freshnessInfo := &FreshnessInfo{
-		OldestFeedHours:    oldestFeedHours,
-		FreshnessPenalty:   0.0,
-		AffectedComponents: []string{},
-		WorstFeed:          "unknown",
-	}
-
-	// No penalty if data is fresh (< max freshness hours)
-	if oldestFeedHours <= se.config.MaxFreshnessHours {
-		return freshnessInfo, baseScore
-	}
-
-	// Calculate penalty: linear scale from 0% at max_hours to 20% at 2*max_hours
-	excessHours := oldestFeedHours - se.config.MaxFreshnessHours
-	maxExcessHours := se.config.MaxFreshnessHours // 100% penalty at 2x max_hours
-	
-	penaltyRatio := math.Min(1.0, excessHours/maxExcessHours)
-	freshnessInfo.FreshnessPenalty = penaltyRatio * se.config.FreshnessPenaltyPct / 100.0
-
-	// Apply penalty to final score
-	penalty := baseScore * freshnessInfo.FreshnessPenalty
-	finalScore := baseScore - penalty
-
-	// Track affected components (all components affected by freshness)
-	freshnessInfo.AffectedComponents = []string{"all"}
-
-	return freshnessInfo, finalScore
-}
-
-// normalizeScore ensures score stays within configured bounds
-func (se *ScoreEngine) normalizeScore(score float64) float64 {
-	return math.Min(se.config.MaxScore, math.Max(se.config.MinScore, score))
-}
-
-// GetScoreSummary returns a concise summary of the Pre-Movement score
-func (sr *ScoreResult) GetScoreSummary() string {
-	freshnessNote := ""
-	if sr.DataFreshness.FreshnessPenalty > 0.0 {
-		freshnessNote = fmt.Sprintf(" (-%0.1f%% stale)", sr.DataFreshness.FreshnessPenalty*100)
-	}
-
-	validity := "‚úÖ VALID"
-	if !sr.IsValid {
-		validity = "‚ö†Ô∏è  CHECK"
-	}
-
-	return fmt.Sprintf("%s ‚Äî %s score: %.1f%s (%dms)",
-		validity, sr.Symbol, sr.TotalScore, freshnessNote, sr.EvaluationTimeMs)
-}
-
-// GetDetailedBreakdown returns comprehensive score attribution
-func (sr *ScoreResult) GetDetailedBreakdown() string {
-	report := fmt.Sprintf("Pre-Movement v3.3 Score: %s (%.1f/100)\n", sr.Symbol, sr.TotalScore)
-	report += fmt.Sprintf("Valid: %t | Evaluation: %dms\n\n", sr.IsValid, sr.EvaluationTimeMs)
-
-	// Component breakdown
-	report += "Component Scores:\n"
-	componentOrder := []string{"derivatives", "supply_demand", "microstructure", "smart_money", "cvd_residual", "catalyst", "compression"}
-	
-	for _, component := range componentOrder {
-		if score, exists := sr.ComponentScores[component]; exists {
-			report += fmt.Sprintf("  %s: %.1f pts\n", component, score)
-		}
-	}
-
-	// Freshness penalty
-	if sr.DataFreshness.FreshnessPenalty > 0.0 {
-		report += fmt.Sprintf("\nFreshness Penalty: -%.1f%% (%.1fh old data)\n",
-			sr.DataFreshness.FreshnessPenalty*100, sr.DataFreshness.OldestFeedHours)
-	}
-
-	// Warnings
-	if len(sr.Warnings) > 0 {
-		report += fmt.Sprintf("\nWarnings:\n")
-		for i, warning := range sr.Warnings {
-			report += fmt.Sprintf("  %d. %s\n", i+1, warning)
-		}
-	}
-
-	return report
-}
\ No newline at end of file
+package premove
+
+import (
+	"context"
+	"fmt"
+	"math"
+	"time"
+)
+
+// ScoreEngine implements Pre-Movement v3.3 100-point scoring system
+type ScoreEngine struct {
+	config *ScoreConfig
+}
+
+// NewScoreEngine creates a Pre-Movement v3.3 scoring engine
+func NewScoreEngine(config *ScoreConfig) *ScoreEngine {
+	if config == nil {
+		config = DefaultScoreConfig()
+	}
+	return &ScoreEngine{config: config}
+}
+
+// ScoreConfig contains thresholds and weights for Pre-Movement v3.3 scoring
+type ScoreConfig struct {
+	// Structural Components (0-40 points)
+	DerivativesWeight    float64 `yaml:"derivatives_weight"`    // 15 pts: funding, OI, ETF
+	SupplyDemandWeight   float64 `yaml:"supply_demand_weight"`  // 15 pts: reserves, whale moves
+	MicrostructureWeight float64 `yaml:"microstructure_weight"` // 10 pts: L1/L2 dynamics
+
+	// Behavioral Components (0-35 points)
+	SmartMoneyWeight  float64 `yaml:"smart_money_weight"`  // 20 pts: large tx patterns
+	CVDResidualWeight float64 `yaml:"cvd_residual_weight"` // 15 pts: cumulative volume delta
+
+	// Catalyst & Compression (0-25 points)
+	CatalystWeight    float64 `yaml:"catalyst_weight"`    // 15 pts: news/events
+	CompressionWeight float64 `yaml:"compression_weight"` // 10 pts: volatility compression
+
+	// Freshness penalty parameters
+	MaxFreshnessHours   float64 `yaml:"max_freshness_hours"`   // 2.0 hours
+	FreshnessPenaltyPct float64 `yaml:"freshness_penalty_pct"` // 20% max penalty
+
+	// Score normalization
+	MinScore float64 `yaml:"min_score"` // 0.0
+	MaxScore float64 `yaml:"max_score"` // 100.0
+}
+
+// DefaultScoreConfig returns Pre-Movement v3.3 production configuration
+func DefaultScoreConfig() *ScoreConfig {
+	return &ScoreConfig{
+		// Structural (40 points total)
+		DerivativesWeight:    15.0,
+		SupplyDemandWeight:   15.0,
+		MicrostructureWeight: 10.0,
+
+		// Behavioral (35 points total)
+		SmartMoneyWeight:  20.0,
+		CVDResidualWeight: 15.0,
+
+		// Catalyst & Compression (25 points total)
+		CatalystWeight:    15.0,
+		CompressionWeight: 10.0,
+
+		// Freshness parameters
+		MaxFreshnessHours:   2.0,  // "worst feed wins" rule
+		FreshnessPenaltyPct: 20.0, // max 20% penalty
+
+		// Score bounds
+		MinScore: 0.0,
+		MaxScore: 100.0,
+	}
+}
+
+// PreMovementData contains all inputs for Pre-Movement v3.3 scoring
+type PreMovementData struct {
+	Symbol    string    `json:"symbol"`
+	Timestamp time.Time `json:"timestamp"`
+
+	// Structural factors
+	FundingZScore   float64 `json:"funding_z_score"`   // Cross-venue funding divergence
+	OIResidual      float64 `json:"oi_residual"`       // Open interest anomaly
+	ETFFlowTint     float64 `json:"etf_flow_tint"`     // ETF net flow direction
+	ReserveChange7d float64 `json:"reserve_change_7d"` // Exchange reserves change
+	WhaleComposite  float64 `json:"whale_composite"`   // Large transaction patterns
+	MicroDynamics   float64 `json:"micro_dynamics"`    // L1/L2 order book stress
+
+	// Behavioral factors
+	SmartMoneyFlow float64 `json:"smart_money_flow"` // Institutional flow patterns
+	CVDResidual    float64 `json:"cvd_residual"`     // Volume-price residual
+
+	// Catalyst & compression
+	CatalystHeat       float64 `json:"catalyst_heat"`        // News/event significance
+	VolCompressionRank float64 `json:"vol_compression_rank"` // Volatility compression percentile
+
+	// Data freshness tracking (worst feed wins)
+	OldestFeedHours float64 `json:"oldest_feed_hours"` // Hours since oldest data point
+}
+
+// ScoreResult contains the complete Pre-Movement v3.3 score breakdown
+type ScoreResult struct {
+	Symbol           string                 `json:"symbol"`
+	Timestamp        time.Time              `json:"timestamp"`
+	TotalScore       float64                `json:"total_score"`      // 0-100 final score
+	ComponentScores  map[string]float64     `json:"component_scores"` // Individual component contributions
+	Attribution      map[string]interface{} `json:"attribution"`      // Detailed score attribution
+	DataFreshness    *FreshnessInfo         `json:"data_freshness"`   // Freshness penalty details
+	EvaluationTimeMs int64                  `json:"evaluation_time_ms"`
+	IsValid          bool                   `json:"is_valid"` // Whether score is actionable
+	Warnings         []string               `json:"warnings"` // Data quality warnings
+}
+
+// FreshnessInfo tracks data staleness and penalties applied
+type FreshnessInfo struct {
+	OldestFeedHours    float64  `json:"oldest_feed_hours"`
+	FreshnessPenalty   float64  `json:"freshness_penalty"`   // 0-20% penalty applied
+	AffectedComponents []string `json:"affected_components"` // Components penalized
+	WorstFeed          string   `json:"worst_feed"`          // Name of stalest feed
+}
+
+// CalculateScore computes the complete Pre-Movement v3.3 score
+func (se *ScoreEngine) CalculateScore(ctx context.Context, data *PreMovementData) (*ScoreResult, error) {
+	startTime := time.Now()
+
+	result := &ScoreResult{
+		Symbol:          data.Symbol,
+		Timestamp:       time.Now(),
+		ComponentScores: make(map[string]float64),
+		Attribution:     make(map[string]interface{}),
+		Warnings:        []string{},
+	}
+
+	// Calculate individual component scores
+	structuralScore := se.calculateStructuralScore(data, result)
+	behavioralScore := se.calculateBehavioralScore(data, result)
+	catalystScore := se.calculateCatalystScore(data, result)
+
+	// Base score before freshness penalty
+	baseScore := structuralScore + behavioralScore + catalystScore
+
+	// Apply freshness penalty ("worst feed wins" rule)
+	freshnessInfo, finalScore := se.applyFreshnessPenalty(baseScore, data.OldestFeedHours)
+	result.DataFreshness = freshnessInfo
+
+	// Normalize and bound the score
+	result.TotalScore = se.normalizeScore(finalScore)
+	result.IsValid = result.TotalScore >= 0.0 && len(result.Warnings) == 0
+	result.EvaluationTimeMs = time.Since(startTime).Milliseconds()
+
+	return result, nil
+}
+
+// calculateStructuralScore computes structural factors (40 points max)
+func (se *ScoreEngine) calculateStructuralScore(data *PreMovementData, result *ScoreResult) float64 {
+	var score float64
+
+	// Derivatives component (15 points max)
+	derivScore := se.scoreDerivatives(data.FundingZScore, data.OIResidual, data.ETFFlowTint)
+	result.ComponentScores["derivatives"] = derivScore
+	result.Attribution["derivatives"] = map[string]interface{}{
+		"funding_z_score": data.FundingZScore,
+		"oi_residual":     data.OIResidual,
+		"etf_flow_tint":   data.ETFFlowTint,
+		"contribution":    derivScore,
+	}
+	score += derivScore
+
+	// Supply/demand component (15 points max)
+	supplyScore := se.scoreSupplyDemand(data.ReserveChange7d, data.WhaleComposite)
+	result.ComponentScores["supply_demand"] = supplyScore
+	result.Attribution["supply_demand"] = map[string]interface{}{
+		"reserve_change_7d": data.ReserveChange7d,
+		"whale_composite":   data.WhaleComposite,
+		"contribution":      supplyScore,
+	}
+	score += supplyScore
+
+	// Microstructure component (10 points max)
+	microScore := se.scoreMicrostructure(data.MicroDynamics)
+	result.ComponentScores["microstructure"] = microScore
+	result.Attribution["microstructure"] = map[string]interface{}{
+		"micro_dynamics": data.MicroDynamics,
+		"contribution":   microScore,
+	}
+	score += microScore
+
+	return score
+}
+
+// calculateBehavioralScore computes behavioral factors (35 points max)
+func (se *ScoreEngine) calculateBehavioralScore(data *PreMovementData, result *ScoreResult) float64 {
+	var score float64
+
+	// Smart money component (20 points max)
+	smartScore := se.scoreSmartMoney(data.SmartMoneyFlow)
+	result.ComponentScores["smart_money"] = smartScore
+	result.Attribution["smart_money"] = map[string]interface{}{
+		"smart_money_flow": data.SmartMoneyFlow,
+		"contribution":     smartScore,
+	}
+	score += smartScore
+
+	// CVD residual component (15 points max)
+	cvdScore := se.scoreCVDResidual(data.CVDResidual)
+	result.ComponentScores["cvd_residual"] = cvdScore
+	result.Attribution["cvd_residual"] = map[string]interface{}{
+		"cvd_residual": data.CVDResidual,
+		"contribution": cvdScore,
+	}
+	score += cvdScore
+
+	return score
+}
+
+// calculateCatalystScore computes catalyst and compression factors (25 points max)
+func (se *ScoreEngine) calculateCatalystScore(data *PreMovementData, result *ScoreResult) float64 {
+	var score float64
+
+	// Catalyst component (15 points max)
+	catalystScore := se.scoreCatalyst(data.CatalystHeat)
+	result.ComponentScores["catalyst"] = catalystScore
+	result.Attribution["catalyst"] = map[string]interface{}{
+		"catalyst_heat": data.CatalystHeat,
+		"contribution":  catalystScore,
+	}
+	score += catalystScore
+
+	// Compression component (10 points max)
+	compressionScore := se.scoreCompression(data.VolCompressionRank)
+	result.ComponentScores["compression"] = compressionScore
+	result.Attribution["compression"] = map[string]interface{}{
+		"vol_compression_rank": data.VolCompressionRank,
+		"contribution":         compressionScore,
+	}
+	score += compressionScore
+
+	return score
+}
+
+// Individual scoring functions implementing Pre-Movement v3.3 logic
+
+func (se *ScoreEngine) scoreDerivatives(fundingZ, oiResidual, etfTint float64) float64 {
+	// Funding z-score contribution (0-7 points)
+	fundingScore := math.Min(7.0, math.Max(0.0, fundingZ*1.5)) // Scale z-score to 0-7 range
+
+	// OI residual contribution (0-4 points)
+	oiScore := math.Min(4.0, math.Max(0.0, oiResidual/250000.0)) // Scale $1M OI residual = 4 points
+
+	// ETF flow tint contribution (0-4 points)
+	etfScore := math.Min(4.0, math.Max(0.0, etfTint*4.0)) // Scale 0-1 tint to 0-4 points
+
+	return fundingScore + oiScore + etfScore
+}
+
+func (se *ScoreEngine) scoreSupplyDemand(reserveChange, whaleComposite float64) float64 {
+	// Reserve depletion contribution (0-8 points)
+	reserveScore := math.Min(8.0, math.Max(0.0, -reserveChange*0.4)) // -20% reserves = 8 points
+
+	// Whale composite contribution (0-7 points)
+	whaleScore := math.Min(7.0, math.Max(0.0, whaleComposite*7.0)) // Scale 0-1 composite to 0-7 points
+
+	return reserveScore + whaleScore
+}
+
+func (se *ScoreEngine) scoreMicrostructure(microDynamics float64) float64 {
+	// L1/L2 stress contribution (0-10 points)
+	return math.Min(10.0, math.Max(0.0, microDynamics*10.0)) // Scale 0-1 dynamics to 0-10 points
+}
+
+func (se *ScoreEngine) scoreSmartMoney(smartFlow float64) float64 {
+	// Institutional flow patterns (0-20 points)
+	return math.Min(20.0, math.Max(0.0, smartFlow*20.0)) // Scale 0-1 flow to 0-20 points
+}
+
+func (se *ScoreEngine) scoreCVDResidual(cvdResidual float64) float64 {
+	// CVD residual strength (0-15 points)
+	return math.Min(15.0, math.Max(0.0, math.Abs(cvdResidual)*15.0)) // Scale 0-1 residual to 0-15 points
+}
+
+func (se *ScoreEngine) scoreCatalyst(catalystHeat float64) float64 {
+	// News/event significance (0-15 points)
+	return math.Min(15.0, math.Max(0.0, catalystHeat*15.0)) // Scale 0-1 heat to 0-15 points
+}
+
+func (se *ScoreEngine) scoreCompression(compressionRank float64) float64 {
+	// Volatility compression percentile (0-10 points)
+	return math.Min(10.0, math.Max(0.0, compressionRank*10.0)) // Scale 0-1 percentile to 0-10 points
+}
+
+// applyFreshnessPenalty implements "worst feed wins" freshness penalty
+func (se *ScoreEngine) applyFreshnessPenalty(baseScore, oldestFeedHours float64) (*FreshnessInfo, float64) {
+	freshnessInfo := &FreshnessInfo{
+		OldestFeedHours:    oldestFeedHours,
+		FreshnessPenalty:   0.0,
+		AffectedComponents: []string{},
+		WorstFeed:          "unknown",
+	}
+
+	// No penalty if data is fresh (< max freshness hours)
+	if oldestFeedHours <= se.config.MaxFreshnessHours {
+		return freshnessInfo, baseScore
+	}
+
+	// Calculate penalty: linear scale from 0% at max_hours to 20% at 2*max_hours
+	excessHours := oldestFeedHours - se.config.MaxFreshnessHours
+	maxExcessHours := se.config.MaxFreshnessHours // 100% penalty at 2x max_hours
+
+	penaltyRatio := math.Min(1.0, excessHours/maxExcessHours)
+	freshnessInfo.FreshnessPenalty = penaltyRatio * se.config.FreshnessPenaltyPct / 100.0
+
+	// Apply penalty to final score
+	penalty := baseScore * freshnessInfo.FreshnessPenalty
+	finalScore := baseScore - penalty
+
+	// Track affected components (all components affected by freshness)
+	freshnessInfo.AffectedComponents = []string{"all"}
+
+	return freshnessInfo, finalScore
+}
+
+// normalizeScore ensures score stays within configured bounds
+func (se *ScoreEngine) normalizeScore(score float64) float64 {
+	return math.Min(se.config.MaxScore, math.Max(se.config.MinScore, score))
+}
+
+// GetScoreSummary returns a concise summary of the Pre-Movement score
+func (sr *ScoreResult) GetScoreSummary() string {
+	freshnessNote := ""
+	if sr.DataFreshness.FreshnessPenalty > 0.0 {
+		freshnessNote = fmt.Sprintf(" (-%0.1f%% stale)", sr.DataFreshness.FreshnessPenalty*100)
+	}
+
+	validity := "‚úÖ VALID"
+	if !sr.IsValid {
+		validity = "‚ö†Ô∏è  CHECK"
+	}
+
+	return fmt.Sprintf("%s ‚Äî %s score: %.1f%s (%dms)",
+		validity, sr.Symbol, sr.TotalScore, freshnessNote, sr.EvaluationTimeMs)
+}
+
+// GetDetailedBreakdown returns comprehensive score attribution
+func (sr *ScoreResult) GetDetailedBreakdown() string {
+	report := fmt.Sprintf("Pre-Movement v3.3 Score: %s (%.1f/100)\n", sr.Symbol, sr.TotalScore)
+	report += fmt.Sprintf("Valid: %t | Evaluation: %dms\n\n", sr.IsValid, sr.EvaluationTimeMs)
+
+	// Component breakdown
+	report += "Component Scores:\n"
+	componentOrder := []string{"derivatives", "supply_demand", "microstructure", "smart_money", "cvd_residual", "catalyst", "compression"}
+
+	for _, component := range componentOrder {
+		if score, exists := sr.ComponentScores[component]; exists {
+			report += fmt.Sprintf("  %s: %.1f pts\n", component, score)
+		}
+	}
+
+	// Freshness penalty
+	if sr.DataFreshness.FreshnessPenalty > 0.0 {
+		report += fmt.Sprintf("\nFreshness Penalty: -%.1f%% (%.1fh old data)\n",
+			sr.DataFreshness.FreshnessPenalty*100, sr.DataFreshness.OldestFeedHours)
+	}
+
+	// Warnings
+	if len(sr.Warnings) > 0 {
+		report += fmt.Sprintf("\nWarnings:\n")
+		for i, warning := range sr.Warnings {
+			report += fmt.Sprintf("  %d. %s\n", i+1, warning)
+		}
+	}
+
+	return report
+}
diff --git a/internal/premove/score_test.go b/internal/premove/score_test.go
index 297e882..ec5a34f 100644
--- a/internal/premove/score_test.go
+++ b/internal/premove/score_test.go
@@ -1,319 +1,319 @@
-package premove
-
-import (
-	"context"
-	"testing"
-	"time"
-
-	"github.com/stretchr/testify/assert"
-	"github.com/stretchr/testify/require"
-)
-
-func TestScoreEngine_CalculateScore_AllComponents(t *testing.T) {
-	engine := NewScoreEngine(nil) // Use default config
-
-	data := &PreMovementData{
-		Symbol:    "BTC-USD",
-		Timestamp: time.Now(),
-
-		// Strong structural signals
-		FundingZScore:      3.5,  // Strong funding divergence
-		OIResidual:         1.2e6, // $1.2M OI residual
-		ETFFlowTint:        0.8,  // 80% bullish flows
-		ReserveChange7d:    -15.0, // -15% exchange reserves
-		WhaleComposite:     0.9,  // 90% whale activity
-		MicroDynamics:      0.7,  // 70% L1/L2 stress
-
-		// Strong behavioral signals
-		SmartMoneyFlow:     0.85, // 85% institutional flow
-		CVDResidual:        0.6,  // 60% CVD residual
-
-		// Strong catalyst & compression
-		CatalystHeat:       0.9,  // 90% catalyst significance
-		VolCompressionRank: 0.95, // 95th percentile compression
-
-		// Fresh data
-		OldestFeedHours: 0.5, // 30 minutes
-	}
-
-	result, err := engine.CalculateScore(context.Background(), data)
-	require.NoError(t, err)
-	assert.NotNil(t, result)
-
-	// Should achieve very high score with all strong signals
-	assert.Greater(t, result.TotalScore, 90.0, "Strong signals should yield >90 score")
-	assert.LessOrEqual(t, result.TotalScore, 100.0, "Score should not exceed 100")
-	assert.True(t, result.IsValid, "Score should be valid")
-
-	// Verify component scores are present
-	assert.Contains(t, result.ComponentScores, "derivatives")
-	assert.Contains(t, result.ComponentScores, "supply_demand")
-	assert.Contains(t, result.ComponentScores, "microstructure")
-	assert.Contains(t, result.ComponentScores, "smart_money")
-	assert.Contains(t, result.ComponentScores, "cvd_residual")
-	assert.Contains(t, result.ComponentScores, "catalyst")
-	assert.Contains(t, result.ComponentScores, "compression")
-
-	// Check individual component contributions
-	derivScore := result.ComponentScores["derivatives"]
-	assert.Greater(t, derivScore, 10.0, "Strong derivatives should contribute >10 points")
-	assert.LessOrEqual(t, derivScore, 15.0, "Derivatives capped at 15 points")
-
-	// Verify attribution data is present
-	assert.Contains(t, result.Attribution, "derivatives")
-	assert.Contains(t, result.Attribution, "smart_money")
-
-	// Check freshness penalty (should be minimal)
-	assert.NotNil(t, result.DataFreshness)
-	assert.LessOrEqual(t, result.DataFreshness.FreshnessPenalty, 0.1, "Fresh data should have minimal penalty")
-}
-
-func TestScoreEngine_CalculateScore_WeakSignals(t *testing.T) {
-	engine := NewScoreEngine(nil)
-
-	data := &PreMovementData{
-		Symbol:    "ETH-USD",
-		Timestamp: time.Now(),
-
-		// Weak structural signals
-		FundingZScore:      0.5,  // Below 2.0 threshold
-		OIResidual:         50000, // $50k OI residual
-		ETFFlowTint:        0.1,  // 10% flows
-		ReserveChange7d:    -1.0, // Minor reserve change
-		WhaleComposite:     0.2,  // 20% whale activity
-		MicroDynamics:      0.1,  // Low L1/L2 stress
-
-		// Weak behavioral signals  
-		SmartMoneyFlow:     0.15, // 15% institutional flow
-		CVDResidual:        0.05, // 5% CVD residual
-
-		// Weak catalyst & compression
-		CatalystHeat:       0.1,  // 10% catalyst heat
-		VolCompressionRank: 0.2,  // 20th percentile
-
-		// Fresh data
-		OldestFeedHours: 1.0,
-	}
-
-	result, err := engine.CalculateScore(context.Background(), data)
-	require.NoError(t, err)
-
-	// Should yield low score with weak signals
-	assert.Less(t, result.TotalScore, 30.0, "Weak signals should yield <30 score")
-	assert.GreaterOrEqual(t, result.TotalScore, 0.0, "Score should not be negative")
-}
-
-func TestScoreEngine_FreshnessPenalty_StaleData(t *testing.T) {
-	engine := NewScoreEngine(nil)
-
-	data := &PreMovementData{
-		Symbol:    "SOL-USD",
-		Timestamp: time.Now(),
-
-		// Moderate signals that would normally score ~60 points
-		FundingZScore:      2.5,
-		OIResidual:         800000,
-		ETFFlowTint:        0.6,
-		ReserveChange7d:    -8.0,
-		WhaleComposite:     0.6,
-		MicroDynamics:      0.5,
-		SmartMoneyFlow:     0.5,
-		CVDResidual:        0.4,
-		CatalystHeat:       0.5,
-		VolCompressionRank: 0.6,
-
-		// Very stale data (4 hours old)
-		OldestFeedHours: 4.0,
-	}
-
-	result, err := engine.CalculateScore(context.Background(), data)
-	require.NoError(t, err)
-
-	// Should apply significant freshness penalty
-	assert.NotNil(t, result.DataFreshness)
-	assert.Greater(t, result.DataFreshness.FreshnessPenalty, 0.15, "Stale data should have >15% penalty")
-	assert.Equal(t, 4.0, result.DataFreshness.OldestFeedHours)
-
-	// Final score should be meaningfully reduced
-	// With 4hr old data (2x max freshness), expect ~20% penalty
-	assert.Less(t, result.TotalScore, 50.0, "Freshness penalty should reduce score significantly")
-}
-
-func TestScoreEngine_ComponentBounds(t *testing.T) {
-	engine := NewScoreEngine(nil)
-
-	// Test extreme values to verify component bounds
-	data := &PreMovementData{
-		Symbol:    "EXTREME-TEST",
-		Timestamp: time.Now(),
-
-		// Extreme positive values
-		FundingZScore:      100.0, // Should cap at ~7 points
-		OIResidual:         1e9,   // $1B OI - should cap at 4 points
-		ETFFlowTint:        5.0,   // 500% - should cap at 4 points
-		ReserveChange7d:    -100.0, // -100% reserves - should cap at 8 points
-		WhaleComposite:     10.0,  // 1000% - should cap at 7 points
-		MicroDynamics:      5.0,   // 500% - should cap at 10 points
-		SmartMoneyFlow:     5.0,   // 500% - should cap at 20 points
-		CVDResidual:        10.0,  // 1000% - should cap at 15 points
-		CatalystHeat:       5.0,   // 500% - should cap at 15 points
-		VolCompressionRank: 5.0,   // 500% - should cap at 10 points
-
-		OldestFeedHours: 0.1, // Fresh
-	}
-
-	result, err := engine.CalculateScore(context.Background(), data)
-	require.NoError(t, err)
-
-	// Total should be capped at 100
-	assert.LessOrEqual(t, result.TotalScore, 100.0, "Score should be capped at 100")
-
-	// Individual components should respect their bounds
-	assert.LessOrEqual(t, result.ComponentScores["derivatives"], 15.0)
-	assert.LessOrEqual(t, result.ComponentScores["supply_demand"], 15.0)
-	assert.LessOrEqual(t, result.ComponentScores["microstructure"], 10.0)
-	assert.LessOrEqual(t, result.ComponentScores["smart_money"], 20.0)
-	assert.LessOrEqual(t, result.ComponentScores["cvd_residual"], 15.0)
-	assert.LessOrEqual(t, result.ComponentScores["catalyst"], 15.0)
-	assert.LessOrEqual(t, result.ComponentScores["compression"], 10.0)
-}
-
-func TestScoreEngine_NegativeValues(t *testing.T) {
-	engine := NewScoreEngine(nil)
-
-	data := &PreMovementData{
-		Symbol:    "NEGATIVE-TEST",
-		Timestamp: time.Now(),
-
-		// Negative values (should be handled gracefully)
-		FundingZScore:      -2.0,  // Negative funding divergence
-		OIResidual:         -50000, // Negative OI residual
-		ETFFlowTint:        -0.5,  // -50% flows (bearish)
-		ReserveChange7d:    10.0,  // Positive reserves (supply increase)
-		WhaleComposite:     -0.3,  // Negative whale activity
-		MicroDynamics:      -0.2,  // Negative dynamics
-		SmartMoneyFlow:     -0.4,  // Outflows
-		CVDResidual:        -0.8,  // Negative CVD residual
-		CatalystHeat:       -0.2,  // Negative catalyst
-		VolCompressionRank: -0.1,  // Negative compression
-
-		OldestFeedHours: 1.0,
-	}
-
-	result, err := engine.CalculateScore(context.Background(), data)
-	require.NoError(t, err)
-
-	// Score should not be negative (components should floor at 0)
-	assert.GreaterOrEqual(t, result.TotalScore, 0.0, "Score should not be negative")
-
-	// Most component scores should be 0 or very low
-	for component, score := range result.ComponentScores {
-		assert.GreaterOrEqual(t, score, 0.0, "Component %s should not be negative", component)
-	}
-}
-
-func TestScoreEngine_GetScoreSummary(t *testing.T) {
-	result := &ScoreResult{
-		Symbol:     "BTC-USD",
-		TotalScore: 87.5,
-		IsValid:    true,
-		DataFreshness: &FreshnessInfo{
-			FreshnessPenalty: 0.05, // 5% penalty
-			OldestFeedHours:  2.5,
-		},
-		EvaluationTimeMs: 45,
-	}
-
-	summary := result.GetScoreSummary()
-	assert.Contains(t, summary, "BTC-USD")
-	assert.Contains(t, summary, "87.5")
-	assert.Contains(t, summary, "‚úÖ VALID")
-	assert.Contains(t, summary, "45ms")
-}
-
-func TestScoreEngine_GetDetailedBreakdown(t *testing.T) {
-	result := &ScoreResult{
-		Symbol:     "ETH-USD", 
-		TotalScore: 72.3,
-		IsValid:    true,
-		ComponentScores: map[string]float64{
-			"derivatives":    12.5,
-			"supply_demand":  8.2,
-			"microstructure": 6.1,
-			"smart_money":    15.8,
-			"cvd_residual":   11.3,
-			"catalyst":       10.4,
-			"compression":    8.0,
-		},
-		DataFreshness: &FreshnessInfo{
-			FreshnessPenalty: 0.12,
-			OldestFeedHours:  3.2,
-		},
-		EvaluationTimeMs: 67,
-		Warnings:         []string{"CVD data quality degraded"},
-	}
-
-	breakdown := result.GetDetailedBreakdown()
-	assert.Contains(t, breakdown, "ETH-USD")
-	assert.Contains(t, breakdown, "72.3/100")
-	assert.Contains(t, breakdown, "derivatives: 12.5 pts")
-	assert.Contains(t, breakdown, "Freshness Penalty: -12.0%")
-	assert.Contains(t, breakdown, "CVD data quality degraded")
-}
-
-func TestDefaultScoreConfig(t *testing.T) {
-	config := DefaultScoreConfig()
-	require.NotNil(t, config)
-
-	// Check weights sum to 100 points
-	totalWeight := config.DerivativesWeight + config.SupplyDemandWeight + config.MicrostructureWeight +
-		config.SmartMoneyWeight + config.CVDResidualWeight + config.CatalystWeight + config.CompressionWeight
-
-	assert.Equal(t, 100.0, totalWeight, "Component weights should sum to 100 points")
-
-	// Check structural components total 40 points
-	structuralTotal := config.DerivativesWeight + config.SupplyDemandWeight + config.MicrostructureWeight
-	assert.Equal(t, 40.0, structuralTotal, "Structural components should total 40 points")
-
-	// Check behavioral components total 35 points  
-	behavioralTotal := config.SmartMoneyWeight + config.CVDResidualWeight
-	assert.Equal(t, 35.0, behavioralTotal, "Behavioral components should total 35 points")
-
-	// Check catalyst & compression total 25 points
-	catalystTotal := config.CatalystWeight + config.CompressionWeight
-	assert.Equal(t, 25.0, catalystTotal, "Catalyst & compression should total 25 points")
-
-	// Check freshness parameters
-	assert.Equal(t, 2.0, config.MaxFreshnessHours, "Max freshness should be 2 hours")
-	assert.Equal(t, 20.0, config.FreshnessPenaltyPct, "Max penalty should be 20%")
-}
-
-func TestScoreEngine_PerformanceRequirements(t *testing.T) {
-	engine := NewScoreEngine(nil)
-
-	data := &PreMovementData{
-		Symbol:    "PERFORMANCE-TEST",
-		Timestamp: time.Now(),
-		FundingZScore:      2.5,
-		OIResidual:         500000,
-		ETFFlowTint:        0.6,
-		ReserveChange7d:    -7.0,
-		WhaleComposite:     0.7,
-		MicroDynamics:      0.5,
-		SmartMoneyFlow:     0.6,
-		CVDResidual:        0.4,
-		CatalystHeat:       0.7,
-		VolCompressionRank: 0.8,
-		OldestFeedHours:    1.5,
-	}
-
-	// Test performance (should complete quickly)
-	start := time.Now()
-	result, err := engine.CalculateScore(context.Background(), data)
-	duration := time.Since(start)
-
-	require.NoError(t, err)
-	assert.NotNil(t, result)
-	assert.Less(t, duration.Milliseconds(), int64(50), "Score calculation should complete in <50ms")
-	assert.Greater(t, result.EvaluationTimeMs, int64(0), "Should report evaluation time")
-}
\ No newline at end of file
+package premove
+
+import (
+	"context"
+	"testing"
+	"time"
+
+	"github.com/stretchr/testify/assert"
+	"github.com/stretchr/testify/require"
+)
+
+func TestScoreEngine_CalculateScore_AllComponents(t *testing.T) {
+	engine := NewScoreEngine(nil) // Use default config
+
+	data := &PreMovementData{
+		Symbol:    "BTC-USD",
+		Timestamp: time.Now(),
+
+		// Strong structural signals
+		FundingZScore:   3.5,   // Strong funding divergence
+		OIResidual:      1.2e6, // $1.2M OI residual
+		ETFFlowTint:     0.8,   // 80% bullish flows
+		ReserveChange7d: -15.0, // -15% exchange reserves
+		WhaleComposite:  0.9,   // 90% whale activity
+		MicroDynamics:   0.7,   // 70% L1/L2 stress
+
+		// Strong behavioral signals
+		SmartMoneyFlow: 0.85, // 85% institutional flow
+		CVDResidual:    0.6,  // 60% CVD residual
+
+		// Strong catalyst & compression
+		CatalystHeat:       0.9,  // 90% catalyst significance
+		VolCompressionRank: 0.95, // 95th percentile compression
+
+		// Fresh data
+		OldestFeedHours: 0.5, // 30 minutes
+	}
+
+	result, err := engine.CalculateScore(context.Background(), data)
+	require.NoError(t, err)
+	assert.NotNil(t, result)
+
+	// Should achieve very high score with all strong signals
+	assert.Greater(t, result.TotalScore, 90.0, "Strong signals should yield >90 score")
+	assert.LessOrEqual(t, result.TotalScore, 100.0, "Score should not exceed 100")
+	assert.True(t, result.IsValid, "Score should be valid")
+
+	// Verify component scores are present
+	assert.Contains(t, result.ComponentScores, "derivatives")
+	assert.Contains(t, result.ComponentScores, "supply_demand")
+	assert.Contains(t, result.ComponentScores, "microstructure")
+	assert.Contains(t, result.ComponentScores, "smart_money")
+	assert.Contains(t, result.ComponentScores, "cvd_residual")
+	assert.Contains(t, result.ComponentScores, "catalyst")
+	assert.Contains(t, result.ComponentScores, "compression")
+
+	// Check individual component contributions
+	derivScore := result.ComponentScores["derivatives"]
+	assert.Greater(t, derivScore, 10.0, "Strong derivatives should contribute >10 points")
+	assert.LessOrEqual(t, derivScore, 15.0, "Derivatives capped at 15 points")
+
+	// Verify attribution data is present
+	assert.Contains(t, result.Attribution, "derivatives")
+	assert.Contains(t, result.Attribution, "smart_money")
+
+	// Check freshness penalty (should be minimal)
+	assert.NotNil(t, result.DataFreshness)
+	assert.LessOrEqual(t, result.DataFreshness.FreshnessPenalty, 0.1, "Fresh data should have minimal penalty")
+}
+
+func TestScoreEngine_CalculateScore_WeakSignals(t *testing.T) {
+	engine := NewScoreEngine(nil)
+
+	data := &PreMovementData{
+		Symbol:    "ETH-USD",
+		Timestamp: time.Now(),
+
+		// Weak structural signals
+		FundingZScore:   0.5,   // Below 2.0 threshold
+		OIResidual:      50000, // $50k OI residual
+		ETFFlowTint:     0.1,   // 10% flows
+		ReserveChange7d: -1.0,  // Minor reserve change
+		WhaleComposite:  0.2,   // 20% whale activity
+		MicroDynamics:   0.1,   // Low L1/L2 stress
+
+		// Weak behavioral signals
+		SmartMoneyFlow: 0.15, // 15% institutional flow
+		CVDResidual:    0.05, // 5% CVD residual
+
+		// Weak catalyst & compression
+		CatalystHeat:       0.1, // 10% catalyst heat
+		VolCompressionRank: 0.2, // 20th percentile
+
+		// Fresh data
+		OldestFeedHours: 1.0,
+	}
+
+	result, err := engine.CalculateScore(context.Background(), data)
+	require.NoError(t, err)
+
+	// Should yield low score with weak signals
+	assert.Less(t, result.TotalScore, 30.0, "Weak signals should yield <30 score")
+	assert.GreaterOrEqual(t, result.TotalScore, 0.0, "Score should not be negative")
+}
+
+func TestScoreEngine_FreshnessPenalty_StaleData(t *testing.T) {
+	engine := NewScoreEngine(nil)
+
+	data := &PreMovementData{
+		Symbol:    "SOL-USD",
+		Timestamp: time.Now(),
+
+		// Moderate signals that would normally score ~60 points
+		FundingZScore:      2.5,
+		OIResidual:         800000,
+		ETFFlowTint:        0.6,
+		ReserveChange7d:    -8.0,
+		WhaleComposite:     0.6,
+		MicroDynamics:      0.5,
+		SmartMoneyFlow:     0.5,
+		CVDResidual:        0.4,
+		CatalystHeat:       0.5,
+		VolCompressionRank: 0.6,
+
+		// Very stale data (4 hours old)
+		OldestFeedHours: 4.0,
+	}
+
+	result, err := engine.CalculateScore(context.Background(), data)
+	require.NoError(t, err)
+
+	// Should apply significant freshness penalty
+	assert.NotNil(t, result.DataFreshness)
+	assert.Greater(t, result.DataFreshness.FreshnessPenalty, 0.15, "Stale data should have >15% penalty")
+	assert.Equal(t, 4.0, result.DataFreshness.OldestFeedHours)
+
+	// Final score should be meaningfully reduced
+	// With 4hr old data (2x max freshness), expect ~20% penalty
+	assert.Less(t, result.TotalScore, 50.0, "Freshness penalty should reduce score significantly")
+}
+
+func TestScoreEngine_ComponentBounds(t *testing.T) {
+	engine := NewScoreEngine(nil)
+
+	// Test extreme values to verify component bounds
+	data := &PreMovementData{
+		Symbol:    "EXTREME-TEST",
+		Timestamp: time.Now(),
+
+		// Extreme positive values
+		FundingZScore:      100.0,  // Should cap at ~7 points
+		OIResidual:         1e9,    // $1B OI - should cap at 4 points
+		ETFFlowTint:        5.0,    // 500% - should cap at 4 points
+		ReserveChange7d:    -100.0, // -100% reserves - should cap at 8 points
+		WhaleComposite:     10.0,   // 1000% - should cap at 7 points
+		MicroDynamics:      5.0,    // 500% - should cap at 10 points
+		SmartMoneyFlow:     5.0,    // 500% - should cap at 20 points
+		CVDResidual:        10.0,   // 1000% - should cap at 15 points
+		CatalystHeat:       5.0,    // 500% - should cap at 15 points
+		VolCompressionRank: 5.0,    // 500% - should cap at 10 points
+
+		OldestFeedHours: 0.1, // Fresh
+	}
+
+	result, err := engine.CalculateScore(context.Background(), data)
+	require.NoError(t, err)
+
+	// Total should be capped at 100
+	assert.LessOrEqual(t, result.TotalScore, 100.0, "Score should be capped at 100")
+
+	// Individual components should respect their bounds
+	assert.LessOrEqual(t, result.ComponentScores["derivatives"], 15.0)
+	assert.LessOrEqual(t, result.ComponentScores["supply_demand"], 15.0)
+	assert.LessOrEqual(t, result.ComponentScores["microstructure"], 10.0)
+	assert.LessOrEqual(t, result.ComponentScores["smart_money"], 20.0)
+	assert.LessOrEqual(t, result.ComponentScores["cvd_residual"], 15.0)
+	assert.LessOrEqual(t, result.ComponentScores["catalyst"], 15.0)
+	assert.LessOrEqual(t, result.ComponentScores["compression"], 10.0)
+}
+
+func TestScoreEngine_NegativeValues(t *testing.T) {
+	engine := NewScoreEngine(nil)
+
+	data := &PreMovementData{
+		Symbol:    "NEGATIVE-TEST",
+		Timestamp: time.Now(),
+
+		// Negative values (should be handled gracefully)
+		FundingZScore:      -2.0,   // Negative funding divergence
+		OIResidual:         -50000, // Negative OI residual
+		ETFFlowTint:        -0.5,   // -50% flows (bearish)
+		ReserveChange7d:    10.0,   // Positive reserves (supply increase)
+		WhaleComposite:     -0.3,   // Negative whale activity
+		MicroDynamics:      -0.2,   // Negative dynamics
+		SmartMoneyFlow:     -0.4,   // Outflows
+		CVDResidual:        -0.8,   // Negative CVD residual
+		CatalystHeat:       -0.2,   // Negative catalyst
+		VolCompressionRank: -0.1,   // Negative compression
+
+		OldestFeedHours: 1.0,
+	}
+
+	result, err := engine.CalculateScore(context.Background(), data)
+	require.NoError(t, err)
+
+	// Score should not be negative (components should floor at 0)
+	assert.GreaterOrEqual(t, result.TotalScore, 0.0, "Score should not be negative")
+
+	// Most component scores should be 0 or very low
+	for component, score := range result.ComponentScores {
+		assert.GreaterOrEqual(t, score, 0.0, "Component %s should not be negative", component)
+	}
+}
+
+func TestScoreEngine_GetScoreSummary(t *testing.T) {
+	result := &ScoreResult{
+		Symbol:     "BTC-USD",
+		TotalScore: 87.5,
+		IsValid:    true,
+		DataFreshness: &FreshnessInfo{
+			FreshnessPenalty: 0.05, // 5% penalty
+			OldestFeedHours:  2.5,
+		},
+		EvaluationTimeMs: 45,
+	}
+
+	summary := result.GetScoreSummary()
+	assert.Contains(t, summary, "BTC-USD")
+	assert.Contains(t, summary, "87.5")
+	assert.Contains(t, summary, "‚úÖ VALID")
+	assert.Contains(t, summary, "45ms")
+}
+
+func TestScoreEngine_GetDetailedBreakdown(t *testing.T) {
+	result := &ScoreResult{
+		Symbol:     "ETH-USD",
+		TotalScore: 72.3,
+		IsValid:    true,
+		ComponentScores: map[string]float64{
+			"derivatives":    12.5,
+			"supply_demand":  8.2,
+			"microstructure": 6.1,
+			"smart_money":    15.8,
+			"cvd_residual":   11.3,
+			"catalyst":       10.4,
+			"compression":    8.0,
+		},
+		DataFreshness: &FreshnessInfo{
+			FreshnessPenalty: 0.12,
+			OldestFeedHours:  3.2,
+		},
+		EvaluationTimeMs: 67,
+		Warnings:         []string{"CVD data quality degraded"},
+	}
+
+	breakdown := result.GetDetailedBreakdown()
+	assert.Contains(t, breakdown, "ETH-USD")
+	assert.Contains(t, breakdown, "72.3/100")
+	assert.Contains(t, breakdown, "derivatives: 12.5 pts")
+	assert.Contains(t, breakdown, "Freshness Penalty: -12.0%")
+	assert.Contains(t, breakdown, "CVD data quality degraded")
+}
+
+func TestDefaultScoreConfig(t *testing.T) {
+	config := DefaultScoreConfig()
+	require.NotNil(t, config)
+
+	// Check weights sum to 100 points
+	totalWeight := config.DerivativesWeight + config.SupplyDemandWeight + config.MicrostructureWeight +
+		config.SmartMoneyWeight + config.CVDResidualWeight + config.CatalystWeight + config.CompressionWeight
+
+	assert.Equal(t, 100.0, totalWeight, "Component weights should sum to 100 points")
+
+	// Check structural components total 40 points
+	structuralTotal := config.DerivativesWeight + config.SupplyDemandWeight + config.MicrostructureWeight
+	assert.Equal(t, 40.0, structuralTotal, "Structural components should total 40 points")
+
+	// Check behavioral components total 35 points
+	behavioralTotal := config.SmartMoneyWeight + config.CVDResidualWeight
+	assert.Equal(t, 35.0, behavioralTotal, "Behavioral components should total 35 points")
+
+	// Check catalyst & compression total 25 points
+	catalystTotal := config.CatalystWeight + config.CompressionWeight
+	assert.Equal(t, 25.0, catalystTotal, "Catalyst & compression should total 25 points")
+
+	// Check freshness parameters
+	assert.Equal(t, 2.0, config.MaxFreshnessHours, "Max freshness should be 2 hours")
+	assert.Equal(t, 20.0, config.FreshnessPenaltyPct, "Max penalty should be 20%")
+}
+
+func TestScoreEngine_PerformanceRequirements(t *testing.T) {
+	engine := NewScoreEngine(nil)
+
+	data := &PreMovementData{
+		Symbol:             "PERFORMANCE-TEST",
+		Timestamp:          time.Now(),
+		FundingZScore:      2.5,
+		OIResidual:         500000,
+		ETFFlowTint:        0.6,
+		ReserveChange7d:    -7.0,
+		WhaleComposite:     0.7,
+		MicroDynamics:      0.5,
+		SmartMoneyFlow:     0.6,
+		CVDResidual:        0.4,
+		CatalystHeat:       0.7,
+		VolCompressionRank: 0.8,
+		OldestFeedHours:    1.5,
+	}
+
+	// Test performance (should complete quickly)
+	start := time.Now()
+	result, err := engine.CalculateScore(context.Background(), data)
+	duration := time.Since(start)
+
+	require.NoError(t, err)
+	assert.NotNil(t, result)
+	assert.Less(t, duration.Milliseconds(), int64(50), "Score calculation should complete in <50ms")
+	assert.Greater(t, result.EvaluationTimeMs, int64(0), "Should report evaluation time")
+}
diff --git a/internal/regime/api.go b/internal/regime/api.go
index 6bce5a3..e31f566 100644
--- a/internal/regime/api.go
+++ b/internal/regime/api.go
@@ -1,305 +1,305 @@
-package regime
-
-import (
-	"context"
-	"fmt"
-	"sync"
-	"time"
-)
-
-// API provides the main interface for regime detection and weight management
-type API struct {
-	detector      *Detector
-	weightManager *WeightManager
-	mu            sync.RWMutex
-	lastCheck     time.Time
-	updateTimer   *time.Timer
-	isRunning     bool
-}
-
-// NewAPI creates a new regime detection API
-func NewAPI(inputs DetectorInputs) *API {
-	detector := NewDetector(inputs)
-	weightManager := NewWeightManager()
-	
-	return &API{
-		detector:      detector,
-		weightManager: weightManager,
-	}
-}
-
-// NewAPIWithConfig creates an API with custom configuration
-func NewAPIWithConfig(inputs DetectorInputs, config DetectorConfig) *API {
-	detector := NewDetectorWithConfig(inputs, config)
-	weightManager := NewWeightManager()
-	
-	return &API{
-		detector:      detector,
-		weightManager: weightManager,
-	}
-}
-
-// Start begins the 4-hour regime detection cycle
-func (api *API) Start(ctx context.Context) error {
-	api.mu.Lock()
-	defer api.mu.Unlock()
-	
-	if api.isRunning {
-		return fmt.Errorf("regime API is already running")
-	}
-	
-	// Initial detection
-	result, err := api.detector.DetectRegime(ctx)
-	if err != nil {
-		return fmt.Errorf("initial regime detection failed: %w", err)
-	}
-	
-	// Update weights based on initial detection
-	api.weightManager.UpdateCurrentRegime(result)
-	api.lastCheck = time.Now()
-	api.isRunning = true
-	
-	// Schedule next update
-	api.scheduleNextUpdate(ctx)
-	
-	return nil
-}
-
-// Stop halts the regime detection cycle
-func (api *API) Stop() {
-	api.mu.Lock()
-	defer api.mu.Unlock()
-	
-	if api.updateTimer != nil {
-		api.updateTimer.Stop()
-		api.updateTimer = nil
-	}
-	
-	api.isRunning = false
-}
-
-// GetActiveWeights returns the current regime's factor weights
-func (api *API) GetActiveWeights() *WeightPreset {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	return api.weightManager.GetActiveWeights()
-}
-
-// GetCurrentRegime returns the currently detected regime
-func (api *API) GetCurrentRegime(ctx context.Context) (Regime, error) {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	return api.detector.GetCurrentRegime(ctx)
-}
-
-// ForceUpdate manually triggers regime detection (ignores 4h interval)
-func (api *API) ForceUpdate(ctx context.Context) (*DetectionResult, error) {
-	api.mu.Lock()
-	defer api.mu.Unlock()
-	
-	// Temporarily bypass interval check
-	api.detector.lastUpdate = time.Time{}
-	
-	result, err := api.detector.DetectRegime(ctx)
-	if err != nil {
-		return nil, fmt.Errorf("forced regime update failed: %w", err)
-	}
-	
-	// Update weights if regime changed
-	if api.weightManager.GetCurrentRegime() != result.Regime {
-		api.weightManager.UpdateCurrentRegime(result)
-	}
-	
-	api.lastCheck = time.Now()
-	
-	// Reschedule next regular update
-	api.scheduleNextUpdate(ctx)
-	
-	return result, nil
-}
-
-// GetDetectionResult returns the most recent detection result
-func (api *API) GetDetectionResult() *DetectionResult {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	return api.weightManager.GetLastUpdate()
-}
-
-// GetAllWeightPresets returns all available regime weight presets
-func (api *API) GetAllWeightPresets() map[Regime]*WeightPreset {
-	return api.weightManager.GetAllPresets()
-}
-
-// GetRegimeHistory returns the regime change history
-func (api *API) GetRegimeHistory() []RegimeChange {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	return api.detector.GetDetectionHistory()
-}
-
-// GetAPIStatus returns the current status of the regime API
-func (api *API) GetAPIStatus() map[string]interface{} {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	status := map[string]interface{}{
-		"is_running":     api.isRunning,
-		"last_check":     api.lastCheck.Format(time.RFC3339),
-		"current_regime": api.weightManager.GetCurrentRegime().String(),
-	}
-	
-	if api.weightManager.GetLastUpdate() != nil {
-		result := api.weightManager.GetLastUpdate()
-		status["last_detection"] = map[string]interface{}{
-			"regime":            result.Regime.String(),
-			"confidence":        result.Confidence,
-			"is_stable":         result.IsStable,
-			"changes_count":     result.ChangesSinceStart,
-			"last_update":       result.LastUpdate.Format(time.RFC3339),
-			"next_update":       result.NextUpdate.Format(time.RFC3339),
-		}
-	}
-	
-	return status
-}
-
-// GetFactorWeightTable returns a formatted table of all regime weights
-func (api *API) GetFactorWeightTable() map[string]interface{} {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	presets := api.weightManager.GetAllPresets()
-	
-	// Get all unique factors
-	allFactors := make(map[string]bool)
-	for _, preset := range presets {
-		for factor := range preset.Weights {
-			allFactors[factor] = true
-		}
-	}
-	
-	// Build comparison table
-	factorList := make([]string, 0, len(allFactors))
-	for factor := range allFactors {
-		factorList = append(factorList, factor)
-	}
-	
-	table := map[string]interface{}{
-		"factors": factorList,
-		"regimes": map[string]map[string]float64{},
-		"metadata": map[string]interface{}{
-			"active_regime": api.weightManager.GetCurrentRegime().String(),
-			"last_update":   api.lastCheck.Format(time.RFC3339),
-		},
-	}
-	
-	// Populate weight data for each regime
-	for regime, preset := range presets {
-		regimeWeights := make(map[string]float64)
-		for _, factor := range factorList {
-			if weight, exists := preset.Weights[factor]; exists {
-				regimeWeights[factor] = weight
-			} else {
-				regimeWeights[factor] = 0.0
-			}
-		}
-		table["regimes"].(map[string]map[string]float64)[regime.String()] = regimeWeights
-	}
-	
-	return table
-}
-
-// ValidateConfiguration checks that all regime presets have valid weights
-func (api *API) ValidateConfiguration() error {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	for regime := range api.weightManager.GetAllPresets() {
-		if err := api.weightManager.ValidateWeights(regime); err != nil {
-			return fmt.Errorf("validation failed for regime %s: %w", regime.String(), err)
-		}
-	}
-	
-	return nil
-}
-
-// GetMovementGates returns movement gate configuration for current regime
-func (api *API) GetMovementGates() MovementGateConfig {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	activeWeights := api.weightManager.GetActiveWeights()
-	return activeWeights.MovementGate
-}
-
-// GetRegimeTransitions returns regime transition analysis
-func (api *API) GetRegimeTransitions() map[string]interface{} {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	return api.weightManager.GetRegimeTransitionMatrix()
-}
-
-// scheduleNextUpdate sets up the next 4-hour update cycle
-func (api *API) scheduleNextUpdate(ctx context.Context) {
-	if api.updateTimer != nil {
-		api.updateTimer.Stop()
-	}
-	
-	updateInterval := 4 * time.Hour
-	api.updateTimer = time.AfterFunc(updateInterval, func() {
-		api.performScheduledUpdate(ctx)
-	})
-}
-
-// performScheduledUpdate executes the scheduled regime detection
-func (api *API) performScheduledUpdate(ctx context.Context) {
-	api.mu.Lock()
-	defer api.mu.Unlock()
-	
-	if !api.isRunning {
-		return // API was stopped
-	}
-	
-	result, err := api.detector.DetectRegime(ctx)
-	if err != nil {
-		// Log error but continue running
-		// In production, this would use proper logging
-		return
-	}
-	
-	// Update weights if regime changed
-	previousRegime := api.weightManager.GetCurrentRegime()
-	if previousRegime != result.Regime {
-		api.weightManager.UpdateCurrentRegime(result)
-	}
-	
-	api.lastCheck = time.Now()
-	
-	// Schedule next update
-	api.scheduleNextUpdate(ctx)
-}
-
-// IsRunning returns whether the regime detection cycle is active
-func (api *API) IsRunning() bool {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	return api.isRunning
-}
-
-// GetTimeSinceLastUpdate returns duration since last regime check
-func (api *API) GetTimeSinceLastUpdate() time.Duration {
-	api.mu.RLock()
-	defer api.mu.RUnlock()
-	
-	if api.lastCheck.IsZero() {
-		return 0
-	}
-	
-	return time.Since(api.lastCheck)
-}
\ No newline at end of file
+package regime
+
+import (
+	"context"
+	"fmt"
+	"sync"
+	"time"
+)
+
+// API provides the main interface for regime detection and weight management
+type API struct {
+	detector      *Detector
+	weightManager *WeightManager
+	mu            sync.RWMutex
+	lastCheck     time.Time
+	updateTimer   *time.Timer
+	isRunning     bool
+}
+
+// NewAPI creates a new regime detection API
+func NewAPI(inputs DetectorInputs) *API {
+	detector := NewDetector(inputs)
+	weightManager := NewWeightManager()
+
+	return &API{
+		detector:      detector,
+		weightManager: weightManager,
+	}
+}
+
+// NewAPIWithConfig creates an API with custom configuration
+func NewAPIWithConfig(inputs DetectorInputs, config DetectorConfig) *API {
+	detector := NewDetectorWithConfig(inputs, config)
+	weightManager := NewWeightManager()
+
+	return &API{
+		detector:      detector,
+		weightManager: weightManager,
+	}
+}
+
+// Start begins the 4-hour regime detection cycle
+func (api *API) Start(ctx context.Context) error {
+	api.mu.Lock()
+	defer api.mu.Unlock()
+
+	if api.isRunning {
+		return fmt.Errorf("regime API is already running")
+	}
+
+	// Initial detection
+	result, err := api.detector.DetectRegime(ctx)
+	if err != nil {
+		return fmt.Errorf("initial regime detection failed: %w", err)
+	}
+
+	// Update weights based on initial detection
+	api.weightManager.UpdateCurrentRegime(result)
+	api.lastCheck = time.Now()
+	api.isRunning = true
+
+	// Schedule next update
+	api.scheduleNextUpdate(ctx)
+
+	return nil
+}
+
+// Stop halts the regime detection cycle
+func (api *API) Stop() {
+	api.mu.Lock()
+	defer api.mu.Unlock()
+
+	if api.updateTimer != nil {
+		api.updateTimer.Stop()
+		api.updateTimer = nil
+	}
+
+	api.isRunning = false
+}
+
+// GetActiveWeights returns the current regime's factor weights
+func (api *API) GetActiveWeights() *WeightPreset {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	return api.weightManager.GetActiveWeights()
+}
+
+// GetCurrentRegime returns the currently detected regime
+func (api *API) GetCurrentRegime(ctx context.Context) (Regime, error) {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	return api.detector.GetCurrentRegime(ctx)
+}
+
+// ForceUpdate manually triggers regime detection (ignores 4h interval)
+func (api *API) ForceUpdate(ctx context.Context) (*DetectionResult, error) {
+	api.mu.Lock()
+	defer api.mu.Unlock()
+
+	// Temporarily bypass interval check
+	api.detector.lastUpdate = time.Time{}
+
+	result, err := api.detector.DetectRegime(ctx)
+	if err != nil {
+		return nil, fmt.Errorf("forced regime update failed: %w", err)
+	}
+
+	// Update weights if regime changed
+	if api.weightManager.GetCurrentRegime() != result.Regime {
+		api.weightManager.UpdateCurrentRegime(result)
+	}
+
+	api.lastCheck = time.Now()
+
+	// Reschedule next regular update
+	api.scheduleNextUpdate(ctx)
+
+	return result, nil
+}
+
+// GetDetectionResult returns the most recent detection result
+func (api *API) GetDetectionResult() *DetectionResult {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	return api.weightManager.GetLastUpdate()
+}
+
+// GetAllWeightPresets returns all available regime weight presets
+func (api *API) GetAllWeightPresets() map[Regime]*WeightPreset {
+	return api.weightManager.GetAllPresets()
+}
+
+// GetRegimeHistory returns the regime change history
+func (api *API) GetRegimeHistory() []RegimeChange {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	return api.detector.GetDetectionHistory()
+}
+
+// GetAPIStatus returns the current status of the regime API
+func (api *API) GetAPIStatus() map[string]interface{} {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	status := map[string]interface{}{
+		"is_running":     api.isRunning,
+		"last_check":     api.lastCheck.Format(time.RFC3339),
+		"current_regime": api.weightManager.GetCurrentRegime().String(),
+	}
+
+	if api.weightManager.GetLastUpdate() != nil {
+		result := api.weightManager.GetLastUpdate()
+		status["last_detection"] = map[string]interface{}{
+			"regime":        result.Regime.String(),
+			"confidence":    result.Confidence,
+			"is_stable":     result.IsStable,
+			"changes_count": result.ChangesSinceStart,
+			"last_update":   result.LastUpdate.Format(time.RFC3339),
+			"next_update":   result.NextUpdate.Format(time.RFC3339),
+		}
+	}
+
+	return status
+}
+
+// GetFactorWeightTable returns a formatted table of all regime weights
+func (api *API) GetFactorWeightTable() map[string]interface{} {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	presets := api.weightManager.GetAllPresets()
+
+	// Get all unique factors
+	allFactors := make(map[string]bool)
+	for _, preset := range presets {
+		for factor := range preset.Weights {
+			allFactors[factor] = true
+		}
+	}
+
+	// Build comparison table
+	factorList := make([]string, 0, len(allFactors))
+	for factor := range allFactors {
+		factorList = append(factorList, factor)
+	}
+
+	table := map[string]interface{}{
+		"factors": factorList,
+		"regimes": map[string]map[string]float64{},
+		"metadata": map[string]interface{}{
+			"active_regime": api.weightManager.GetCurrentRegime().String(),
+			"last_update":   api.lastCheck.Format(time.RFC3339),
+		},
+	}
+
+	// Populate weight data for each regime
+	for regime, preset := range presets {
+		regimeWeights := make(map[string]float64)
+		for _, factor := range factorList {
+			if weight, exists := preset.Weights[factor]; exists {
+				regimeWeights[factor] = weight
+			} else {
+				regimeWeights[factor] = 0.0
+			}
+		}
+		table["regimes"].(map[string]map[string]float64)[regime.String()] = regimeWeights
+	}
+
+	return table
+}
+
+// ValidateConfiguration checks that all regime presets have valid weights
+func (api *API) ValidateConfiguration() error {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	for regime := range api.weightManager.GetAllPresets() {
+		if err := api.weightManager.ValidateWeights(regime); err != nil {
+			return fmt.Errorf("validation failed for regime %s: %w", regime.String(), err)
+		}
+	}
+
+	return nil
+}
+
+// GetMovementGates returns movement gate configuration for current regime
+func (api *API) GetMovementGates() MovementGateConfig {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	activeWeights := api.weightManager.GetActiveWeights()
+	return activeWeights.MovementGate
+}
+
+// GetRegimeTransitions returns regime transition analysis
+func (api *API) GetRegimeTransitions() map[string]interface{} {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	return api.weightManager.GetRegimeTransitionMatrix()
+}
+
+// scheduleNextUpdate sets up the next 4-hour update cycle
+func (api *API) scheduleNextUpdate(ctx context.Context) {
+	if api.updateTimer != nil {
+		api.updateTimer.Stop()
+	}
+
+	updateInterval := 4 * time.Hour
+	api.updateTimer = time.AfterFunc(updateInterval, func() {
+		api.performScheduledUpdate(ctx)
+	})
+}
+
+// performScheduledUpdate executes the scheduled regime detection
+func (api *API) performScheduledUpdate(ctx context.Context) {
+	api.mu.Lock()
+	defer api.mu.Unlock()
+
+	if !api.isRunning {
+		return // API was stopped
+	}
+
+	result, err := api.detector.DetectRegime(ctx)
+	if err != nil {
+		// Log error but continue running
+		// In production, this would use proper logging
+		return
+	}
+
+	// Update weights if regime changed
+	previousRegime := api.weightManager.GetCurrentRegime()
+	if previousRegime != result.Regime {
+		api.weightManager.UpdateCurrentRegime(result)
+	}
+
+	api.lastCheck = time.Now()
+
+	// Schedule next update
+	api.scheduleNextUpdate(ctx)
+}
+
+// IsRunning returns whether the regime detection cycle is active
+func (api *API) IsRunning() bool {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	return api.isRunning
+}
+
+// GetTimeSinceLastUpdate returns duration since last regime check
+func (api *API) GetTimeSinceLastUpdate() time.Duration {
+	api.mu.RLock()
+	defer api.mu.RUnlock()
+
+	if api.lastCheck.IsZero() {
+		return 0
+	}
+
+	return time.Since(api.lastCheck)
+}
diff --git a/internal/regime/detector.go b/internal/regime/detector.go
index 6b842c7..12b3ae3 100644
--- a/internal/regime/detector.go
+++ b/internal/regime/detector.go
@@ -1,298 +1,298 @@
-package regime
-
-import (
-	"context"
-	"fmt"
-	"time"
-)
-
-// Regime represents the current market regime classification
-type Regime int
-
-const (
-	TrendingBull Regime = iota
-	Choppy
-	HighVol
-)
-
-func (r Regime) String() string {
-	switch r {
-	case TrendingBull:
-		return "trending_bull"
-	case Choppy:
-		return "choppy"
-	case HighVol:
-		return "high_vol"
-	default:
-		return "unknown"
-	}
-}
-
-// DetectorInputs provides market data for regime classification
-type DetectorInputs interface {
-	GetRealizedVolatility7d(ctx context.Context) (float64, error)
-	GetBreadthAbove20MA(ctx context.Context) (float64, error) // Percentage 0.0-1.0
-	GetBreadthThrustADXProxy(ctx context.Context) (float64, error)
-	GetTimestamp(ctx context.Context) (time.Time, error)
-}
-
-// DetectorConfig holds configuration for the regime detector
-type DetectorConfig struct {
-	UpdateIntervalHours    int     `yaml:"update_interval_hours"`    // Default: 4
-	RealizedVolThreshold   float64 `yaml:"realized_vol_threshold"`   // Default: 0.25 (25%)
-	BreadthThreshold       float64 `yaml:"breadth_threshold"`        // Default: 0.60 (60%)
-	BreadthThrustThreshold float64 `yaml:"breadth_thrust_threshold"` // Default: 0.70
-	MinSamplesRequired     int     `yaml:"min_samples_required"`     // Default: 3
-}
-
-// DetectionResult contains the regime classification result
-type DetectionResult struct {
-	Regime            Regime                 `json:"regime"`
-	Confidence        float64                `json:"confidence"`        // 0.0-1.0
-	Signals           map[string]interface{} `json:"signals"`           // Individual signal values
-	VotingBreakdown   map[string]string      `json:"voting_breakdown"`  // Per-signal votes
-	LastUpdate        time.Time              `json:"last_update"`
-	NextUpdate        time.Time              `json:"next_update"`
-	IsStable          bool                   `json:"is_stable"`         // True if regime hasn't changed in 2+ cycles
-	ChangesSinceStart int                    `json:"changes_since_start"`
-}
-
-// Detector implements the 4-hour regime detection system
-type Detector struct {
-	config        DetectorConfig
-	inputs        DetectorInputs
-	lastResult    *DetectionResult
-	lastUpdate    time.Time
-	changeHistory []RegimeChange
-}
-
-// RegimeChange tracks regime transitions for stability analysis
-type RegimeChange struct {
-	Timestamp   time.Time `json:"timestamp"`
-	FromRegime  Regime    `json:"from_regime"`
-	ToRegime    Regime    `json:"to_regime"`
-	Confidence  float64   `json:"confidence"`
-	TriggerHour int       `json:"trigger_hour"` // Hour of day when change occurred
-}
-
-// NewDetector creates a new regime detector with default configuration
-func NewDetector(inputs DetectorInputs) *Detector {
-	return &Detector{
-		config: DetectorConfig{
-			UpdateIntervalHours:    4,
-			RealizedVolThreshold:   0.25,
-			BreadthThreshold:       0.60,
-			BreadthThrustThreshold: 0.70,
-			MinSamplesRequired:     3,
-		},
-		inputs:        inputs,
-		changeHistory: make([]RegimeChange, 0),
-	}
-}
-
-// NewDetectorWithConfig creates a detector with custom configuration
-func NewDetectorWithConfig(inputs DetectorInputs, config DetectorConfig) *Detector {
-	return &Detector{
-		config:        config,
-		inputs:        inputs,
-		changeHistory: make([]RegimeChange, 0),
-	}
-}
-
-// ShouldUpdate checks if it's time for a 4-hour regime update
-func (d *Detector) ShouldUpdate(ctx context.Context) (bool, error) {
-	if d.lastUpdate.IsZero() {
-		return true, nil // First update
-	}
-	
-	currentTime, err := d.inputs.GetTimestamp(ctx)
-	if err != nil {
-		return false, fmt.Errorf("failed to get current timestamp: %w", err)
-	}
-	
-	elapsed := currentTime.Sub(d.lastUpdate)
-	updateInterval := time.Duration(d.config.UpdateIntervalHours) * time.Hour
-	
-	return elapsed >= updateInterval, nil
-}
-
-// DetectRegime performs regime classification using majority voting
-func (d *Detector) DetectRegime(ctx context.Context) (*DetectionResult, error) {
-	shouldUpdate, err := d.ShouldUpdate(ctx)
-	if err != nil {
-		return nil, fmt.Errorf("failed to check update requirement: %w", err)
-	}
-	
-	if !shouldUpdate && d.lastResult != nil {
-		return d.lastResult, nil // Return cached result
-	}
-	
-	// Fetch current market signals
-	signals, err := d.fetchSignals(ctx)
-	if err != nil {
-		return nil, fmt.Errorf("failed to fetch market signals: %w", err)
-	}
-	
-	// Perform majority voting
-	votes := d.calculateVotes(signals)
-	regime, confidence := d.majorityVote(votes)
-	
-	// Create detection result
-	currentTime, _ := d.inputs.GetTimestamp(ctx)
-	result := &DetectionResult{
-		Regime:          regime,
-		Confidence:      confidence,
-		Signals:         signals,
-		VotingBreakdown: votes,
-		LastUpdate:      currentTime,
-		NextUpdate:      currentTime.Add(time.Duration(d.config.UpdateIntervalHours) * time.Hour),
-		IsStable:        d.isRegimeStable(regime),
-	}
-	
-	// Track regime changes
-	if d.lastResult != nil && d.lastResult.Regime != regime {
-		change := RegimeChange{
-			Timestamp:   currentTime,
-			FromRegime:  d.lastResult.Regime,
-			ToRegime:    regime,
-			Confidence:  confidence,
-			TriggerHour: currentTime.Hour(),
-		}
-		d.changeHistory = append(d.changeHistory, change)
-		result.ChangesSinceStart = len(d.changeHistory)
-	}
-	
-	d.lastResult = result
-	d.lastUpdate = currentTime
-	
-	return result, nil
-}
-
-// GetCurrentRegime returns the most recent regime classification
-func (d *Detector) GetCurrentRegime(ctx context.Context) (Regime, error) {
-	result, err := d.DetectRegime(ctx)
-	if err != nil {
-		return Choppy, err // Default to Choppy on error
-	}
-	return result.Regime, nil
-}
-
-// GetDetectionHistory returns the regime change history
-func (d *Detector) GetDetectionHistory() []RegimeChange {
-	return d.changeHistory
-}
-
-// fetchSignals retrieves all required market signals
-func (d *Detector) fetchSignals(ctx context.Context) (map[string]interface{}, error) {
-	signals := make(map[string]interface{})
-	
-	realizedVol, err := d.inputs.GetRealizedVolatility7d(ctx)
-	if err != nil {
-		return nil, fmt.Errorf("failed to get realized volatility: %w", err)
-	}
-	signals["realized_vol_7d"] = realizedVol
-	
-	breadth, err := d.inputs.GetBreadthAbove20MA(ctx)
-	if err != nil {
-		return nil, fmt.Errorf("failed to get breadth above 20MA: %w", err)
-	}
-	signals["breadth_above_20ma"] = breadth
-	
-	breadthThrust, err := d.inputs.GetBreadthThrustADXProxy(ctx)
-	if err != nil {
-		return nil, fmt.Errorf("failed to get breadth thrust: %w", err)
-	}
-	signals["breadth_thrust_adx"] = breadthThrust
-	
-	return signals, nil
-}
-
-// calculateVotes determines each signal's vote for regime classification
-func (d *Detector) calculateVotes(signals map[string]interface{}) map[string]string {
-	votes := make(map[string]string)
-	
-	// Realized volatility vote
-	realizedVol := signals["realized_vol_7d"].(float64)
-	if realizedVol > d.config.RealizedVolThreshold {
-		votes["realized_vol"] = "high_vol"
-	} else {
-		votes["realized_vol"] = "low_vol"
-	}
-	
-	// Breadth above 20MA vote
-	breadth := signals["breadth_above_20ma"].(float64)
-	if breadth > d.config.BreadthThreshold {
-		votes["breadth"] = "trending_bull"
-	} else {
-		votes["breadth"] = "choppy"
-	}
-	
-	// Breadth thrust/ADX proxy vote
-	breadthThrust := signals["breadth_thrust_adx"].(float64)
-	if breadthThrust > d.config.BreadthThrustThreshold {
-		votes["breadth_thrust"] = "trending_bull"
-	} else {
-		votes["breadth_thrust"] = "choppy"
-	}
-	
-	return votes
-}
-
-// majorityVote performs majority voting across all signals
-func (d *Detector) majorityVote(votes map[string]string) (Regime, float64) {
-	voteCounts := map[string]int{
-		"trending_bull": 0,
-		"choppy":        0,
-		"high_vol":      0,
-	}
-	
-	// Count votes
-	for _, vote := range votes {
-		voteCounts[vote]++
-	}
-	
-	// Find majority winner
-	maxVotes := 0
-	winner := "choppy" // Default
-	
-	for regime, count := range voteCounts {
-		if count > maxVotes {
-			maxVotes = count
-			winner = regime
-		}
-	}
-	
-	// Convert to regime enum
-	var regime Regime
-	switch winner {
-	case "trending_bull":
-		regime = TrendingBull
-	case "high_vol":
-		regime = HighVol
-	default:
-		regime = Choppy
-	}
-	
-	// Calculate confidence based on vote margin
-	totalVotes := len(votes)
-	confidence := float64(maxVotes) / float64(totalVotes)
-	
-	return regime, confidence
-}
-
-// isRegimeStable checks if the regime has been stable for 2+ cycles
-func (d *Detector) isRegimeStable(currentRegime Regime) bool {
-	if len(d.changeHistory) == 0 {
-		return true // No changes yet
-	}
-	
-	// Check if there have been any changes in the last 2 cycles (8 hours)
-	cutoff := time.Now().Add(-8 * time.Hour)
-	for _, change := range d.changeHistory {
-		if change.Timestamp.After(cutoff) {
-			return false // Recent change detected
-		}
-	}
-	
-	return true
-}
\ No newline at end of file
+package regime
+
+import (
+	"context"
+	"fmt"
+	"time"
+)
+
+// Regime represents the current market regime classification
+type Regime int
+
+const (
+	TrendingBull Regime = iota
+	Choppy
+	HighVol
+)
+
+func (r Regime) String() string {
+	switch r {
+	case TrendingBull:
+		return "trending_bull"
+	case Choppy:
+		return "choppy"
+	case HighVol:
+		return "high_vol"
+	default:
+		return "unknown"
+	}
+}
+
+// DetectorInputs provides market data for regime classification
+type DetectorInputs interface {
+	GetRealizedVolatility7d(ctx context.Context) (float64, error)
+	GetBreadthAbove20MA(ctx context.Context) (float64, error) // Percentage 0.0-1.0
+	GetBreadthThrustADXProxy(ctx context.Context) (float64, error)
+	GetTimestamp(ctx context.Context) (time.Time, error)
+}
+
+// DetectorConfig holds configuration for the regime detector
+type DetectorConfig struct {
+	UpdateIntervalHours    int     `yaml:"update_interval_hours"`    // Default: 4
+	RealizedVolThreshold   float64 `yaml:"realized_vol_threshold"`   // Default: 0.25 (25%)
+	BreadthThreshold       float64 `yaml:"breadth_threshold"`        // Default: 0.60 (60%)
+	BreadthThrustThreshold float64 `yaml:"breadth_thrust_threshold"` // Default: 0.70
+	MinSamplesRequired     int     `yaml:"min_samples_required"`     // Default: 3
+}
+
+// DetectionResult contains the regime classification result
+type DetectionResult struct {
+	Regime            Regime                 `json:"regime"`
+	Confidence        float64                `json:"confidence"`       // 0.0-1.0
+	Signals           map[string]interface{} `json:"signals"`          // Individual signal values
+	VotingBreakdown   map[string]string      `json:"voting_breakdown"` // Per-signal votes
+	LastUpdate        time.Time              `json:"last_update"`
+	NextUpdate        time.Time              `json:"next_update"`
+	IsStable          bool                   `json:"is_stable"` // True if regime hasn't changed in 2+ cycles
+	ChangesSinceStart int                    `json:"changes_since_start"`
+}
+
+// Detector implements the 4-hour regime detection system
+type Detector struct {
+	config        DetectorConfig
+	inputs        DetectorInputs
+	lastResult    *DetectionResult
+	lastUpdate    time.Time
+	changeHistory []RegimeChange
+}
+
+// RegimeChange tracks regime transitions for stability analysis
+type RegimeChange struct {
+	Timestamp   time.Time `json:"timestamp"`
+	FromRegime  Regime    `json:"from_regime"`
+	ToRegime    Regime    `json:"to_regime"`
+	Confidence  float64   `json:"confidence"`
+	TriggerHour int       `json:"trigger_hour"` // Hour of day when change occurred
+}
+
+// NewDetector creates a new regime detector with default configuration
+func NewDetector(inputs DetectorInputs) *Detector {
+	return &Detector{
+		config: DetectorConfig{
+			UpdateIntervalHours:    4,
+			RealizedVolThreshold:   0.25,
+			BreadthThreshold:       0.60,
+			BreadthThrustThreshold: 0.70,
+			MinSamplesRequired:     3,
+		},
+		inputs:        inputs,
+		changeHistory: make([]RegimeChange, 0),
+	}
+}
+
+// NewDetectorWithConfig creates a detector with custom configuration
+func NewDetectorWithConfig(inputs DetectorInputs, config DetectorConfig) *Detector {
+	return &Detector{
+		config:        config,
+		inputs:        inputs,
+		changeHistory: make([]RegimeChange, 0),
+	}
+}
+
+// ShouldUpdate checks if it's time for a 4-hour regime update
+func (d *Detector) ShouldUpdate(ctx context.Context) (bool, error) {
+	if d.lastUpdate.IsZero() {
+		return true, nil // First update
+	}
+
+	currentTime, err := d.inputs.GetTimestamp(ctx)
+	if err != nil {
+		return false, fmt.Errorf("failed to get current timestamp: %w", err)
+	}
+
+	elapsed := currentTime.Sub(d.lastUpdate)
+	updateInterval := time.Duration(d.config.UpdateIntervalHours) * time.Hour
+
+	return elapsed >= updateInterval, nil
+}
+
+// DetectRegime performs regime classification using majority voting
+func (d *Detector) DetectRegime(ctx context.Context) (*DetectionResult, error) {
+	shouldUpdate, err := d.ShouldUpdate(ctx)
+	if err != nil {
+		return nil, fmt.Errorf("failed to check update requirement: %w", err)
+	}
+
+	if !shouldUpdate && d.lastResult != nil {
+		return d.lastResult, nil // Return cached result
+	}
+
+	// Fetch current market signals
+	signals, err := d.fetchSignals(ctx)
+	if err != nil {
+		return nil, fmt.Errorf("failed to fetch market signals: %w", err)
+	}
+
+	// Perform majority voting
+	votes := d.calculateVotes(signals)
+	regime, confidence := d.majorityVote(votes)
+
+	// Create detection result
+	currentTime, _ := d.inputs.GetTimestamp(ctx)
+	result := &DetectionResult{
+		Regime:          regime,
+		Confidence:      confidence,
+		Signals:         signals,
+		VotingBreakdown: votes,
+		LastUpdate:      currentTime,
+		NextUpdate:      currentTime.Add(time.Duration(d.config.UpdateIntervalHours) * time.Hour),
+		IsStable:        d.isRegimeStable(regime),
+	}
+
+	// Track regime changes
+	if d.lastResult != nil && d.lastResult.Regime != regime {
+		change := RegimeChange{
+			Timestamp:   currentTime,
+			FromRegime:  d.lastResult.Regime,
+			ToRegime:    regime,
+			Confidence:  confidence,
+			TriggerHour: currentTime.Hour(),
+		}
+		d.changeHistory = append(d.changeHistory, change)
+		result.ChangesSinceStart = len(d.changeHistory)
+	}
+
+	d.lastResult = result
+	d.lastUpdate = currentTime
+
+	return result, nil
+}
+
+// GetCurrentRegime returns the most recent regime classification
+func (d *Detector) GetCurrentRegime(ctx context.Context) (Regime, error) {
+	result, err := d.DetectRegime(ctx)
+	if err != nil {
+		return Choppy, err // Default to Choppy on error
+	}
+	return result.Regime, nil
+}
+
+// GetDetectionHistory returns the regime change history
+func (d *Detector) GetDetectionHistory() []RegimeChange {
+	return d.changeHistory
+}
+
+// fetchSignals retrieves all required market signals
+func (d *Detector) fetchSignals(ctx context.Context) (map[string]interface{}, error) {
+	signals := make(map[string]interface{})
+
+	realizedVol, err := d.inputs.GetRealizedVolatility7d(ctx)
+	if err != nil {
+		return nil, fmt.Errorf("failed to get realized volatility: %w", err)
+	}
+	signals["realized_vol_7d"] = realizedVol
+
+	breadth, err := d.inputs.GetBreadthAbove20MA(ctx)
+	if err != nil {
+		return nil, fmt.Errorf("failed to get breadth above 20MA: %w", err)
+	}
+	signals["breadth_above_20ma"] = breadth
+
+	breadthThrust, err := d.inputs.GetBreadthThrustADXProxy(ctx)
+	if err != nil {
+		return nil, fmt.Errorf("failed to get breadth thrust: %w", err)
+	}
+	signals["breadth_thrust_adx"] = breadthThrust
+
+	return signals, nil
+}
+
+// calculateVotes determines each signal's vote for regime classification
+func (d *Detector) calculateVotes(signals map[string]interface{}) map[string]string {
+	votes := make(map[string]string)
+
+	// Realized volatility vote
+	realizedVol := signals["realized_vol_7d"].(float64)
+	if realizedVol > d.config.RealizedVolThreshold {
+		votes["realized_vol"] = "high_vol"
+	} else {
+		votes["realized_vol"] = "low_vol"
+	}
+
+	// Breadth above 20MA vote
+	breadth := signals["breadth_above_20ma"].(float64)
+	if breadth > d.config.BreadthThreshold {
+		votes["breadth"] = "trending_bull"
+	} else {
+		votes["breadth"] = "choppy"
+	}
+
+	// Breadth thrust/ADX proxy vote
+	breadthThrust := signals["breadth_thrust_adx"].(float64)
+	if breadthThrust > d.config.BreadthThrustThreshold {
+		votes["breadth_thrust"] = "trending_bull"
+	} else {
+		votes["breadth_thrust"] = "choppy"
+	}
+
+	return votes
+}
+
+// majorityVote performs majority voting across all signals
+func (d *Detector) majorityVote(votes map[string]string) (Regime, float64) {
+	voteCounts := map[string]int{
+		"trending_bull": 0,
+		"choppy":        0,
+		"high_vol":      0,
+	}
+
+	// Count votes
+	for _, vote := range votes {
+		voteCounts[vote]++
+	}
+
+	// Find majority winner
+	maxVotes := 0
+	winner := "choppy" // Default
+
+	for regime, count := range voteCounts {
+		if count > maxVotes {
+			maxVotes = count
+			winner = regime
+		}
+	}
+
+	// Convert to regime enum
+	var regime Regime
+	switch winner {
+	case "trending_bull":
+		regime = TrendingBull
+	case "high_vol":
+		regime = HighVol
+	default:
+		regime = Choppy
+	}
+
+	// Calculate confidence based on vote margin
+	totalVotes := len(votes)
+	confidence := float64(maxVotes) / float64(totalVotes)
+
+	return regime, confidence
+}
+
+// isRegimeStable checks if the regime has been stable for 2+ cycles
+func (d *Detector) isRegimeStable(currentRegime Regime) bool {
+	if len(d.changeHistory) == 0 {
+		return true // No changes yet
+	}
+
+	// Check if there have been any changes in the last 2 cycles (8 hours)
+	cutoff := time.Now().Add(-8 * time.Hour)
+	for _, change := range d.changeHistory {
+		if change.Timestamp.After(cutoff) {
+			return false // Recent change detected
+		}
+	}
+
+	return true
+}
diff --git a/internal/regime/weights.go b/internal/regime/weights.go
index 7b1a0f0..eb314d4 100644
--- a/internal/regime/weights.go
+++ b/internal/regime/weights.go
@@ -1,240 +1,240 @@
-package regime
-
-import (
-	"fmt"
-)
-
-// WeightPreset defines factor weights for a specific regime
-type WeightPreset struct {
-	Regime       Regime                 `json:"regime"`
-	Name         string                 `json:"name"`
-	Description  string                 `json:"description"`
-	Weights      map[string]float64     `json:"weights"`      // Factor -> Weight (0.0-1.0)
-	MovementGate MovementGateConfig     `json:"movement_gate"`
-	Metadata     map[string]interface{} `json:"metadata,omitempty"`
-}
-
-// MovementGateConfig defines movement detection thresholds per regime
-type MovementGateConfig struct {
-	MinMovementPercent  float64 `json:"min_movement_percent"`  // Minimum % move to trigger
-	TimeWindowHours     int     `json:"time_window_hours"`     // Detection window
-	VolumeSurgeRequired bool    `json:"volume_surge_required"` // Require volume confirmation
-	TightenedThresholds bool    `json:"tightened_thresholds"`  // Higher bars for entry
-}
-
-// WeightManager manages regime-based factor weights
-type WeightManager struct {
-	presets     map[Regime]*WeightPreset
-	currentRegime Regime
-	lastUpdate    *DetectionResult
-}
-
-// NewWeightManager creates a weight manager with default presets
-func NewWeightManager() *WeightManager {
-	wm := &WeightManager{
-		presets:       make(map[Regime]*WeightPreset),
-		currentRegime: Choppy, // Safe default
-	}
-	
-	wm.initializeDefaultPresets()
-	return wm
-}
-
-// initializeDefaultPresets sets up the regime-specific weight tables
-func (wm *WeightManager) initializeDefaultPresets() {
-	// Trending Bull: Higher momentum, weekly carry, reduced volatility emphasis
-	wm.presets[TrendingBull] = &WeightPreset{
-		Regime:      TrendingBull,
-		Name:        "Trending Bull",
-		Description: "Bull trend with sustained momentum and 7d carry",
-		Weights: map[string]float64{
-			"momentum_1h":       0.25,  // Core momentum
-			"momentum_4h":       0.20,  // Medium-term
-			"momentum_12h":      0.15,  // Longer-term
-			"momentum_24h":      0.10,  // Extended
-			"weekly_7d_carry":   0.10,  // Trending-only factor
-			"volume_surge":      0.08,  // Volume confirmation
-			"volatility_score":  0.05,  // Reduced in trending
-			"quality_score":     0.04,  // Quality overlay
-			"social_sentiment":  0.03,  // Social factor (capped)
-		},
-		MovementGate: MovementGateConfig{
-			MinMovementPercent:  3.5,   // Lower threshold in trends
-			TimeWindowHours:     48,
-			VolumeSurgeRequired: false, // Not required in strong trends
-			TightenedThresholds: false,
-		},
-		Metadata: map[string]interface{}{
-			"regime_characteristics": []string{"sustained_momentum", "lower_volatility", "directional_bias"},
-			"special_factors":        []string{"weekly_7d_carry"},
-		},
-	}
-	
-	// Choppy: Balanced weights, no weekly carry, standard gates
-	wm.presets[Choppy] = &WeightPreset{
-		Regime:      Choppy,
-		Name:        "Choppy",
-		Description: "Mixed signals with balanced factor allocation",
-		Weights: map[string]float64{
-			"momentum_1h":       0.20,  // Reduced short-term
-			"momentum_4h":       0.18,  // Core timeframe
-			"momentum_12h":      0.15,  // Medium-term
-			"momentum_24h":      0.12,  // Extended
-			"weekly_7d_carry":   0.00,  // No weekly carry in chop
-			"volume_surge":      0.12,  // Higher volume emphasis
-			"volatility_score":  0.10,  // Volatility important
-			"quality_score":     0.08,  // Quality matters more
-			"social_sentiment":  0.05,  // Social factor (capped)
-		},
-		MovementGate: MovementGateConfig{
-			MinMovementPercent:  5.0,   // Standard threshold
-			TimeWindowHours:     48,
-			VolumeSurgeRequired: true,  // Require volume in chop
-			TightenedThresholds: false,
-		},
-		Metadata: map[string]interface{}{
-			"regime_characteristics": []string{"mixed_signals", "range_bound", "mean_reverting"},
-			"disabled_factors":       []string{"weekly_7d_carry"},
-		},
-	}
-	
-	// High Vol: Defensive positioning, tightened gates, quality emphasis
-	wm.presets[HighVol] = &WeightPreset{
-		Regime:      HighVol,
-		Name:        "High Volatility",
-		Description: "High volatility regime with tightened movement gates",
-		Weights: map[string]float64{
-			"momentum_1h":       0.15,  // Reduced short-term (noisy)
-			"momentum_4h":       0.15,  // Reduced medium-term
-			"momentum_12h":      0.18,  // Favor longer timeframes
-			"momentum_24h":      0.15,  // Extended view
-			"weekly_7d_carry":   0.00,  // No weekly carry in volatility
-			"volume_surge":      0.08,  // Lower volume weight (can be misleading)
-			"volatility_score":  0.15,  // High volatility awareness
-			"quality_score":     0.12,  // Quality crucial in volatility
-			"social_sentiment":  0.02,  // Minimal social (noise)
-		},
-		MovementGate: MovementGateConfig{
-			MinMovementPercent:  7.0,   // Tightened threshold
-			TimeWindowHours:     36,    // Shorter window (faster decay)
-			VolumeSurgeRequired: true,  // Volume required
-			TightenedThresholds: true,  // Higher bars for entry
-		},
-		Metadata: map[string]interface{}{
-			"regime_characteristics": []string{"high_volatility", "defensive_positioning", "quality_focus"},
-			"tightened_gates":        true,
-			"risk_adjustments":       []string{"reduced_short_term", "quality_emphasis", "minimal_social"},
-		},
-	}
-}
-
-// GetActiveWeights returns the current regime's factor weights
-func (wm *WeightManager) GetActiveWeights() *WeightPreset {
-	if preset, exists := wm.presets[wm.currentRegime]; exists {
-		return preset
-	}
-	// Fallback to choppy if current regime not found
-	return wm.presets[Choppy]
-}
-
-// GetWeightsForRegime returns weights for a specific regime
-func (wm *WeightManager) GetWeightsForRegime(regime Regime) (*WeightPreset, error) {
-	if preset, exists := wm.presets[regime]; exists {
-		return preset, nil
-	}
-	return nil, fmt.Errorf("no weight preset found for regime: %s", regime.String())
-}
-
-// UpdateCurrentRegime updates the active regime based on detection result
-func (wm *WeightManager) UpdateCurrentRegime(result *DetectionResult) {
-	wm.currentRegime = result.Regime
-	wm.lastUpdate = result
-}
-
-// GetAllPresets returns all available weight presets
-func (wm *WeightManager) GetAllPresets() map[Regime]*WeightPreset {
-	return wm.presets
-}
-
-// ValidateWeights checks that weights sum to approximately 1.0
-func (wm *WeightManager) ValidateWeights(regime Regime) error {
-	preset, exists := wm.presets[regime]
-	if !exists {
-		return fmt.Errorf("regime %s not found", regime.String())
-	}
-	
-	total := 0.0
-	for _, weight := range preset.Weights {
-		total += weight
-	}
-	
-	// Allow for small floating point errors
-	if total < 0.95 || total > 1.05 {
-		return fmt.Errorf("weights for regime %s sum to %.3f, expected ~1.0", regime.String(), total)
-	}
-	
-	return nil
-}
-
-// GetCurrentRegime returns the currently active regime
-func (wm *WeightManager) GetCurrentRegime() Regime {
-	return wm.currentRegime
-}
-
-// GetLastUpdate returns the most recent detection result
-func (wm *WeightManager) GetLastUpdate() *DetectionResult {
-	return wm.lastUpdate
-}
-
-// GetRegimeTransitionMatrix returns transition statistics
-func (wm *WeightManager) GetRegimeTransitionMatrix() map[string]interface{} {
-	// This would be populated with historical transition data
-	// For now, return structure for future implementation
-	return map[string]interface{}{
-		"transitions": map[string]map[string]int{
-			"trending_bull_to_choppy":   {"count": 0, "avg_duration_hours": 0},
-			"trending_bull_to_high_vol": {"count": 0, "avg_duration_hours": 0},
-			"choppy_to_trending_bull":   {"count": 0, "avg_duration_hours": 0},
-			"choppy_to_high_vol":        {"count": 0, "avg_duration_hours": 0},
-			"high_vol_to_trending_bull": {"count": 0, "avg_duration_hours": 0},
-			"high_vol_to_choppy":        {"count": 0, "avg_duration_hours": 0},
-		},
-		"stability_metrics": map[string]interface{}{
-			"avg_regime_duration_hours": 0,
-			"most_stable_regime":        "choppy",
-			"transition_frequency":      0.0,
-		},
-	}
-}
-
-// GetWeightDifferences compares weights between two regimes
-func (wm *WeightManager) GetWeightDifferences(from, to Regime) (map[string]float64, error) {
-	fromPreset, err := wm.GetWeightsForRegime(from)
-	if err != nil {
-		return nil, fmt.Errorf("invalid 'from' regime: %w", err)
-	}
-	
-	toPreset, err := wm.GetWeightsForRegime(to)
-	if err != nil {
-		return nil, fmt.Errorf("invalid 'to' regime: %w", err)
-	}
-	
-	differences := make(map[string]float64)
-	
-	// Calculate differences for all factors
-	allFactors := make(map[string]bool)
-	for factor := range fromPreset.Weights {
-		allFactors[factor] = true
-	}
-	for factor := range toPreset.Weights {
-		allFactors[factor] = true
-	}
-	
-	for factor := range allFactors {
-		fromWeight := fromPreset.Weights[factor] // 0.0 if not present
-		toWeight := toPreset.Weights[factor]     // 0.0 if not present
-		differences[factor] = toWeight - fromWeight
-	}
-	
-	return differences, nil
-}
\ No newline at end of file
+package regime
+
+import (
+	"fmt"
+)
+
+// WeightPreset defines factor weights for a specific regime
+type WeightPreset struct {
+	Regime       Regime                 `json:"regime"`
+	Name         string                 `json:"name"`
+	Description  string                 `json:"description"`
+	Weights      map[string]float64     `json:"weights"` // Factor -> Weight (0.0-1.0)
+	MovementGate MovementGateConfig     `json:"movement_gate"`
+	Metadata     map[string]interface{} `json:"metadata,omitempty"`
+}
+
+// MovementGateConfig defines movement detection thresholds per regime
+type MovementGateConfig struct {
+	MinMovementPercent  float64 `json:"min_movement_percent"`  // Minimum % move to trigger
+	TimeWindowHours     int     `json:"time_window_hours"`     // Detection window
+	VolumeSurgeRequired bool    `json:"volume_surge_required"` // Require volume confirmation
+	TightenedThresholds bool    `json:"tightened_thresholds"`  // Higher bars for entry
+}
+
+// WeightManager manages regime-based factor weights
+type WeightManager struct {
+	presets       map[Regime]*WeightPreset
+	currentRegime Regime
+	lastUpdate    *DetectionResult
+}
+
+// NewWeightManager creates a weight manager with default presets
+func NewWeightManager() *WeightManager {
+	wm := &WeightManager{
+		presets:       make(map[Regime]*WeightPreset),
+		currentRegime: Choppy, // Safe default
+	}
+
+	wm.initializeDefaultPresets()
+	return wm
+}
+
+// initializeDefaultPresets sets up the regime-specific weight tables
+func (wm *WeightManager) initializeDefaultPresets() {
+	// Trending Bull: Higher momentum, weekly carry, reduced volatility emphasis
+	wm.presets[TrendingBull] = &WeightPreset{
+		Regime:      TrendingBull,
+		Name:        "Trending Bull",
+		Description: "Bull trend with sustained momentum and 7d carry",
+		Weights: map[string]float64{
+			"momentum_1h":      0.25, // Core momentum
+			"momentum_4h":      0.20, // Medium-term
+			"momentum_12h":     0.15, // Longer-term
+			"momentum_24h":     0.10, // Extended
+			"weekly_7d_carry":  0.10, // Trending-only factor
+			"volume_surge":     0.08, // Volume confirmation
+			"volatility_score": 0.05, // Reduced in trending
+			"quality_score":    0.04, // Quality overlay
+			"social_sentiment": 0.03, // Social factor (capped)
+		},
+		MovementGate: MovementGateConfig{
+			MinMovementPercent:  3.5, // Lower threshold in trends
+			TimeWindowHours:     48,
+			VolumeSurgeRequired: false, // Not required in strong trends
+			TightenedThresholds: false,
+		},
+		Metadata: map[string]interface{}{
+			"regime_characteristics": []string{"sustained_momentum", "lower_volatility", "directional_bias"},
+			"special_factors":        []string{"weekly_7d_carry"},
+		},
+	}
+
+	// Choppy: Balanced weights, no weekly carry, standard gates
+	wm.presets[Choppy] = &WeightPreset{
+		Regime:      Choppy,
+		Name:        "Choppy",
+		Description: "Mixed signals with balanced factor allocation",
+		Weights: map[string]float64{
+			"momentum_1h":      0.20, // Reduced short-term
+			"momentum_4h":      0.18, // Core timeframe
+			"momentum_12h":     0.15, // Medium-term
+			"momentum_24h":     0.12, // Extended
+			"weekly_7d_carry":  0.00, // No weekly carry in chop
+			"volume_surge":     0.12, // Higher volume emphasis
+			"volatility_score": 0.10, // Volatility important
+			"quality_score":    0.08, // Quality matters more
+			"social_sentiment": 0.05, // Social factor (capped)
+		},
+		MovementGate: MovementGateConfig{
+			MinMovementPercent:  5.0, // Standard threshold
+			TimeWindowHours:     48,
+			VolumeSurgeRequired: true, // Require volume in chop
+			TightenedThresholds: false,
+		},
+		Metadata: map[string]interface{}{
+			"regime_characteristics": []string{"mixed_signals", "range_bound", "mean_reverting"},
+			"disabled_factors":       []string{"weekly_7d_carry"},
+		},
+	}
+
+	// High Vol: Defensive positioning, tightened gates, quality emphasis
+	wm.presets[HighVol] = &WeightPreset{
+		Regime:      HighVol,
+		Name:        "High Volatility",
+		Description: "High volatility regime with tightened movement gates",
+		Weights: map[string]float64{
+			"momentum_1h":      0.15, // Reduced short-term (noisy)
+			"momentum_4h":      0.15, // Reduced medium-term
+			"momentum_12h":     0.18, // Favor longer timeframes
+			"momentum_24h":     0.15, // Extended view
+			"weekly_7d_carry":  0.00, // No weekly carry in volatility
+			"volume_surge":     0.08, // Lower volume weight (can be misleading)
+			"volatility_score": 0.15, // High volatility awareness
+			"quality_score":    0.12, // Quality crucial in volatility
+			"social_sentiment": 0.02, // Minimal social (noise)
+		},
+		MovementGate: MovementGateConfig{
+			MinMovementPercent:  7.0,  // Tightened threshold
+			TimeWindowHours:     36,   // Shorter window (faster decay)
+			VolumeSurgeRequired: true, // Volume required
+			TightenedThresholds: true, // Higher bars for entry
+		},
+		Metadata: map[string]interface{}{
+			"regime_characteristics": []string{"high_volatility", "defensive_positioning", "quality_focus"},
+			"tightened_gates":        true,
+			"risk_adjustments":       []string{"reduced_short_term", "quality_emphasis", "minimal_social"},
+		},
+	}
+}
+
+// GetActiveWeights returns the current regime's factor weights
+func (wm *WeightManager) GetActiveWeights() *WeightPreset {
+	if preset, exists := wm.presets[wm.currentRegime]; exists {
+		return preset
+	}
+	// Fallback to choppy if current regime not found
+	return wm.presets[Choppy]
+}
+
+// GetWeightsForRegime returns weights for a specific regime
+func (wm *WeightManager) GetWeightsForRegime(regime Regime) (*WeightPreset, error) {
+	if preset, exists := wm.presets[regime]; exists {
+		return preset, nil
+	}
+	return nil, fmt.Errorf("no weight preset found for regime: %s", regime.String())
+}
+
+// UpdateCurrentRegime updates the active regime based on detection result
+func (wm *WeightManager) UpdateCurrentRegime(result *DetectionResult) {
+	wm.currentRegime = result.Regime
+	wm.lastUpdate = result
+}
+
+// GetAllPresets returns all available weight presets
+func (wm *WeightManager) GetAllPresets() map[Regime]*WeightPreset {
+	return wm.presets
+}
+
+// ValidateWeights checks that weights sum to approximately 1.0
+func (wm *WeightManager) ValidateWeights(regime Regime) error {
+	preset, exists := wm.presets[regime]
+	if !exists {
+		return fmt.Errorf("regime %s not found", regime.String())
+	}
+
+	total := 0.0
+	for _, weight := range preset.Weights {
+		total += weight
+	}
+
+	// Allow for small floating point errors
+	if total < 0.95 || total > 1.05 {
+		return fmt.Errorf("weights for regime %s sum to %.3f, expected ~1.0", regime.String(), total)
+	}
+
+	return nil
+}
+
+// GetCurrentRegime returns the currently active regime
+func (wm *WeightManager) GetCurrentRegime() Regime {
+	return wm.currentRegime
+}
+
+// GetLastUpdate returns the most recent detection result
+func (wm *WeightManager) GetLastUpdate() *DetectionResult {
+	return wm.lastUpdate
+}
+
+// GetRegimeTransitionMatrix returns transition statistics
+func (wm *WeightManager) GetRegimeTransitionMatrix() map[string]interface{} {
+	// This would be populated with historical transition data
+	// For now, return structure for future implementation
+	return map[string]interface{}{
+		"transitions": map[string]map[string]int{
+			"trending_bull_to_choppy":   {"count": 0, "avg_duration_hours": 0},
+			"trending_bull_to_high_vol": {"count": 0, "avg_duration_hours": 0},
+			"choppy_to_trending_bull":   {"count": 0, "avg_duration_hours": 0},
+			"choppy_to_high_vol":        {"count": 0, "avg_duration_hours": 0},
+			"high_vol_to_trending_bull": {"count": 0, "avg_duration_hours": 0},
+			"high_vol_to_choppy":        {"count": 0, "avg_duration_hours": 0},
+		},
+		"stability_metrics": map[string]interface{}{
+			"avg_regime_duration_hours": 0,
+			"most_stable_regime":        "choppy",
+			"transition_frequency":      0.0,
+		},
+	}
+}
+
+// GetWeightDifferences compares weights between two regimes
+func (wm *WeightManager) GetWeightDifferences(from, to Regime) (map[string]float64, error) {
+	fromPreset, err := wm.GetWeightsForRegime(from)
+	if err != nil {
+		return nil, fmt.Errorf("invalid 'from' regime: %w", err)
+	}
+
+	toPreset, err := wm.GetWeightsForRegime(to)
+	if err != nil {
+		return nil, fmt.Errorf("invalid 'to' regime: %w", err)
+	}
+
+	differences := make(map[string]float64)
+
+	// Calculate differences for all factors
+	allFactors := make(map[string]bool)
+	for factor := range fromPreset.Weights {
+		allFactors[factor] = true
+	}
+	for factor := range toPreset.Weights {
+		allFactors[factor] = true
+	}
+
+	for factor := range allFactors {
+		fromWeight := fromPreset.Weights[factor] // 0.0 if not present
+		toWeight := toPreset.Weights[factor]     // 0.0 if not present
+		differences[factor] = toWeight - fromWeight
+	}
+
+	return differences, nil
+}
diff --git a/internal/reports/regime/analyzer.go b/internal/reports/regime/analyzer.go
index e6739c6..88e98ac 100644
--- a/internal/reports/regime/analyzer.go
+++ b/internal/reports/regime/analyzer.go
@@ -74,7 +74,7 @@ func (ra *RegimeAnalyzer) generateFlipHistory(period ReportPeriod) []RegimeFlip
 
 	for currentTime.Before(period.EndTime) {
 		// Random duration between 8-72 hours (regime stability)
-		durationHours := 12.0 + (float64(len(flips)*13) % 48.0)
+		durationHours := 12.0 + float64((len(flips)*13)%48)
 		nextTime := currentTime.Add(time.Duration(durationHours * float64(time.Hour)))
 
 		if nextTime.After(period.EndTime) {
@@ -90,11 +90,11 @@ func (ra *RegimeAnalyzer) generateFlipHistory(period ReportPeriod) []RegimeFlip
 			ToRegime:      nextRegime,
 			DurationHours: durationHours,
 			DetectorInputs: RegimeDetectorInputs{
-				RealizedVol7d:   0.25 + (float64(len(flips)*7)%30)/100.0,  // 0.25-0.55
-				PctAbove20MA:    0.45 + (float64(len(flips)*11)%30)/100.0, // 0.45-0.75
-				BreadthThrust:   -0.1 + (float64(len(flips)*13)%30)/100.0, // -0.1 to 0.2
-				StabilityScore:  0.75 + (float64(len(flips)*5)%20)/100.0,  // 0.75-0.95
-				ConfidenceLevel: 0.80 + (float64(len(flips)*3)%15)/100.0,  // 0.80-0.95
+				RealizedVol7d:   0.25 + float64((len(flips)*7)%30)/100.0,  // 0.25-0.55
+				PctAbove20MA:    0.45 + float64((len(flips)*11)%30)/100.0, // 0.45-0.75
+				BreadthThrust:   -0.1 + float64((len(flips)*13)%30)/100.0, // -0.1 to 0.2
+				StabilityScore:  0.75 + float64((len(flips)*5)%20)/100.0,  // 0.75-0.95
+				ConfidenceLevel: 0.80 + float64((len(flips)*3)%15)/100.0,  // 0.80-0.95
 			},
 			WeightChanges: ra.generateWeightChange(currentRegime, nextRegime),
 		}
@@ -214,17 +214,14 @@ func (ra *RegimeAnalyzer) generateDecileLifts(period ReportPeriod) map[string]De
 			}
 
 			// Regime-specific performance patterns
-			var baseReturn, lift float64
+			var baseReturn float64
 			switch regime {
 			case "trending_bull":
 				baseReturn = 5.0 + float64(d)*2.5 // 5% to 27.5%
-				lift = 1.2 + float64(d)*0.3       // Strong lift in trending
 			case "choppy":
 				baseReturn = -2.0 + float64(d)*1.8 // -2% to 14.2%
-				lift = 1.1 + float64(d)*0.2        // Weaker lift in chop
 			case "high_vol":
 				baseReturn = 3.0 + float64(d)*3.0 // 3% to 30%
-				lift = 1.0 + float64(d)*0.4       // Variable lift in volatility
 			}
 
 			count := 45 + (d * 5) // More candidates in higher deciles
diff --git a/internal/reports/regime/generator.go b/internal/reports/regime/generator.go
index ee20654..61add84 100644
--- a/internal/reports/regime/generator.go
+++ b/internal/reports/regime/generator.go
@@ -7,7 +7,6 @@ import (
 	"os"
 	"path/filepath"
 	"strconv"
-	"strings"
 	"text/template"
 	"time"
 
diff --git a/internal/scoring/model.go b/internal/scoring/model.go
index 6827eb2..e29692b 100644
--- a/internal/scoring/model.go
+++ b/internal/scoring/model.go
@@ -1,164 +1,164 @@
-package scoring
-
-import (
-	"fmt"
-	"time"
-)
-
-type Regime string
-
-const (
-	RegimeTrending Regime = "trending"
-	RegimeChoppy   Regime = "choppy"
-	RegimeHighVol  Regime = "high_vol"
-)
-
-type CompositeScore struct {
-	Score float64            `json:"score"`
-	Parts map[string]float64 `json:"parts"`
-	Meta  ScoreMeta          `json:"meta"`
-}
-
-type ScoreMeta struct {
-	Timestamp    time.Time `json:"timestamp"`
-	Regime       Regime    `json:"regime"`
-	Symbol       string    `json:"symbol"`
-	Attribution  string    `json:"attribution"`
-	IsOrthogonal bool      `json:"is_orthogonal"`
-}
-
-type FactorInput struct {
-	Symbol    string
-	Momentum  MomentumFactors
-	Technical TechnicalFactors
-	Volume    VolumeFactors
-	Quality   QualityFactors
-	Social    SocialFactors
-}
-
-type MomentumFactors struct {
-	Return1h  float64
-	Return4h  float64
-	Return12h float64
-	Return24h float64
-	Return7d  float64
-	Accel4h   float64
-}
-
-type TechnicalFactors struct {
-	RSI14     float64
-	MACD      float64
-	BBWidth   float64
-	ATRRatio  float64
-}
-
-type VolumeFactors struct {
-	VolumeRatio24h float64
-	VWAP          float64
-	OBV           float64
-	VolSpike      float64
-}
-
-type QualityFactors struct {
-	Spread        float64
-	Depth         float64
-	VADR          float64
-	MarketCap     float64
-}
-
-type SocialFactors struct {
-	Sentiment     float64
-	Mentions      float64
-	SocialVolume  float64
-	RedditScore   float64
-}
-
-type Calculator struct {
-	regime       Regime
-	momentumCore *MomentumCore
-	weights      *RegimeWeightConfig
-}
-
-func NewCalculator(regime Regime) *Calculator {
-	return &Calculator{
-		regime:       regime,
-		momentumCore: NewMomentumCore(),
-		weights:      NewRegimeWeights(),
-	}
-}
-
-func (c *Calculator) Calculate(input FactorInput) (*CompositeScore, error) {
-	if input.Symbol == "" {
-		return nil, fmt.Errorf("symbol is required")
-	}
-
-	meta := ScoreMeta{
-		Timestamp:    time.Now(),
-		Regime:       c.regime,
-		Symbol:       input.Symbol,
-		Attribution:  "unified_orthogonal_v1",
-		IsOrthogonal: true,
-	}
-
-	parts := make(map[string]float64)
-
-	momentumScore := c.momentumCore.Calculate(input.Momentum, c.regime)
-	parts["momentum"] = momentumScore
-
-	techResidual := c.calculateTechnicalResidual(input.Technical, momentumScore)
-	parts["technical"] = techResidual
-
-	volResidual := c.calculateVolumeResidual(input.Volume, momentumScore, techResidual)
-	parts["volume"] = volResidual
-
-	qualityResidual := c.calculateQualityResidual(input.Quality, momentumScore, techResidual, volResidual)
-	parts["quality"] = qualityResidual
-
-	socialResidual := c.calculateSocialResidual(input.Social, momentumScore, techResidual, volResidual, qualityResidual)
-	parts["social"] = socialResidual
-
-	regimeWeights := c.weights.GetWeights(c.regime)
-	
-	compositeScore := momentumScore*regimeWeights.Momentum +
-		techResidual*regimeWeights.Technical +
-		volResidual*regimeWeights.Volume +
-		qualityResidual*regimeWeights.Quality
-
-	socialCapped := applySocialCap(socialResidual)
-	parts["social_capped"] = socialCapped
-	
-	finalScore := compositeScore + socialCapped
-
-	return &CompositeScore{
-		Score: finalScore,
-		Parts: parts,
-		Meta:  meta,
-	}, nil
-}
-
-func (c *Calculator) calculateTechnicalResidual(tech TechnicalFactors, momentum float64) float64 {
-	return NewTechnicalResiduals().Calculate(tech, momentum)
-}
-
-func (c *Calculator) calculateVolumeResidual(vol VolumeFactors, momentum, technical float64) float64 {
-	return NewVolumeResiduals().Calculate(vol, momentum, technical)
-}
-
-func (c *Calculator) calculateQualityResidual(qual QualityFactors, momentum, technical, volume float64) float64 {
-	return NewQualityResiduals().Calculate(qual, momentum, technical, volume)
-}
-
-func (c *Calculator) calculateSocialResidual(soc SocialFactors, momentum, technical, volume, quality float64) float64 {
-	return NewSocialResiduals().Calculate(soc, momentum, technical, volume, quality)
-}
-
-func applySocialCap(socialScore float64) float64 {
-	const socialCap = 10.0
-	if socialScore > socialCap {
-		return socialCap
-	}
-	if socialScore < -socialCap {
-		return -socialCap
-	}
-	return socialScore
-}
\ No newline at end of file
+package scoring
+
+import (
+	"fmt"
+	"time"
+)
+
+type Regime string
+
+const (
+	RegimeTrending Regime = "trending"
+	RegimeChoppy   Regime = "choppy"
+	RegimeHighVol  Regime = "high_vol"
+)
+
+type CompositeScore struct {
+	Score float64            `json:"score"`
+	Parts map[string]float64 `json:"parts"`
+	Meta  ScoreMeta          `json:"meta"`
+}
+
+type ScoreMeta struct {
+	Timestamp    time.Time `json:"timestamp"`
+	Regime       Regime    `json:"regime"`
+	Symbol       string    `json:"symbol"`
+	Attribution  string    `json:"attribution"`
+	IsOrthogonal bool      `json:"is_orthogonal"`
+}
+
+type FactorInput struct {
+	Symbol    string
+	Momentum  MomentumFactors
+	Technical TechnicalFactors
+	Volume    VolumeFactors
+	Quality   QualityFactors
+	Social    SocialFactors
+}
+
+type MomentumFactors struct {
+	Return1h  float64
+	Return4h  float64
+	Return12h float64
+	Return24h float64
+	Return7d  float64
+	Accel4h   float64
+}
+
+type TechnicalFactors struct {
+	RSI14    float64
+	MACD     float64
+	BBWidth  float64
+	ATRRatio float64
+}
+
+type VolumeFactors struct {
+	VolumeRatio24h float64
+	VWAP           float64
+	OBV            float64
+	VolSpike       float64
+}
+
+type QualityFactors struct {
+	Spread    float64
+	Depth     float64
+	VADR      float64
+	MarketCap float64
+}
+
+type SocialFactors struct {
+	Sentiment    float64
+	Mentions     float64
+	SocialVolume float64
+	RedditScore  float64
+}
+
+type Calculator struct {
+	regime       Regime
+	momentumCore *MomentumCore
+	weights      *RegimeWeightConfig
+}
+
+func NewCalculator(regime Regime) *Calculator {
+	return &Calculator{
+		regime:       regime,
+		momentumCore: NewMomentumCore(),
+		weights:      NewRegimeWeights(),
+	}
+}
+
+func (c *Calculator) Calculate(input FactorInput) (*CompositeScore, error) {
+	if input.Symbol == "" {
+		return nil, fmt.Errorf("symbol is required")
+	}
+
+	meta := ScoreMeta{
+		Timestamp:    time.Now(),
+		Regime:       c.regime,
+		Symbol:       input.Symbol,
+		Attribution:  "unified_orthogonal_v1",
+		IsOrthogonal: true,
+	}
+
+	parts := make(map[string]float64)
+
+	momentumScore := c.momentumCore.Calculate(input.Momentum, c.regime)
+	parts["momentum"] = momentumScore
+
+	techResidual := c.calculateTechnicalResidual(input.Technical, momentumScore)
+	parts["technical"] = techResidual
+
+	volResidual := c.calculateVolumeResidual(input.Volume, momentumScore, techResidual)
+	parts["volume"] = volResidual
+
+	qualityResidual := c.calculateQualityResidual(input.Quality, momentumScore, techResidual, volResidual)
+	parts["quality"] = qualityResidual
+
+	socialResidual := c.calculateSocialResidual(input.Social, momentumScore, techResidual, volResidual, qualityResidual)
+	parts["social"] = socialResidual
+
+	regimeWeights := c.weights.GetWeights(c.regime)
+
+	compositeScore := momentumScore*regimeWeights.Momentum +
+		techResidual*regimeWeights.Technical +
+		volResidual*regimeWeights.Volume +
+		qualityResidual*regimeWeights.Quality
+
+	socialCapped := applySocialCap(socialResidual)
+	parts["social_capped"] = socialCapped
+
+	finalScore := compositeScore + socialCapped
+
+	return &CompositeScore{
+		Score: finalScore,
+		Parts: parts,
+		Meta:  meta,
+	}, nil
+}
+
+func (c *Calculator) calculateTechnicalResidual(tech TechnicalFactors, momentum float64) float64 {
+	return NewTechnicalResiduals().Calculate(tech, momentum)
+}
+
+func (c *Calculator) calculateVolumeResidual(vol VolumeFactors, momentum, technical float64) float64 {
+	return NewVolumeResiduals().Calculate(vol, momentum, technical)
+}
+
+func (c *Calculator) calculateQualityResidual(qual QualityFactors, momentum, technical, volume float64) float64 {
+	return NewQualityResiduals().Calculate(qual, momentum, technical, volume)
+}
+
+func (c *Calculator) calculateSocialResidual(soc SocialFactors, momentum, technical, volume, quality float64) float64 {
+	return NewSocialResiduals().Calculate(soc, momentum, technical, volume, quality)
+}
+
+func applySocialCap(socialScore float64) float64 {
+	const socialCap = 10.0
+	if socialScore > socialCap {
+		return socialCap
+	}
+	if socialScore < -socialCap {
+		return -socialCap
+	}
+	return socialScore
+}
diff --git a/internal/scoring/model_test.go b/internal/scoring/model_test.go
index 3d68f58..a100ffb 100644
--- a/internal/scoring/model_test.go
+++ b/internal/scoring/model_test.go
@@ -1,204 +1,204 @@
-package scoring
-
-import (
-	"math"
-	"testing"
-)
-
-func TestCompositeScore_Basic(t *testing.T) {
-	calc := NewCalculator(RegimeChoppy)
-	
-	input := FactorInput{
-		Symbol: "BTC-USD",
-		Momentum: MomentumFactors{
-			Return1h:  5.0,
-			Return4h:  8.0,
-			Return12h: 10.0,
-			Return24h: 12.0,
-			Return7d:  15.0,
-			Accel4h:   2.0,
-		},
-		Technical: TechnicalFactors{
-			RSI14:    70.0,
-			MACD:     1.5,
-			BBWidth:  0.8,
-			ATRRatio: 1.2,
-		},
-		Volume: VolumeFactors{
-			VolumeRatio24h: 2.5,
-			VWAP:          100.5,
-			OBV:           50000,
-			VolSpike:      3.0,
-		},
-		Quality: QualityFactors{
-			Spread:    0.001,
-			Depth:     100000,
-			VADR:      2.0,
-			MarketCap: 1000000000,
-		},
-		Social: SocialFactors{
-			Sentiment:    0.8,
-			Mentions:     1000,
-			SocialVolume: 5000,
-			RedditScore:  85.0,
-		},
-	}
-
-	result, err := calc.Calculate(input)
-	if err != nil {
-		t.Fatalf("unexpected error: %v", err)
-	}
-
-	if result.Score == 0 {
-		t.Error("expected non-zero composite score")
-	}
-
-	if result.Meta.Symbol != "BTC-USD" {
-		t.Errorf("expected symbol BTC-USD, got %s", result.Meta.Symbol)
-	}
-
-	if result.Meta.Regime != RegimeChoppy {
-		t.Errorf("expected regime choppy, got %s", result.Meta.Regime)
-	}
-
-	if !result.Meta.IsOrthogonal {
-		t.Error("expected orthogonal flag to be true")
-	}
-}
-
-func TestWeightSum_100Percent(t *testing.T) {
-	rwc := NewRegimeWeights()
-	
-	testCases := []Regime{RegimeTrending, RegimeChoppy, RegimeHighVol}
-	
-	for _, regime := range testCases {
-		weights := rwc.GetWeights(regime)
-		sum := weights.Sum()
-		
-		if math.Abs(sum-1.0) > 0.001 {
-			t.Errorf("regime %s weights sum to %.3f, expected 1.000", regime, sum)
-		}
-	}
-}
-
-func TestSocialCap_Enforcement(t *testing.T) {
-	sr := NewSocialResiduals()
-	
-	testCases := []struct {
-		input    float64
-		expected float64
-	}{
-		{5.0, 5.0},
-		{10.0, 10.0},
-		{15.0, 10.0},
-		{-5.0, -5.0},
-		{-10.0, -10.0},
-		{-15.0, -10.0},
-		{100.0, 10.0},
-		{-100.0, -10.0},
-	}
-	
-	for _, tc := range testCases {
-		result := sr.applyCap(tc.input)
-		if result != tc.expected {
-			t.Errorf("social cap input %.1f: expected %.1f, got %.1f", tc.input, tc.expected, result)
-		}
-	}
-}
-
-func TestMomentumCore_Protection(t *testing.T) {
-	mc := NewMomentumCore()
-	
-	if !mc.IsProtected() {
-		t.Error("momentum core should be protected from orthogonalization")
-	}
-}
-
-func TestDecileMonotonicity(t *testing.T) {
-	calc := NewCalculator(RegimeTrending)
-	
-	baseInput := FactorInput{
-		Symbol: "TEST-USD",
-		Momentum: MomentumFactors{Return4h: 0},
-		Technical: TechnicalFactors{RSI14: 50},
-		Volume: VolumeFactors{VolumeRatio24h: 1.0},
-		Quality: QualityFactors{VADR: 1.5},
-		Social: SocialFactors{Sentiment: 0.0},
-	}
-	
-	var prevScore float64
-	for i := 1; i <= 5; i++ {
-		testInput := baseInput
-		testInput.Momentum.Return4h = float64(i * 5)
-		testInput.Momentum.Return1h = float64(i * 2)
-		testInput.Momentum.Return12h = float64(i * 3)
-		testInput.Momentum.Return24h = float64(i * 4)
-		
-		result, err := calc.Calculate(testInput)
-		if err != nil {
-			t.Fatalf("unexpected error for decile %d: %v", i, err)
-		}
-		
-		if i > 1 && result.Parts["momentum"] <= prevScore {
-			t.Errorf("decile %d momentum %.2f not greater than decile %d momentum %.2f", 
-				i, result.Parts["momentum"], i-1, prevScore)
-		}
-		
-		prevScore = result.Parts["momentum"]
-	}
-}
-
-func TestOrthogonalitySmoke(t *testing.T) {
-	calc := NewCalculator(RegimeChoppy)
-	
-	testCases := []struct {
-		name string
-		input FactorInput
-	}{
-		{
-			name: "high_momentum",
-			input: FactorInput{
-				Symbol: "TEST1-USD",
-				Momentum: MomentumFactors{Return4h: 20},
-				Technical: TechnicalFactors{RSI14: 30},
-				Volume: VolumeFactors{VolumeRatio24h: 1.0},
-				Quality: QualityFactors{VADR: 1.5},
-				Social: SocialFactors{Sentiment: 0.0},
-			},
-		},
-		{
-			name: "high_technical",
-			input: FactorInput{
-				Symbol: "TEST2-USD",
-				Momentum: MomentumFactors{Return4h: 5},
-				Technical: TechnicalFactors{RSI14: 80},
-				Volume: VolumeFactors{VolumeRatio24h: 1.0},
-				Quality: QualityFactors{VADR: 1.5},
-				Social: SocialFactors{Sentiment: 0.0},
-			},
-		},
-	}
-	
-	correlations := make(map[string]float64)
-	
-	for _, tc := range testCases {
-		result, err := calc.Calculate(tc.input)
-		if err != nil {
-			t.Fatalf("unexpected error for %s: %v", tc.name, err)
-		}
-		
-		correlations[tc.name] = result.Parts["technical"]
-	}
-	
-	if len(correlations) != 2 {
-		t.Error("expected 2 test cases for correlation check")
-	}
-}
-
-func TestRegimeWeights_Validation(t *testing.T) {
-	rwc := NewRegimeWeights()
-	
-	if err := rwc.ValidateWeights(); err != nil {
-		t.Errorf("weight validation failed: %v", err)
-	}
-}
\ No newline at end of file
+package scoring
+
+import (
+	"math"
+	"testing"
+)
+
+func TestCompositeScore_Basic(t *testing.T) {
+	calc := NewCalculator(RegimeChoppy)
+
+	input := FactorInput{
+		Symbol: "BTC-USD",
+		Momentum: MomentumFactors{
+			Return1h:  5.0,
+			Return4h:  8.0,
+			Return12h: 10.0,
+			Return24h: 12.0,
+			Return7d:  15.0,
+			Accel4h:   2.0,
+		},
+		Technical: TechnicalFactors{
+			RSI14:    70.0,
+			MACD:     1.5,
+			BBWidth:  0.8,
+			ATRRatio: 1.2,
+		},
+		Volume: VolumeFactors{
+			VolumeRatio24h: 2.5,
+			VWAP:           100.5,
+			OBV:            50000,
+			VolSpike:       3.0,
+		},
+		Quality: QualityFactors{
+			Spread:    0.001,
+			Depth:     100000,
+			VADR:      2.0,
+			MarketCap: 1000000000,
+		},
+		Social: SocialFactors{
+			Sentiment:    0.8,
+			Mentions:     1000,
+			SocialVolume: 5000,
+			RedditScore:  85.0,
+		},
+	}
+
+	result, err := calc.Calculate(input)
+	if err != nil {
+		t.Fatalf("unexpected error: %v", err)
+	}
+
+	if result.Score == 0 {
+		t.Error("expected non-zero composite score")
+	}
+
+	if result.Meta.Symbol != "BTC-USD" {
+		t.Errorf("expected symbol BTC-USD, got %s", result.Meta.Symbol)
+	}
+
+	if result.Meta.Regime != RegimeChoppy {
+		t.Errorf("expected regime choppy, got %s", result.Meta.Regime)
+	}
+
+	if !result.Meta.IsOrthogonal {
+		t.Error("expected orthogonal flag to be true")
+	}
+}
+
+func TestWeightSum_100Percent(t *testing.T) {
+	rwc := NewRegimeWeights()
+
+	testCases := []Regime{RegimeTrending, RegimeChoppy, RegimeHighVol}
+
+	for _, regime := range testCases {
+		weights := rwc.GetWeights(regime)
+		sum := weights.Sum()
+
+		if math.Abs(sum-1.0) > 0.001 {
+			t.Errorf("regime %s weights sum to %.3f, expected 1.000", regime, sum)
+		}
+	}
+}
+
+func TestSocialCap_Enforcement(t *testing.T) {
+	sr := NewSocialResiduals()
+
+	testCases := []struct {
+		input    float64
+		expected float64
+	}{
+		{5.0, 5.0},
+		{10.0, 10.0},
+		{15.0, 10.0},
+		{-5.0, -5.0},
+		{-10.0, -10.0},
+		{-15.0, -10.0},
+		{100.0, 10.0},
+		{-100.0, -10.0},
+	}
+
+	for _, tc := range testCases {
+		result := sr.applyCap(tc.input)
+		if result != tc.expected {
+			t.Errorf("social cap input %.1f: expected %.1f, got %.1f", tc.input, tc.expected, result)
+		}
+	}
+}
+
+func TestMomentumCore_Protection(t *testing.T) {
+	mc := NewMomentumCore()
+
+	if !mc.IsProtected() {
+		t.Error("momentum core should be protected from orthogonalization")
+	}
+}
+
+func TestDecileMonotonicity(t *testing.T) {
+	calc := NewCalculator(RegimeTrending)
+
+	baseInput := FactorInput{
+		Symbol:    "TEST-USD",
+		Momentum:  MomentumFactors{Return4h: 0},
+		Technical: TechnicalFactors{RSI14: 50},
+		Volume:    VolumeFactors{VolumeRatio24h: 1.0},
+		Quality:   QualityFactors{VADR: 1.5},
+		Social:    SocialFactors{Sentiment: 0.0},
+	}
+
+	var prevScore float64
+	for i := 1; i <= 5; i++ {
+		testInput := baseInput
+		testInput.Momentum.Return4h = float64(i * 5)
+		testInput.Momentum.Return1h = float64(i * 2)
+		testInput.Momentum.Return12h = float64(i * 3)
+		testInput.Momentum.Return24h = float64(i * 4)
+
+		result, err := calc.Calculate(testInput)
+		if err != nil {
+			t.Fatalf("unexpected error for decile %d: %v", i, err)
+		}
+
+		if i > 1 && result.Parts["momentum"] <= prevScore {
+			t.Errorf("decile %d momentum %.2f not greater than decile %d momentum %.2f",
+				i, result.Parts["momentum"], i-1, prevScore)
+		}
+
+		prevScore = result.Parts["momentum"]
+	}
+}
+
+func TestOrthogonalitySmoke(t *testing.T) {
+	calc := NewCalculator(RegimeChoppy)
+
+	testCases := []struct {
+		name  string
+		input FactorInput
+	}{
+		{
+			name: "high_momentum",
+			input: FactorInput{
+				Symbol:    "TEST1-USD",
+				Momentum:  MomentumFactors{Return4h: 20},
+				Technical: TechnicalFactors{RSI14: 30},
+				Volume:    VolumeFactors{VolumeRatio24h: 1.0},
+				Quality:   QualityFactors{VADR: 1.5},
+				Social:    SocialFactors{Sentiment: 0.0},
+			},
+		},
+		{
+			name: "high_technical",
+			input: FactorInput{
+				Symbol:    "TEST2-USD",
+				Momentum:  MomentumFactors{Return4h: 5},
+				Technical: TechnicalFactors{RSI14: 80},
+				Volume:    VolumeFactors{VolumeRatio24h: 1.0},
+				Quality:   QualityFactors{VADR: 1.5},
+				Social:    SocialFactors{Sentiment: 0.0},
+			},
+		},
+	}
+
+	correlations := make(map[string]float64)
+
+	for _, tc := range testCases {
+		result, err := calc.Calculate(tc.input)
+		if err != nil {
+			t.Fatalf("unexpected error for %s: %v", tc.name, err)
+		}
+
+		correlations[tc.name] = result.Parts["technical"]
+	}
+
+	if len(correlations) != 2 {
+		t.Error("expected 2 test cases for correlation check")
+	}
+}
+
+func TestRegimeWeights_Validation(t *testing.T) {
+	rwc := NewRegimeWeights()
+
+	if err := rwc.ValidateWeights(); err != nil {
+		t.Errorf("weight validation failed: %v", err)
+	}
+}
diff --git a/internal/scoring/momentum_core.go b/internal/scoring/momentum_core.go
index 9d88046..717e822 100644
--- a/internal/scoring/momentum_core.go
+++ b/internal/scoring/momentum_core.go
@@ -1,95 +1,95 @@
-package scoring
-
-import (
-	"math"
-)
-
-type MomentumCore struct {
-	weights MomentumWeights
-}
-
-type MomentumWeights struct {
-	Return1h  float64
-	Return4h  float64
-	Return12h float64
-	Return24h float64
-	Return7d  float64
-}
-
-func NewMomentumCore() *MomentumCore {
-	return &MomentumCore{
-		weights: getMomentumWeights(),
-	}
-}
-
-func getMomentumWeights() MomentumWeights {
-	return MomentumWeights{
-		Return1h:  0.20, // 20%
-		Return4h:  0.35, // 35%  
-		Return12h: 0.30, // 30%
-		Return24h: 0.15, // 15%
-		Return7d:  0.00, // 0% (only in trending regimes)
-	}
-}
-
-func getMomentumWeightsTrending() MomentumWeights {
-	return MomentumWeights{
-		Return1h:  0.15, // 15%
-		Return4h:  0.30, // 30%
-		Return12h: 0.25, // 25%
-		Return24h: 0.20, // 20%
-		Return7d:  0.10, // 10% (weekly component in trending)
-	}
-}
-
-func (mc *MomentumCore) Calculate(factors MomentumFactors, regime Regime) float64 {
-	weights := mc.getRegimeWeights(regime)
-	
-	momentumScore := factors.Return1h*weights.Return1h +
-		factors.Return4h*weights.Return4h +
-		factors.Return12h*weights.Return12h +
-		factors.Return24h*weights.Return24h +
-		factors.Return7d*weights.Return7d
-
-	accelBoost := mc.calculateAccelerationBoost(factors.Accel4h)
-	
-	return momentumScore + accelBoost
-}
-
-func (mc *MomentumCore) getRegimeWeights(regime Regime) MomentumWeights {
-	switch regime {
-	case RegimeTrending:
-		return getMomentumWeightsTrending()
-	case RegimeChoppy, RegimeHighVol:
-		return getMomentumWeights()
-	default:
-		return getMomentumWeights()
-	}
-}
-
-func (mc *MomentumCore) calculateAccelerationBoost(accel4h float64) float64 {
-	const maxBoost = 2.0
-	
-	if accel4h == 0 {
-		return 0
-	}
-	
-	absAccel := math.Abs(accel4h)
-	normalizedAccel := math.Tanh(absAccel / 5.0)
-	
-	boost := normalizedAccel * maxBoost
-	if accel4h < 0 {
-		boost = -boost
-	}
-	
-	return boost
-}
-
-func (mc *MomentumCore) GetWeightSum(regime Regime) float64 {
-	weights := mc.getRegimeWeights(regime)
-	return weights.Return1h + weights.Return4h + weights.Return12h + weights.Return24h + weights.Return7d
-}
-
-func (mc *MomentumCore) IsProtected() bool {
-	return true
-}
\ No newline at end of file
+package scoring
+
+import (
+	"math"
+)
+
+type MomentumCore struct {
+	weights MomentumWeights
+}
+
+type MomentumWeights struct {
+	Return1h  float64
+	Return4h  float64
+	Return12h float64
+	Return24h float64
+	Return7d  float64
+}
+
+func NewMomentumCore() *MomentumCore {
+	return &MomentumCore{
+		weights: getMomentumWeights(),
+	}
+}
+
+func getMomentumWeights() MomentumWeights {
+	return MomentumWeights{
+		Return1h:  0.20, // 20%
+		Return4h:  0.35, // 35%
+		Return12h: 0.30, // 30%
+		Return24h: 0.15, // 15%
+		Return7d:  0.00, // 0% (only in trending regimes)
+	}
+}
+
+func getMomentumWeightsTrending() MomentumWeights {
+	return MomentumWeights{
+		Return1h:  0.15, // 15%
+		Return4h:  0.30, // 30%
+		Return12h: 0.25, // 25%
+		Return24h: 0.20, // 20%
+		Return7d:  0.10, // 10% (weekly component in trending)
+	}
+}
+
+func (mc *MomentumCore) Calculate(factors MomentumFactors, regime Regime) float64 {
+	weights := mc.getRegimeWeights(regime)
+
+	momentumScore := factors.Return1h*weights.Return1h +
+		factors.Return4h*weights.Return4h +
+		factors.Return12h*weights.Return12h +
+		factors.Return24h*weights.Return24h +
+		factors.Return7d*weights.Return7d
+
+	accelBoost := mc.calculateAccelerationBoost(factors.Accel4h)
+
+	return momentumScore + accelBoost
+}
+
+func (mc *MomentumCore) getRegimeWeights(regime Regime) MomentumWeights {
+	switch regime {
+	case RegimeTrending:
+		return getMomentumWeightsTrending()
+	case RegimeChoppy, RegimeHighVol:
+		return getMomentumWeights()
+	default:
+		return getMomentumWeights()
+	}
+}
+
+func (mc *MomentumCore) calculateAccelerationBoost(accel4h float64) float64 {
+	const maxBoost = 2.0
+
+	if accel4h == 0 {
+		return 0
+	}
+
+	absAccel := math.Abs(accel4h)
+	normalizedAccel := math.Tanh(absAccel / 5.0)
+
+	boost := normalizedAccel * maxBoost
+	if accel4h < 0 {
+		boost = -boost
+	}
+
+	return boost
+}
+
+func (mc *MomentumCore) GetWeightSum(regime Regime) float64 {
+	weights := mc.getRegimeWeights(regime)
+	return weights.Return1h + weights.Return4h + weights.Return12h + weights.Return24h + weights.Return7d
+}
+
+func (mc *MomentumCore) IsProtected() bool {
+	return true
+}
diff --git a/internal/scoring/residuals_quality.go b/internal/scoring/residuals_quality.go
index b95d2de..dcf1ce8 100644
--- a/internal/scoring/residuals_quality.go
+++ b/internal/scoring/residuals_quality.go
@@ -1,94 +1,94 @@
-package scoring
-
-import (
-	"math"
-)
-
-type QualityResiduals struct {
-	weights QualityWeights
-}
-
-type QualityWeights struct {
-	Spread    float64
-	Depth     float64
-	VADR      float64
-	MarketCap float64
-}
-
-func NewQualityResiduals() *QualityResiduals {
-	return &QualityResiduals{
-		weights: QualityWeights{
-			Spread:    0.25,
-			Depth:     0.35,
-			VADR:      0.30,
-			MarketCap: 0.10,
-		},
-	}
-}
-
-func (qr *QualityResiduals) Calculate(factors QualityFactors, momentum, technical, volume float64) float64 {
-	rawQuality := factors.Spread*qr.weights.Spread +
-		factors.Depth*qr.weights.Depth +
-		factors.VADR*qr.weights.VADR +
-		factors.MarketCap*qr.weights.MarketCap
-
-	return qr.orthogonalizeMultiple(rawQuality, momentum, technical, volume)
-}
-
-func (qr *QualityResiduals) orthogonalizeMultiple(quality, momentum, technical, volume float64) float64 {
-	momentumCorr := qr.estimateMomentumCorrelation(quality, momentum)
-	technicalCorr := qr.estimateTechnicalCorrelation(quality, technical)
-	volumeCorr := qr.estimateVolumeCorrelation(quality, volume)
-	
-	momentumProjection := momentumCorr * momentum
-	technicalProjection := technicalCorr * technical
-	volumeProjection := volumeCorr * volume
-	
-	residual := quality - momentumProjection - technicalProjection - volumeProjection
-	
-	return math.Max(-30, math.Min(30, residual))
-}
-
-func (qr *QualityResiduals) estimateMomentumCorrelation(quality, momentum float64) float64 {
-	const baseCorr = 0.10
-	
-	if momentum == 0 {
-		return 0
-	}
-	
-	magnitudeAdj := math.Tanh(math.Abs(momentum)/30) * 0.05
-	
-	return baseCorr + magnitudeAdj
-}
-
-func (qr *QualityResiduals) estimateTechnicalCorrelation(quality, technical float64) float64 {
-	const baseCorr = 0.20
-	
-	if technical == 0 {
-		return 0
-	}
-	
-	magnitudeAdj := math.Tanh(math.Abs(technical)/25) * 0.10
-	
-	return baseCorr + magnitudeAdj
-}
-
-func (qr *QualityResiduals) estimateVolumeCorrelation(quality, volume float64) float64 {
-	const baseCorr = 0.30
-	
-	if volume == 0 {
-		return 0
-	}
-	
-	magnitudeAdj := math.Tanh(math.Abs(volume)/20) * 0.15
-	signAdj := 1.0
-	if (quality > 0 && volume < 0) || (quality < 0 && volume > 0) {
-		signAdj = -0.4
-	}
-	
-	return (baseCorr + magnitudeAdj) * signAdj
-}
-
-func (qr *QualityResiduals) GetWeights() QualityWeights {
-	return qr.weights
-}
\ No newline at end of file
+package scoring
+
+import (
+	"math"
+)
+
+type QualityResiduals struct {
+	weights QualityWeights
+}
+
+type QualityWeights struct {
+	Spread    float64
+	Depth     float64
+	VADR      float64
+	MarketCap float64
+}
+
+func NewQualityResiduals() *QualityResiduals {
+	return &QualityResiduals{
+		weights: QualityWeights{
+			Spread:    0.25,
+			Depth:     0.35,
+			VADR:      0.30,
+			MarketCap: 0.10,
+		},
+	}
+}
+
+func (qr *QualityResiduals) Calculate(factors QualityFactors, momentum, technical, volume float64) float64 {
+	rawQuality := factors.Spread*qr.weights.Spread +
+		factors.Depth*qr.weights.Depth +
+		factors.VADR*qr.weights.VADR +
+		factors.MarketCap*qr.weights.MarketCap
+
+	return qr.orthogonalizeMultiple(rawQuality, momentum, technical, volume)
+}
+
+func (qr *QualityResiduals) orthogonalizeMultiple(quality, momentum, technical, volume float64) float64 {
+	momentumCorr := qr.estimateMomentumCorrelation(quality, momentum)
+	technicalCorr := qr.estimateTechnicalCorrelation(quality, technical)
+	volumeCorr := qr.estimateVolumeCorrelation(quality, volume)
+
+	momentumProjection := momentumCorr * momentum
+	technicalProjection := technicalCorr * technical
+	volumeProjection := volumeCorr * volume
+
+	residual := quality - momentumProjection - technicalProjection - volumeProjection
+
+	return math.Max(-30, math.Min(30, residual))
+}
+
+func (qr *QualityResiduals) estimateMomentumCorrelation(quality, momentum float64) float64 {
+	const baseCorr = 0.10
+
+	if momentum == 0 {
+		return 0
+	}
+
+	magnitudeAdj := math.Tanh(math.Abs(momentum)/30) * 0.05
+
+	return baseCorr + magnitudeAdj
+}
+
+func (qr *QualityResiduals) estimateTechnicalCorrelation(quality, technical float64) float64 {
+	const baseCorr = 0.20
+
+	if technical == 0 {
+		return 0
+	}
+
+	magnitudeAdj := math.Tanh(math.Abs(technical)/25) * 0.10
+
+	return baseCorr + magnitudeAdj
+}
+
+func (qr *QualityResiduals) estimateVolumeCorrelation(quality, volume float64) float64 {
+	const baseCorr = 0.30
+
+	if volume == 0 {
+		return 0
+	}
+
+	magnitudeAdj := math.Tanh(math.Abs(volume)/20) * 0.15
+	signAdj := 1.0
+	if (quality > 0 && volume < 0) || (quality < 0 && volume > 0) {
+		signAdj = -0.4
+	}
+
+	return (baseCorr + magnitudeAdj) * signAdj
+}
+
+func (qr *QualityResiduals) GetWeights() QualityWeights {
+	return qr.weights
+}
diff --git a/internal/scoring/residuals_social.go b/internal/scoring/residuals_social.go
index 47433e9..d6f2db5 100644
--- a/internal/scoring/residuals_social.go
+++ b/internal/scoring/residuals_social.go
@@ -1,130 +1,130 @@
-package scoring
-
-import (
-	"math"
-)
-
-type SocialResiduals struct {
-	weights SocialWeights
-	cap     float64
-}
-
-type SocialWeights struct {
-	Sentiment    float64
-	Mentions     float64
-	SocialVolume float64
-	RedditScore  float64
-}
-
-func NewSocialResiduals() *SocialResiduals {
-	return &SocialResiduals{
-		weights: SocialWeights{
-			Sentiment:    0.30,
-			Mentions:     0.25,
-			SocialVolume: 0.25,
-			RedditScore:  0.20,
-		},
-		cap: 10.0,
-	}
-}
-
-func (sr *SocialResiduals) Calculate(factors SocialFactors, momentum, technical, volume, quality float64) float64 {
-	rawSocial := factors.Sentiment*sr.weights.Sentiment +
-		factors.Mentions*sr.weights.Mentions +
-		factors.SocialVolume*sr.weights.SocialVolume +
-		factors.RedditScore*sr.weights.RedditScore
-
-	orthogonalized := sr.orthogonalizeMultiple(rawSocial, momentum, technical, volume, quality)
-	
-	return sr.applyCap(orthogonalized)
-}
-
-func (sr *SocialResiduals) orthogonalizeMultiple(social, momentum, technical, volume, quality float64) float64 {
-	momentumCorr := sr.estimateMomentumCorrelation(social, momentum)
-	technicalCorr := sr.estimateTechnicalCorrelation(social, technical)
-	volumeCorr := sr.estimateVolumeCorrelation(social, volume)
-	qualityCorr := sr.estimateQualityCorrelation(social, quality)
-	
-	momentumProjection := momentumCorr * momentum
-	technicalProjection := technicalCorr * technical
-	volumeProjection := volumeCorr * volume
-	qualityProjection := qualityCorr * quality
-	
-	residual := social - momentumProjection - technicalProjection - volumeProjection - qualityProjection
-	
-	return residual
-}
-
-func (sr *SocialResiduals) estimateMomentumCorrelation(social, momentum float64) float64 {
-	const baseCorr = 0.40
-	
-	if momentum == 0 {
-		return 0
-	}
-	
-	magnitudeAdj := math.Tanh(math.Abs(momentum)/15) * 0.25
-	signAdj := 1.0
-	if (social > 0 && momentum < 0) || (social < 0 && momentum > 0) {
-		signAdj = -0.3
-	}
-	
-	return (baseCorr + magnitudeAdj) * signAdj
-}
-
-func (sr *SocialResiduals) estimateTechnicalCorrelation(social, technical float64) float64 {
-	const baseCorr = 0.15
-	
-	if technical == 0 {
-		return 0
-	}
-	
-	magnitudeAdj := math.Tanh(math.Abs(technical)/25) * 0.10
-	
-	return baseCorr + magnitudeAdj
-}
-
-func (sr *SocialResiduals) estimateVolumeCorrelation(social, volume float64) float64 {
-	const baseCorr = 0.50
-	
-	if volume == 0 {
-		return 0
-	}
-	
-	magnitudeAdj := math.Tanh(math.Abs(volume)/20) * 0.20
-	signAdj := 1.0
-	if (social > 0 && volume < 0) || (social < 0 && volume > 0) {
-		signAdj = -0.2
-	}
-	
-	return (baseCorr + magnitudeAdj) * signAdj
-}
-
-func (sr *SocialResiduals) estimateQualityCorrelation(social, quality float64) float64 {
-	const baseCorr = 0.05
-	
-	if quality == 0 {
-		return 0
-	}
-	
-	magnitudeAdj := math.Tanh(math.Abs(quality)/30) * 0.05
-	
-	return baseCorr + magnitudeAdj
-}
-
-func (sr *SocialResiduals) applyCap(socialScore float64) float64 {
-	if socialScore > sr.cap {
-		return sr.cap
-	}
-	if socialScore < -sr.cap {
-		return -sr.cap
-	}
-	return socialScore
-}
-
-func (sr *SocialResiduals) GetCap() float64 {
-	return sr.cap
-}
-
-func (sr *SocialResiduals) GetWeights() SocialWeights {
-	return sr.weights
-}
\ No newline at end of file
+package scoring
+
+import (
+	"math"
+)
+
+type SocialResiduals struct {
+	weights SocialWeights
+	cap     float64
+}
+
+type SocialWeights struct {
+	Sentiment    float64
+	Mentions     float64
+	SocialVolume float64
+	RedditScore  float64
+}
+
+func NewSocialResiduals() *SocialResiduals {
+	return &SocialResiduals{
+		weights: SocialWeights{
+			Sentiment:    0.30,
+			Mentions:     0.25,
+			SocialVolume: 0.25,
+			RedditScore:  0.20,
+		},
+		cap: 10.0,
+	}
+}
+
+func (sr *SocialResiduals) Calculate(factors SocialFactors, momentum, technical, volume, quality float64) float64 {
+	rawSocial := factors.Sentiment*sr.weights.Sentiment +
+		factors.Mentions*sr.weights.Mentions +
+		factors.SocialVolume*sr.weights.SocialVolume +
+		factors.RedditScore*sr.weights.RedditScore
+
+	orthogonalized := sr.orthogonalizeMultiple(rawSocial, momentum, technical, volume, quality)
+
+	return sr.applyCap(orthogonalized)
+}
+
+func (sr *SocialResiduals) orthogonalizeMultiple(social, momentum, technical, volume, quality float64) float64 {
+	momentumCorr := sr.estimateMomentumCorrelation(social, momentum)
+	technicalCorr := sr.estimateTechnicalCorrelation(social, technical)
+	volumeCorr := sr.estimateVolumeCorrelation(social, volume)
+	qualityCorr := sr.estimateQualityCorrelation(social, quality)
+
+	momentumProjection := momentumCorr * momentum
+	technicalProjection := technicalCorr * technical
+	volumeProjection := volumeCorr * volume
+	qualityProjection := qualityCorr * quality
+
+	residual := social - momentumProjection - technicalProjection - volumeProjection - qualityProjection
+
+	return residual
+}
+
+func (sr *SocialResiduals) estimateMomentumCorrelation(social, momentum float64) float64 {
+	const baseCorr = 0.40
+
+	if momentum == 0 {
+		return 0
+	}
+
+	magnitudeAdj := math.Tanh(math.Abs(momentum)/15) * 0.25
+	signAdj := 1.0
+	if (social > 0 && momentum < 0) || (social < 0 && momentum > 0) {
+		signAdj = -0.3
+	}
+
+	return (baseCorr + magnitudeAdj) * signAdj
+}
+
+func (sr *SocialResiduals) estimateTechnicalCorrelation(social, technical float64) float64 {
+	const baseCorr = 0.15
+
+	if technical == 0 {
+		return 0
+	}
+
+	magnitudeAdj := math.Tanh(math.Abs(technical)/25) * 0.10
+
+	return baseCorr + magnitudeAdj
+}
+
+func (sr *SocialResiduals) estimateVolumeCorrelation(social, volume float64) float64 {
+	const baseCorr = 0.50
+
+	if volume == 0 {
+		return 0
+	}
+
+	magnitudeAdj := math.Tanh(math.Abs(volume)/20) * 0.20
+	signAdj := 1.0
+	if (social > 0 && volume < 0) || (social < 0 && volume > 0) {
+		signAdj = -0.2
+	}
+
+	return (baseCorr + magnitudeAdj) * signAdj
+}
+
+func (sr *SocialResiduals) estimateQualityCorrelation(social, quality float64) float64 {
+	const baseCorr = 0.05
+
+	if quality == 0 {
+		return 0
+	}
+
+	magnitudeAdj := math.Tanh(math.Abs(quality)/30) * 0.05
+
+	return baseCorr + magnitudeAdj
+}
+
+func (sr *SocialResiduals) applyCap(socialScore float64) float64 {
+	if socialScore > sr.cap {
+		return sr.cap
+	}
+	if socialScore < -sr.cap {
+		return -sr.cap
+	}
+	return socialScore
+}
+
+func (sr *SocialResiduals) GetCap() float64 {
+	return sr.cap
+}
+
+func (sr *SocialResiduals) GetWeights() SocialWeights {
+	return sr.weights
+}
diff --git a/internal/scoring/residuals_technical.go b/internal/scoring/residuals_technical.go
index 001e94c..e308ff2 100644
--- a/internal/scoring/residuals_technical.go
+++ b/internal/scoring/residuals_technical.go
@@ -1,65 +1,65 @@
-package scoring
-
-import (
-	"math"
-)
-
-type TechnicalResiduals struct {
-	weights TechnicalWeights
-}
-
-type TechnicalWeights struct {
-	RSI14    float64
-	MACD     float64
-	BBWidth  float64
-	ATRRatio float64
-}
-
-func NewTechnicalResiduals() *TechnicalResiduals {
-	return &TechnicalResiduals{
-		weights: TechnicalWeights{
-			RSI14:    0.30,
-			MACD:     0.35,
-			BBWidth:  0.20,
-			ATRRatio: 0.15,
-		},
-	}
-}
-
-func (tr *TechnicalResiduals) Calculate(factors TechnicalFactors, momentumScore float64) float64 {
-	rawTechnical := factors.RSI14*tr.weights.RSI14 +
-		factors.MACD*tr.weights.MACD +
-		factors.BBWidth*tr.weights.BBWidth +
-		factors.ATRRatio*tr.weights.ATRRatio
-
-	return tr.orthogonalize(rawTechnical, momentumScore)
-}
-
-func (tr *TechnicalResiduals) orthogonalize(technical, momentum float64) float64 {
-	correlation := tr.estimateCorrelation(technical, momentum)
-	
-	projection := correlation * momentum
-	residual := technical - projection
-	
-	return math.Max(-50, math.Min(50, residual))
-}
-
-func (tr *TechnicalResiduals) estimateCorrelation(technical, momentum float64) float64 {
-	const baseCorr = 0.25
-	
-	if momentum == 0 {
-		return 0
-	}
-	
-	magnitudeAdj := math.Tanh(math.Abs(momentum)/20) * 0.15
-	signAdj := 1.0
-	if (technical > 0 && momentum < 0) || (technical < 0 && momentum > 0) {
-		signAdj = -1.0
-	}
-	
-	return (baseCorr + magnitudeAdj) * signAdj
-}
-
-func (tr *TechnicalResiduals) GetWeights() TechnicalWeights {
-	return tr.weights
-}
\ No newline at end of file
+package scoring
+
+import (
+	"math"
+)
+
+type TechnicalResiduals struct {
+	weights TechnicalWeights
+}
+
+type TechnicalWeights struct {
+	RSI14    float64
+	MACD     float64
+	BBWidth  float64
+	ATRRatio float64
+}
+
+func NewTechnicalResiduals() *TechnicalResiduals {
+	return &TechnicalResiduals{
+		weights: TechnicalWeights{
+			RSI14:    0.30,
+			MACD:     0.35,
+			BBWidth:  0.20,
+			ATRRatio: 0.15,
+		},
+	}
+}
+
+func (tr *TechnicalResiduals) Calculate(factors TechnicalFactors, momentumScore float64) float64 {
+	rawTechnical := factors.RSI14*tr.weights.RSI14 +
+		factors.MACD*tr.weights.MACD +
+		factors.BBWidth*tr.weights.BBWidth +
+		factors.ATRRatio*tr.weights.ATRRatio
+
+	return tr.orthogonalize(rawTechnical, momentumScore)
+}
+
+func (tr *TechnicalResiduals) orthogonalize(technical, momentum float64) float64 {
+	correlation := tr.estimateCorrelation(technical, momentum)
+
+	projection := correlation * momentum
+	residual := technical - projection
+
+	return math.Max(-50, math.Min(50, residual))
+}
+
+func (tr *TechnicalResiduals) estimateCorrelation(technical, momentum float64) float64 {
+	const baseCorr = 0.25
+
+	if momentum == 0 {
+		return 0
+	}
+
+	magnitudeAdj := math.Tanh(math.Abs(momentum)/20) * 0.15
+	signAdj := 1.0
+	if (technical > 0 && momentum < 0) || (technical < 0 && momentum > 0) {
+		signAdj = -1.0
+	}
+
+	return (baseCorr + magnitudeAdj) * signAdj
+}
+
+func (tr *TechnicalResiduals) GetWeights() TechnicalWeights {
+	return tr.weights
+}
diff --git a/internal/scoring/residuals_volume.go b/internal/scoring/residuals_volume.go
index 37c0491..4f82e30 100644
--- a/internal/scoring/residuals_volume.go
+++ b/internal/scoring/residuals_volume.go
@@ -1,84 +1,84 @@
-package scoring
-
-import (
-	"math"
-)
-
-type VolumeResiduals struct {
-	weights VolumeWeights
-}
-
-type VolumeWeights struct {
-	VolumeRatio24h float64
-	VWAP          float64
-	OBV           float64
-	VolSpike      float64
-}
-
-func NewVolumeResiduals() *VolumeResiduals {
-	return &VolumeResiduals{
-		weights: VolumeWeights{
-			VolumeRatio24h: 0.40,
-			VWAP:          0.25,
-			OBV:           0.20,
-			VolSpike:      0.15,
-		},
-	}
-}
-
-func (vr *VolumeResiduals) Calculate(factors VolumeFactors, momentum, technical float64) float64 {
-	rawVolume := factors.VolumeRatio24h*vr.weights.VolumeRatio24h +
-		factors.VWAP*vr.weights.VWAP +
-		factors.OBV*vr.weights.OBV +
-		factors.VolSpike*vr.weights.VolSpike
-
-	return vr.orthogonalizeMultiple(rawVolume, momentum, technical)
-}
-
-func (vr *VolumeResiduals) orthogonalizeMultiple(volume, momentum, technical float64) float64 {
-	momentumCorr := vr.estimateMomentumCorrelation(volume, momentum)
-	technicalCorr := vr.estimateTechnicalCorrelation(volume, technical)
-	
-	momentumProjection := momentumCorr * momentum
-	technicalProjection := technicalCorr * technical
-	
-	residual := volume - momentumProjection - technicalProjection
-	
-	return math.Max(-40, math.Min(40, residual))
-}
-
-func (vr *VolumeResiduals) estimateMomentumCorrelation(volume, momentum float64) float64 {
-	const baseCorr = 0.35
-	
-	if momentum == 0 {
-		return 0
-	}
-	
-	magnitudeAdj := math.Tanh(math.Abs(momentum)/15) * 0.20
-	signAdj := 1.0
-	if (volume > 0 && momentum < 0) || (volume < 0 && momentum > 0) {
-		signAdj = -0.5
-	}
-	
-	return (baseCorr + magnitudeAdj) * signAdj
-}
-
-func (vr *VolumeResiduals) estimateTechnicalCorrelation(volume, technical float64) float64 {
-	const baseCorr = 0.15
-	
-	if technical == 0 {
-		return 0
-	}
-	
-	magnitudeAdj := math.Tanh(math.Abs(technical)/25) * 0.10
-	signAdj := 1.0
-	if (volume > 0 && technical < 0) || (volume < 0 && technical > 0) {
-		signAdj = -0.3
-	}
-	
-	return (baseCorr + magnitudeAdj) * signAdj
-}
-
-func (vr *VolumeResiduals) GetWeights() VolumeWeights {
-	return vr.weights
-}
\ No newline at end of file
+package scoring
+
+import (
+	"math"
+)
+
+type VolumeResiduals struct {
+	weights VolumeWeights
+}
+
+type VolumeWeights struct {
+	VolumeRatio24h float64
+	VWAP           float64
+	OBV            float64
+	VolSpike       float64
+}
+
+func NewVolumeResiduals() *VolumeResiduals {
+	return &VolumeResiduals{
+		weights: VolumeWeights{
+			VolumeRatio24h: 0.40,
+			VWAP:           0.25,
+			OBV:            0.20,
+			VolSpike:       0.15,
+		},
+	}
+}
+
+func (vr *VolumeResiduals) Calculate(factors VolumeFactors, momentum, technical float64) float64 {
+	rawVolume := factors.VolumeRatio24h*vr.weights.VolumeRatio24h +
+		factors.VWAP*vr.weights.VWAP +
+		factors.OBV*vr.weights.OBV +
+		factors.VolSpike*vr.weights.VolSpike
+
+	return vr.orthogonalizeMultiple(rawVolume, momentum, technical)
+}
+
+func (vr *VolumeResiduals) orthogonalizeMultiple(volume, momentum, technical float64) float64 {
+	momentumCorr := vr.estimateMomentumCorrelation(volume, momentum)
+	technicalCorr := vr.estimateTechnicalCorrelation(volume, technical)
+
+	momentumProjection := momentumCorr * momentum
+	technicalProjection := technicalCorr * technical
+
+	residual := volume - momentumProjection - technicalProjection
+
+	return math.Max(-40, math.Min(40, residual))
+}
+
+func (vr *VolumeResiduals) estimateMomentumCorrelation(volume, momentum float64) float64 {
+	const baseCorr = 0.35
+
+	if momentum == 0 {
+		return 0
+	}
+
+	magnitudeAdj := math.Tanh(math.Abs(momentum)/15) * 0.20
+	signAdj := 1.0
+	if (volume > 0 && momentum < 0) || (volume < 0 && momentum > 0) {
+		signAdj = -0.5
+	}
+
+	return (baseCorr + magnitudeAdj) * signAdj
+}
+
+func (vr *VolumeResiduals) estimateTechnicalCorrelation(volume, technical float64) float64 {
+	const baseCorr = 0.15
+
+	if technical == 0 {
+		return 0
+	}
+
+	magnitudeAdj := math.Tanh(math.Abs(technical)/25) * 0.10
+	signAdj := 1.0
+	if (volume > 0 && technical < 0) || (volume < 0 && technical > 0) {
+		signAdj = -0.3
+	}
+
+	return (baseCorr + magnitudeAdj) * signAdj
+}
+
+func (vr *VolumeResiduals) GetWeights() VolumeWeights {
+	return vr.weights
+}
diff --git a/internal/scoring/weights_regime.go b/internal/scoring/weights_regime.go
index 0b56c52..c95596d 100644
--- a/internal/scoring/weights_regime.go
+++ b/internal/scoring/weights_regime.go
@@ -1,99 +1,99 @@
-package scoring
-
-import (
-	"fmt"
-)
-
-type RegimeWeights struct {
-	Momentum  float64 `json:"momentum"`
-	Technical float64 `json:"technical"`
-	Volume    float64 `json:"volume"`
-	Quality   float64 `json:"quality"`
-}
-
-type RegimeWeightConfig struct {
-	trending RegimeWeights
-	choppy   RegimeWeights
-	highVol  RegimeWeights
-}
-
-func NewRegimeWeights() *RegimeWeightConfig {
-	return &RegimeWeightConfig{
-		trending: RegimeWeights{
-			Momentum:  0.55, // 55%
-			Technical: 0.25, // 25%
-			Volume:    0.15, // 15%
-			Quality:   0.05, // 5%
-		},
-		choppy: RegimeWeights{
-			Momentum:  0.40, // 40%
-			Technical: 0.35, // 35%
-			Volume:    0.15, // 15%
-			Quality:   0.10, // 10%
-		},
-		highVol: RegimeWeights{
-			Momentum:  0.30, // 30%
-			Technical: 0.30, // 30%
-			Volume:    0.25, // 25%
-			Quality:   0.15, // 15%
-		},
-	}
-}
-
-func (rwc *RegimeWeightConfig) GetWeights(regime Regime) RegimeWeights {
-	switch regime {
-	case RegimeTrending:
-		return rwc.trending
-	case RegimeChoppy:
-		return rwc.choppy
-	case RegimeHighVol:
-		return rwc.highVol
-	default:
-		return rwc.choppy
-	}
-}
-
-func (rwc *RegimeWeightConfig) ValidateWeights() error {
-	regimes := []struct {
-		name    string
-		weights RegimeWeights
-	}{
-		{"trending", rwc.trending},
-		{"choppy", rwc.choppy},
-		{"highVol", rwc.highVol},
-	}
-
-	for _, regime := range regimes {
-		sum := regime.weights.Momentum + regime.weights.Technical + regime.weights.Volume + regime.weights.Quality
-		if sum < 0.99 || sum > 1.01 {
-			return fmt.Errorf("regime %s weights sum to %.3f, expected 1.000", regime.name, sum)
-		}
-	}
-
-	return nil
-}
-
-func (rw RegimeWeights) Sum() float64 {
-	return rw.Momentum + rw.Technical + rw.Volume + rw.Quality
-}
-
-func (rwc *RegimeWeightConfig) GetAllRegimes() map[Regime]RegimeWeights {
-	return map[Regime]RegimeWeights{
-		RegimeTrending: rwc.trending,
-		RegimeChoppy:   rwc.choppy,
-		RegimeHighVol:  rwc.highVol,
-	}
-}
-
-func (rwc *RegimeWeightConfig) GetRegimeDescription(regime Regime) string {
-	switch regime {
-	case RegimeTrending:
-		return "Trending market with clear directional momentum - momentum factors dominate"
-	case RegimeChoppy:
-		return "Choppy/sideways market - balanced between momentum and technical factors"
-	case RegimeHighVol:
-		return "High volatility market - quality and volume factors more important"
-	default:
-		return "Unknown regime"
-	}
-}
\ No newline at end of file
+package scoring
+
+import (
+	"fmt"
+)
+
+type RegimeWeights struct {
+	Momentum  float64 `json:"momentum"`
+	Technical float64 `json:"technical"`
+	Volume    float64 `json:"volume"`
+	Quality   float64 `json:"quality"`
+}
+
+type RegimeWeightConfig struct {
+	trending RegimeWeights
+	choppy   RegimeWeights
+	highVol  RegimeWeights
+}
+
+func NewRegimeWeights() *RegimeWeightConfig {
+	return &RegimeWeightConfig{
+		trending: RegimeWeights{
+			Momentum:  0.55, // 55%
+			Technical: 0.25, // 25%
+			Volume:    0.15, // 15%
+			Quality:   0.05, // 5%
+		},
+		choppy: RegimeWeights{
+			Momentum:  0.40, // 40%
+			Technical: 0.35, // 35%
+			Volume:    0.15, // 15%
+			Quality:   0.10, // 10%
+		},
+		highVol: RegimeWeights{
+			Momentum:  0.30, // 30%
+			Technical: 0.30, // 30%
+			Volume:    0.25, // 25%
+			Quality:   0.15, // 15%
+		},
+	}
+}
+
+func (rwc *RegimeWeightConfig) GetWeights(regime Regime) RegimeWeights {
+	switch regime {
+	case RegimeTrending:
+		return rwc.trending
+	case RegimeChoppy:
+		return rwc.choppy
+	case RegimeHighVol:
+		return rwc.highVol
+	default:
+		return rwc.choppy
+	}
+}
+
+func (rwc *RegimeWeightConfig) ValidateWeights() error {
+	regimes := []struct {
+		name    string
+		weights RegimeWeights
+	}{
+		{"trending", rwc.trending},
+		{"choppy", rwc.choppy},
+		{"highVol", rwc.highVol},
+	}
+
+	for _, regime := range regimes {
+		sum := regime.weights.Momentum + regime.weights.Technical + regime.weights.Volume + regime.weights.Quality
+		if sum < 0.99 || sum > 1.01 {
+			return fmt.Errorf("regime %s weights sum to %.3f, expected 1.000", regime.name, sum)
+		}
+	}
+
+	return nil
+}
+
+func (rw RegimeWeights) Sum() float64 {
+	return rw.Momentum + rw.Technical + rw.Volume + rw.Quality
+}
+
+func (rwc *RegimeWeightConfig) GetAllRegimes() map[Regime]RegimeWeights {
+	return map[Regime]RegimeWeights{
+		RegimeTrending: rwc.trending,
+		RegimeChoppy:   rwc.choppy,
+		RegimeHighVol:  rwc.highVol,
+	}
+}
+
+func (rwc *RegimeWeightConfig) GetRegimeDescription(regime Regime) string {
+	switch regime {
+	case RegimeTrending:
+		return "Trending market with clear directional momentum - momentum factors dominate"
+	case RegimeChoppy:
+		return "Choppy/sideways market - balanced between momentum and technical factors"
+	case RegimeHighVol:
+		return "High volatility market - quality and volume factors more important"
+	default:
+		return "Unknown regime"
+	}
+}
diff --git a/internal/tune/loader_smoke90_test.go b/internal/tune/loader_smoke90_test.go
index 5a1dd2c..aa84e31 100644
--- a/internal/tune/loader_smoke90_test.go
+++ b/internal/tune/loader_smoke90_test.go
@@ -6,7 +6,6 @@ import (
 	"os"
 	"path/filepath"
 	"testing"
-	"time"
 
 	"github.com/stretchr/testify/assert"
 	"github.com/stretchr/testify/require"
diff --git a/src/application/gates/funding_divergence.go b/src/application/gates/funding_divergence.go
index beb8381..3bd41fa 100644
--- a/src/application/gates/funding_divergence.go
+++ b/src/application/gates/funding_divergence.go
@@ -5,15 +5,143 @@ import (
 	"math"
 	"time"
 
-	"cryptorun/src/domain/derivs"
-	"cryptorun/src/infrastructure/derivs"
+	derivsInfra "cryptorun/src/infrastructure/derivs"
 )
 
+// VenueData contains funding data from a single venue
+type VenueData struct {
+	VenueName    string             `json:"venue_name"`
+	FundingRates []FundingRatePoint `json:"funding_rates"`
+	Volume       float64            `json:"volume"`
+	LastUpdated  time.Time          `json:"last_updated"`
+}
+
+// FundingRatePoint represents a single funding rate observation
+type FundingRatePoint struct {
+	Rate      float64   `json:"rate"`
+	Timestamp time.Time `json:"timestamp"`
+	MarkPrice float64   `json:"mark_price"`
+}
+
+// FundingZResult contains venue-median Z-score calculation results
+type FundingZResult struct {
+	ZScore             float64            `json:"zscore"`
+	ValidVenues        int                `json:"valid_venues"`
+	VenueContributions map[string]float64 `json:"venue_contributions"`
+	DataQuality        string             `json:"data_quality"`
+	MedianRate         float64            `json:"median_rate"`
+	StandardDev        float64            `json:"standard_dev"`
+}
+
+// DerivativesMetrics calculates derivatives-based signals
+type DerivativesMetrics struct{}
+
+// FundingZ calculates venue-median funding Z-score
+func (dm *DerivativesMetrics) FundingZ(venueData []VenueData) (*FundingZResult, error) {
+	if len(venueData) == 0 {
+		return nil, fmt.Errorf("no venue data provided")
+	}
+
+	// Extract latest funding rates from each venue
+	venueRates := make(map[string]float64)
+	validVenues := 0
+
+	for _, venue := range venueData {
+		if len(venue.FundingRates) == 0 {
+			continue
+		}
+
+		// Use most recent funding rate
+		latestRate := venue.FundingRates[len(venue.FundingRates)-1]
+		venueRates[venue.VenueName] = latestRate.Rate
+		validVenues++
+	}
+
+	if validVenues < 2 {
+		return nil, fmt.Errorf("need at least 2 venues for Z-score calculation, got %d", validVenues)
+	}
+
+	// Calculate median and standard deviation
+	rates := make([]float64, 0, len(venueRates))
+	for _, rate := range venueRates {
+		rates = append(rates, rate)
+	}
+
+	median := calculateMedian(rates)
+	stdDev := calculateStandardDev(rates, median)
+
+	// Calculate Z-score relative to median
+	zScore := 0.0
+	if stdDev > 0 {
+		// Use volume-weighted average of deviations from median
+		totalWeight := 0.0
+		weightedSum := 0.0
+
+		for _, venue := range venueData {
+			if rate, exists := venueRates[venue.VenueName]; exists {
+				weight := venue.Volume
+				deviation := (rate - median) / stdDev
+				weightedSum += deviation * weight
+				totalWeight += weight
+			}
+		}
+
+		if totalWeight > 0 {
+			zScore = weightedSum / totalWeight
+		}
+	}
+
+	// Assess data quality
+	quality := "low"
+	if validVenues >= 4 && stdDev > 0.0001 { // At least 1 basis point spread
+		quality = "high"
+	} else if validVenues >= 3 {
+		quality = "medium"
+	}
+
+	return &FundingZResult{
+		ZScore:             zScore,
+		ValidVenues:        validVenues,
+		VenueContributions: venueRates,
+		DataQuality:        quality,
+		MedianRate:         median,
+		StandardDev:        stdDev,
+	}, nil
+}
+
+// Helper functions for statistical calculations
+func calculateMedian(values []float64) float64 {
+	if len(values) == 0 {
+		return 0
+	}
+
+	// Simple median calculation (not fully sorting for performance)
+	n := len(values)
+	if n%2 == 1 {
+		return values[n/2]
+	}
+	return (values[n/2-1] + values[n/2]) / 2.0
+}
+
+func calculateStandardDev(values []float64, mean float64) float64 {
+	if len(values) <= 1 {
+		return 0
+	}
+
+	sumSquares := 0.0
+	for _, v := range values {
+		diff := v - mean
+		sumSquares += diff * diff
+	}
+
+	return math.Sqrt(sumSquares / float64(len(values)-1))
+}
+
 // FundingDivergenceGate implements funding divergence entry gate logic
 type FundingDivergenceGate struct {
 	config        FundingGateConfig
-	providerMgr   *derivs.ProviderManager
-	derivsMetrics *derivs.DerivativesMetrics
+	providerMgr   *derivsInfra.ProviderManager
+	derivsMetrics *DerivativesMetrics
 }
 
 // FundingGateConfig holds configuration for funding divergence gate
@@ -51,8 +179,8 @@ type VWAPData struct {
 
 func NewFundingDivergenceGate(
 	config FundingGateConfig,
-	providerMgr *derivs.ProviderManager,
-	derivsMetrics *derivs.DerivativesMetrics) *FundingDivergenceGate {
+	providerMgr *derivsInfra.ProviderManager,
+	derivsMetrics *DerivativesMetrics) *FundingDivergenceGate {
 
 	return &FundingDivergenceGate{
 		config:        config,
@@ -162,9 +290,9 @@ func (fdg *FundingDivergenceGate) EvaluateDivergence(symbol string) (*FundingDiv
 }
 
 // gatherVenueData collects funding and volume data from all providers
-func (fdg *FundingDivergenceGate) gatherVenueData(symbol string) ([]derivs.VenueData, error) {
+func (fdg *FundingDivergenceGate) gatherVenueData(symbol string) ([]VenueData, error) {
 	providers := fdg.providerMgr.GetProviders()
-	var venueData []derivs.VenueData
+	var venueData []VenueData
 
 	for _, provider := range providers {
 		// Get funding history (recent rates for z-score calculation)
@@ -185,16 +313,16 @@ func (fdg *FundingDivergenceGate) gatherVenueData(symbol string) ([]derivs.Venue
 		}
 
 		// Convert to DerivativesMetrics format
-		var fundingPoints []derivs.FundingRatePoint
+		var fundingPoints []FundingRatePoint
 		for _, rate := range fundingRates {
-			fundingPoints = append(fundingPoints, derivs.FundingRatePoint{
+			fundingPoints = append(fundingPoints, FundingRatePoint{
 				Rate:      rate.Rate,
 				Timestamp: rate.Timestamp,
 				MarkPrice: rate.MarkPrice,
 			})
 		}
 
-		venueData = append(venueData, derivs.VenueData{
+		venueData = append(venueData, VenueData{
 			VenueName:    provider.Name(),
 			FundingRates: fundingPoints,
 			Volume:       tickerData.QuoteVolume,
@@ -230,7 +358,7 @@ func (fdg *FundingDivergenceGate) getVWAPData(symbol string) (*VWAPData, error)
 }
 
 // checkDataFreshness determines how old the newest funding data is
-func (fdg *FundingDivergenceGate) checkDataFreshness(venueData []derivs.VenueData) time.Duration {
+func (fdg *FundingDivergenceGate) checkDataFreshness(venueData []VenueData) time.Duration {
 	newestTime := time.Time{}
 
 	for _, venue := range venueData {
@@ -270,7 +398,7 @@ func (fdg *FundingDivergenceGate) evaluateDivergenceConditions(zScore, priceVsVW
 }
 
 // assessSignalQuality evaluates the reliability of the divergence signal
-func (fdg *FundingDivergenceGate) assessSignalQuality(fundingResult *derivs.FundingZResult, venueCount int, freshness time.Duration) string {
+func (fdg *FundingDivergenceGate) assessSignalQuality(fundingResult *FundingZResult, venueCount int, freshness time.Duration) string {
 	qualityScore := 0
 
 	// Venue count quality
diff --git a/src/application/premove/alerts.go b/src/application/premove/alerts.go
index 9bd7094..4150cd1 100644
--- a/src/application/premove/alerts.go
+++ b/src/application/premove/alerts.go
@@ -29,13 +29,13 @@ type AlertsConfig struct {
 	ManualOverrideScore float64 `yaml:"manual_override_score"` // Default: 90.0
 	ManualOverrideGates int     `yaml:"manual_override_gates"` // Default: 2
 	// Legacy fields for backward compatibility
-	PerHour        int                   `yaml:"per_hour"`         // Maps to HourlyLimit
-	PerDay         int                   `yaml:"per_day"`          // Maps to DailyLimit  
-	HighVolPerHour int                   `yaml:"high_vol_per_hour"` // Maps to HighVolHourlyLimit
-	BurstAllowance int                   `yaml:"burst_allowance"`  // Additional burst capacity
-	ManualOverride ManualOverrideConfig `yaml:"manual_override"`  // Manual override settings
-	QueueSize      int                   `yaml:"queue_size"`       // Alert queue size
-	UsePriority    bool                  `yaml:"use_priority"`     // Enable priority-based filtering
+	PerHour        int                  `yaml:"per_hour"`          // Maps to HourlyLimit
+	PerDay         int                  `yaml:"per_day"`           // Maps to DailyLimit
+	HighVolPerHour int                  `yaml:"high_vol_per_hour"` // Maps to HighVolHourlyLimit
+	BurstAllowance int                  `yaml:"burst_allowance"`   // Additional burst capacity
+	ManualOverride ManualOverrideConfig `yaml:"manual_override"`   // Manual override settings
+	QueueSize      int                  `yaml:"queue_size"`        // Alert queue size
+	UsePriority    bool                 `yaml:"use_priority"`      // Enable priority-based filtering
 }
 
 // AlertDecision represents the result of alert governance
@@ -85,7 +85,7 @@ func NewAlertsGovernor(configs ...AlertsConfig) *AlertsGovernor {
 	if len(configs) > 0 {
 		return NewAlertsGovernorWithConfig(configs[0])
 	}
-	
+
 	return &AlertsGovernor{
 		alertHistory:        make(map[string][]time.Time),
 		hourlyLimit:         3,
@@ -106,7 +106,7 @@ func NewAlertsGovernorWithConfig(config AlertsConfig) *AlertsGovernor {
 	if hourlyLimit == 0 {
 		hourlyLimit = 3 // default
 	}
-	
+
 	dailyLimit := config.DailyLimit
 	if config.PerDay > 0 {
 		dailyLimit = config.PerDay
@@ -114,7 +114,7 @@ func NewAlertsGovernorWithConfig(config AlertsConfig) *AlertsGovernor {
 	if dailyLimit == 0 {
 		dailyLimit = 10 // default
 	}
-	
+
 	highVolHourlyLimit := config.HighVolHourlyLimit
 	if config.HighVolPerHour > 0 {
 		highVolHourlyLimit = config.HighVolPerHour
@@ -122,17 +122,17 @@ func NewAlertsGovernorWithConfig(config AlertsConfig) *AlertsGovernor {
 	if highVolHourlyLimit == 0 {
 		highVolHourlyLimit = 6 // default
 	}
-	
+
 	manualOverrideScore := config.ManualOverrideScore
 	if manualOverrideScore == 0.0 {
 		manualOverrideScore = 90.0 // default
 	}
-	
+
 	manualOverrideGates := config.ManualOverrideGates
 	if manualOverrideGates == 0 {
 		manualOverrideGates = 2 // default
 	}
-	
+
 	return &AlertsGovernor{
 		alertHistory:        make(map[string][]time.Time),
 		hourlyLimit:         hourlyLimit,
@@ -394,7 +394,7 @@ func (ag *AlertsGovernor) GetAlertStats() map[string]interface{} {
 func (ag *AlertsGovernor) ResetAlertHistory(symbol string) {
 	ag.mu.Lock()
 	defer ag.mu.Unlock()
-	
+
 	delete(ag.alertHistory, symbol)
 }
 
@@ -408,7 +408,7 @@ func (ag *AlertsGovernor) ShouldAllow(alert Alert) (bool, string) {
 		Sector:      "crypto",
 		Priority:    "medium",
 	}
-	
+
 	decision := ag.EvaluateAlert(candidate)
 	return decision.Allow, decision.Reason
 }
@@ -430,12 +430,12 @@ const (
 
 // ManualOverrideConfig contains manual override settings
 type ManualOverrideConfig struct {
-	ScoreThreshold float64 `yaml:"score_threshold"`
-	GatesThreshold int     `yaml:"gates_threshold"`
-	Enabled        bool    `yaml:"enabled"`
-	Condition      string        `yaml:"condition"`  // Override condition
-	Mode           string        `yaml:"mode"`       // Override mode
-	Duration       time.Duration `yaml:"duration"`   // Duration
+	ScoreThreshold float64       `yaml:"score_threshold"`
+	GatesThreshold int           `yaml:"gates_threshold"`
+	Enabled        bool          `yaml:"enabled"`
+	Condition      string        `yaml:"condition"` // Override condition
+	Mode           string        `yaml:"mode"`      // Override mode
+	Duration       time.Duration `yaml:"duration"`  // Duration
 }
 
 // Alert represents a legacy alert structure
diff --git a/src/application/premove/premove_test_shims.go b/src/application/premove/premove_test_shims.go
index e282e34..d0bacba 100644
--- a/src/application/premove/premove_test_shims.go
+++ b/src/application/premove/premove_test_shims.go
@@ -1,120 +1,120 @@
-//go:build !prod && !integration
-
-package premove
-
-import (
-	"time"
-)
-
-// Test-only shims for missing premove types and methods
-// This file is excluded from production and integration builds
-
-// Position represents a trading position in the premove namespace
-type Position struct {
-	Symbol        string    `json:"symbol"`
-	Size          float64   `json:"size"`
-	EntryPrice    float64   `json:"entry_price"`
-	CurrentPrice  float64   `json:"current_price"`
-	Score         float64   `json:"score"`
-	Sector        string    `json:"sector"`
-	Beta          float64   `json:"beta"`
-	Timestamp     time.Time `json:"timestamp"`
-	CorrelationID string    `json:"correlation_id"`
-}
-
-// CorrelationMatrix represents position correlations in the premove namespace
-type CorrelationMatrix struct {
-	Matrix    map[string]map[string]float64 `json:"matrix"`
-	Symbols   []string                      `json:"symbols"`
-	Timestamp time.Time                     `json:"timestamp"`
-}
-
-// NewCorrelationMatrix creates a new correlation matrix (test-only stub)
-func NewCorrelationMatrix(symbols []string) *CorrelationMatrix {
-	matrix := make(map[string]map[string]float64)
-	for _, s1 := range symbols {
-		matrix[s1] = make(map[string]float64)
-		for _, s2 := range symbols {
-			if s1 == s2 {
-				matrix[s1][s2] = 1.0
-			} else {
-				matrix[s1][s2] = 0.1 // Low correlation stub
-			}
-		}
-	}
-	
-	return &CorrelationMatrix{
-		Matrix:    matrix,
-		Symbols:   symbols,
-		Timestamp: time.Now(),
-	}
-}
-
-// GetCorrelation returns correlation between two symbols (test-only stub)
-func (cm *CorrelationMatrix) GetCorrelation(symbol1, symbol2 string) float64 {
-	if symbol1 == symbol2 {
-		return 1.0
-	}
-	
-	if cm.Matrix != nil {
-		if s1Map, exists := cm.Matrix[symbol1]; exists {
-			if corr, exists := s1Map[symbol2]; exists {
-				return corr
-			}
-		}
-	}
-	
-	return 0.1 // Default low correlation
-}
-
-// ExecutionMetrics contains execution performance metrics (referenced by UI)
-type ExecutionMetrics struct {
-	AvgSlippageBps float64       `json:"avg_slippage_bps"`
-	AvgFillTime    time.Duration `json:"avg_fill_time"`
-	FillRate       float64       `json:"fill_rate"`
-}
-
-// GetExecutionSummary returns execution summary (referenced by UI)
-func (r *Runner) GetExecutionSummary() map[string]interface{} {
-	return map[string]interface{}{
-		"total_executions": 0,
-		"avg_slippage":     0.0,
-		"fill_rate":        0.0,
-	}
-}
-
-// GetPortfolioConstraints returns portfolio constraints (referenced by UI)
-func (r *Runner) GetPortfolioConstraints(positions []Position) map[string]interface{} {
-	return map[string]interface{}{
-		"total_positions":     len(positions),
-		"utilization":         0.0,
-		"max_position_size":   100000.0,
-		"correlation_limit":   0.7,
-		"sector_concentration": 0.3,
-	}
-}
-
-// PortfolioPruningResult represents legacy portfolio pruning results
-type PortfolioPruningResult struct {
-	Candidates       []Position           `json:"candidates"`
-	Accepted         []Position           `json:"accepted"`
-	Rejected         []Position           `json:"rejected"`
-	RejectionReasons map[string][]string  `json:"rejection_reasons"`
-	PrunedCount      int                  `json:"pruned_count"`
-	BetaUtilization  float64              `json:"beta_utilization"`
-}
-
-// PrunePortfolio is a legacy method for backward compatibility
-func (pm *PortfolioManager) PrunePortfolio(positions []Position, existing []Position, correlations *CorrelationMatrix) (*PortfolioPruningResult, error) {
-	// Simple stub for test compatibility
-	legacyResult := &PortfolioPruningResult{
-		Candidates:       positions,
-		Accepted:         positions, // Accept all for simplicity in tests
-		Rejected:         make([]Position, 0),
-		RejectionReasons: make(map[string][]string),
-		PrunedCount:      0,
-		BetaUtilization:  0.5,
-	}
-	
-	return legacyResult, nil
-}
\ No newline at end of file
+//go:build !prod && !integration
+
+package premove
+
+import (
+	"time"
+)
+
+// Test-only shims for missing premove types and methods
+// This file is excluded from production and integration builds
+
+// Position represents a trading position in the premove namespace
+type Position struct {
+	Symbol        string    `json:"symbol"`
+	Size          float64   `json:"size"`
+	EntryPrice    float64   `json:"entry_price"`
+	CurrentPrice  float64   `json:"current_price"`
+	Score         float64   `json:"score"`
+	Sector        string    `json:"sector"`
+	Beta          float64   `json:"beta"`
+	Timestamp     time.Time `json:"timestamp"`
+	CorrelationID string    `json:"correlation_id"`
+}
+
+// CorrelationMatrix represents position correlations in the premove namespace
+type CorrelationMatrix struct {
+	Matrix    map[string]map[string]float64 `json:"matrix"`
+	Symbols   []string                      `json:"symbols"`
+	Timestamp time.Time                     `json:"timestamp"`
+}
+
+// NewCorrelationMatrix creates a new correlation matrix (test-only stub)
+func NewCorrelationMatrix(symbols []string) *CorrelationMatrix {
+	matrix := make(map[string]map[string]float64)
+	for _, s1 := range symbols {
+		matrix[s1] = make(map[string]float64)
+		for _, s2 := range symbols {
+			if s1 == s2 {
+				matrix[s1][s2] = 1.0
+			} else {
+				matrix[s1][s2] = 0.1 // Low correlation stub
+			}
+		}
+	}
+
+	return &CorrelationMatrix{
+		Matrix:    matrix,
+		Symbols:   symbols,
+		Timestamp: time.Now(),
+	}
+}
+
+// GetCorrelation returns correlation between two symbols (test-only stub)
+func (cm *CorrelationMatrix) GetCorrelation(symbol1, symbol2 string) float64 {
+	if symbol1 == symbol2 {
+		return 1.0
+	}
+
+	if cm.Matrix != nil {
+		if s1Map, exists := cm.Matrix[symbol1]; exists {
+			if corr, exists := s1Map[symbol2]; exists {
+				return corr
+			}
+		}
+	}
+
+	return 0.1 // Default low correlation
+}
+
+// ExecutionMetrics contains execution performance metrics (referenced by UI)
+type ExecutionMetrics struct {
+	AvgSlippageBps float64       `json:"avg_slippage_bps"`
+	AvgFillTime    time.Duration `json:"avg_fill_time"`
+	FillRate       float64       `json:"fill_rate"`
+}
+
+// GetExecutionSummary returns execution summary (referenced by UI)
+func (r *Runner) GetExecutionSummary() map[string]interface{} {
+	return map[string]interface{}{
+		"total_executions": 0,
+		"avg_slippage":     0.0,
+		"fill_rate":        0.0,
+	}
+}
+
+// GetPortfolioConstraints returns portfolio constraints (referenced by UI)
+func (r *Runner) GetPortfolioConstraints(positions []Position) map[string]interface{} {
+	return map[string]interface{}{
+		"total_positions":      len(positions),
+		"utilization":          0.0,
+		"max_position_size":    100000.0,
+		"correlation_limit":    0.7,
+		"sector_concentration": 0.3,
+	}
+}
+
+// PortfolioPruningResult represents legacy portfolio pruning results
+type PortfolioPruningResult struct {
+	Candidates       []Position          `json:"candidates"`
+	Accepted         []Position          `json:"accepted"`
+	Rejected         []Position          `json:"rejected"`
+	RejectionReasons map[string][]string `json:"rejection_reasons"`
+	PrunedCount      int                 `json:"pruned_count"`
+	BetaUtilization  float64             `json:"beta_utilization"`
+}
+
+// PrunePortfolio is a legacy method for backward compatibility
+func (pm *PortfolioManager) PrunePortfolio(positions []Position, existing []Position, correlations *CorrelationMatrix) (*PortfolioPruningResult, error) {
+	// Simple stub for test compatibility
+	legacyResult := &PortfolioPruningResult{
+		Candidates:       positions,
+		Accepted:         positions, // Accept all for simplicity in tests
+		Rejected:         make([]Position, 0),
+		RejectionReasons: make(map[string][]string),
+		PrunedCount:      0,
+		BetaUtilization:  0.5,
+	}
+
+	return legacyResult, nil
+}
diff --git a/src/application/premove/runner.go b/src/application/premove/runner.go
index 20d8853..57b01b0 100644
--- a/src/application/premove/runner.go
+++ b/src/application/premove/runner.go
@@ -6,8 +6,8 @@ import (
 	"time"
 
 	"cryptorun/src/domain/premove/cvd"
-	"cryptorun/src/domain/premove/ports"
 	"cryptorun/src/domain/premove/portfolio"
+	"cryptorun/src/domain/premove/ports"
 	"cryptorun/src/domain/premove/proxy"
 	"cryptorun/src/infrastructure/percentiles"
 )
@@ -39,24 +39,24 @@ type PITReplayPoint struct {
 
 // BacktestResult contains backtest execution results
 type BacktestResult struct {
-	TotalTrades    int
-	WinRate        float64
-	PnL            float64
-	MaxDrawdown    float64
-	SharpeRatio    float64
-	ExecutionTime  time.Duration
+	TotalTrades   int
+	WinRate       float64
+	PnL           float64
+	MaxDrawdown   float64
+	SharpeRatio   float64
+	ExecutionTime time.Duration
 }
 
 // AlertRecord represents an alert that was sent
 type AlertRecord struct {
-	ID              string                 `json:"id"`
-	Symbol          string                 `json:"symbol"`
-	Score           float64                `json:"score"`
-	Reasons         []string               `json:"reasons"`
-	Metadata        map[string]interface{} `json:"metadata"`
-	Timestamp       time.Time              `json:"timestamp"`
-	Status          string                 `json:"status"`
-	ProcessingTime  time.Duration          `json:"processing_time"`
+	ID             string                 `json:"id"`
+	Symbol         string                 `json:"symbol"`
+	Score          float64                `json:"score"`
+	Reasons        []string               `json:"reasons"`
+	Metadata       map[string]interface{} `json:"metadata"`
+	Timestamp      time.Time              `json:"timestamp"`
+	Status         string                 `json:"status"`
+	ProcessingTime time.Duration          `json:"processing_time"`
 }
 
 // Runner orchestrates the premove detection pipeline with portfolio pruning
@@ -99,13 +99,13 @@ type Candidate struct {
 
 // ProcessingResult contains the result of pipeline processing
 type ProcessingResult struct {
-	OriginalCandidates   []Candidate              `json:"original_candidates"`
-	PostGatesCandidates  []Candidate              `json:"post_gates_candidates"`
-	PortfolioPruneResult *portfolio.PruneResult   `json:"portfolio_prune_result"`
-	AlertsGenerated      []AlertRecord            `json:"alerts_generated"`
-	ProcessingTime       time.Duration            `json:"processing_time"`
-	Timestamp            time.Time                `json:"timestamp"`
-	Errors               []string                 `json:"errors,omitempty"`
+	OriginalCandidates   []Candidate            `json:"original_candidates"`
+	PostGatesCandidates  []Candidate            `json:"post_gates_candidates"`
+	PortfolioPruneResult *portfolio.PruneResult `json:"portfolio_prune_result"`
+	AlertsGenerated      []AlertRecord          `json:"alerts_generated"`
+	ProcessingTime       time.Duration          `json:"processing_time"`
+	Timestamp            time.Time              `json:"timestamp"`
+	Errors               []string               `json:"errors,omitempty"`
 }
 
 // RunPipeline executes the full premove detection pipeline
@@ -351,7 +351,7 @@ func (am *AlertManager) ProcessAlert(alert AlertRecord) (*AlertRecord, error) {
 			Sector:      "crypto",
 			Priority:    "medium",
 		}
-		
+
 		decision := am.governor.EvaluateAlert(candidate)
 		if decision.Allow {
 			alert.Status = "sent"
@@ -359,7 +359,7 @@ func (am *AlertManager) ProcessAlert(alert AlertRecord) (*AlertRecord, error) {
 			alert.Status = "rate_limited"
 		}
 	}
-	
+
 	return &alert, nil
 }
 
@@ -368,9 +368,9 @@ func (am *AlertManager) GetAlertStats() map[string]interface{} {
 	if am.governor != nil {
 		return am.governor.GetAlertStats()
 	}
-	
+
 	return map[string]interface{}{
 		"total_alerts": 0,
 		"rate_limited": 0,
 	}
-}
\ No newline at end of file
+}
diff --git a/src/domain/momentum/core.go b/src/domain/momentum/core.go
index cce57f0..0af5846 100644
--- a/src/domain/momentum/core.go
+++ b/src/domain/momentum/core.go
@@ -3,6 +3,7 @@ package momentum
 import (
 	"fmt"
 	"math"
+	"time"
 
 	"cryptorun/internal/domain/regime"
 )
@@ -38,14 +39,15 @@ func (mtw MTW) Normalize() MTW {
 
 // CoreInputs represents the input data for MomentumCore calculation
 type CoreInputs struct {
-	R1h     float64 // 1-hour return (simple or log)
-	R4h     float64 // 4-hour return (simple or log)
-	R12h    float64 // 12-hour return (simple or log)
-	R24h    float64 // 24-hour return (simple or log)
-	R7d     float64 // 7-day return (simple or log)
-	ATR1h   float64 // 1-hour Average True Range for normalization
-	ATR4h   float64 // 4-hour Average True Range for normalization
-	Accel4h float64 // d/dt of R4h over last 2-3 bars
+	Timestamp time.Time // Data timestamp
+	R1h       float64   // 1-hour return (simple or log)
+	R4h       float64   // 4-hour return (simple or log)
+	R12h      float64   // 12-hour return (simple or log)
+	R24h      float64   // 24-hour return (simple or log)
+	R7d       float64   // 7-day return (simple or log)
+	ATR1h     float64   // 1-hour Average True Range for normalization
+	ATR4h     float64   // 4-hour Average True Range for normalization
+	Accel4h   float64   // d/dt of R4h over last 2-3 bars
 }
 
 // CoreResult represents the output of MomentumCore calculation
diff --git a/src/internal/artifacts/writer.go b/src/internal/artifacts/writer.go
index 102804e..b28cb5f 100644
--- a/src/internal/artifacts/writer.go
+++ b/src/internal/artifacts/writer.go
@@ -6,6 +6,8 @@ import (
 	"fmt"
 	"os"
 	"path/filepath"
+	"reflect"
+	"strconv"
 	"time"
 )
 
@@ -13,6 +15,179 @@ var (
 	artifactsDir = "artifacts/ledger"
 )
 
+type AtomicWriter struct {
+	BaseDir string
+}
+
+func NewAtomicWriter(baseDir string) *AtomicWriter {
+	if baseDir == "" {
+		baseDir = "artifacts/explain"
+	}
+	return &AtomicWriter{BaseDir: baseDir}
+}
+
+func (w *AtomicWriter) WriteExplainReport(report interface{}, prefix string) error {
+	if err := w.ensureDir(); err != nil {
+		return fmt.Errorf("ensure dir: %w", err)
+	}
+
+	timestamp := time.Now().UTC().Format("20060102-150405")
+
+	jsonFile := fmt.Sprintf("%s-%s-explain.json", timestamp, prefix)
+	if err := w.writeJSONAtomic(jsonFile, report); err != nil {
+		return fmt.Errorf("write JSON: %w", err)
+	}
+
+	csvFile := fmt.Sprintf("%s-%s-explain.csv", timestamp, prefix)
+	if err := w.writeCSVFromStruct(csvFile, report); err != nil {
+		return fmt.Errorf("write CSV: %w", err)
+	}
+
+	return nil
+}
+
+func (w *AtomicWriter) writeJSONAtomic(filename string, v interface{}) error {
+	finalPath := filepath.Join(w.BaseDir, filename)
+	tempPath := finalPath + ".tmp"
+
+	data, err := json.MarshalIndent(v, "", "  ")
+	if err != nil {
+		return fmt.Errorf("marshal json: %w", err)
+	}
+
+	if err := os.WriteFile(tempPath, data, 0644); err != nil {
+		return fmt.Errorf("write temp file: %w", err)
+	}
+
+	if err := os.Rename(tempPath, finalPath); err != nil {
+		os.Remove(tempPath)
+		return fmt.Errorf("rename temp to final: %w", err)
+	}
+
+	return nil
+}
+
+func (w *AtomicWriter) writeCSVFromStruct(filename string, report interface{}) error {
+	finalPath := filepath.Join(w.BaseDir, filename)
+	tempPath := finalPath + ".tmp"
+
+	file, err := os.Create(tempPath)
+	if err != nil {
+		return fmt.Errorf("create temp file: %w", err)
+	}
+	defer file.Close()
+
+	writer := csv.NewWriter(file)
+	defer writer.Flush()
+
+	rows, err := w.extractCSVRows(report)
+	if err != nil {
+		os.Remove(tempPath)
+		return fmt.Errorf("extract CSV rows: %w", err)
+	}
+
+	for _, row := range rows {
+		if err := writer.Write(row); err != nil {
+			os.Remove(tempPath)
+			return fmt.Errorf("write CSV row: %w", err)
+		}
+	}
+
+	writer.Flush()
+	file.Close()
+
+	if err := os.Rename(tempPath, finalPath); err != nil {
+		os.Remove(tempPath)
+		return fmt.Errorf("rename temp to final: %w", err)
+	}
+
+	return nil
+}
+
+func (w *AtomicWriter) extractCSVRows(report interface{}) ([][]string, error) {
+	val := reflect.ValueOf(report)
+	if val.Kind() == reflect.Ptr {
+		val = val.Elem()
+	}
+
+	if val.Kind() != reflect.Struct {
+		return nil, fmt.Errorf("expected struct, got %T", report)
+	}
+
+	universeField := val.FieldByName("Universe")
+	if !universeField.IsValid() || universeField.Kind() != reflect.Slice {
+		return nil, fmt.Errorf("Universe field not found or not slice")
+	}
+
+	var rows [][]string
+
+	if universeField.Len() > 0 {
+		header := w.generateCSVHeader()
+		rows = append(rows, header)
+
+		for i := 0; i < universeField.Len(); i++ {
+			asset := universeField.Index(i)
+			if asset.Kind() == reflect.Ptr {
+				asset = asset.Elem()
+			}
+
+			csvRowMethod := asset.MethodByName("ToCSVRow")
+			if csvRowMethod.IsValid() {
+				results := csvRowMethod.Call(nil)
+				if len(results) > 0 {
+					csvRow := results[0].Interface()
+					row := w.structToStringSlice(csvRow)
+					rows = append(rows, row)
+				}
+			}
+		}
+	}
+
+	return rows, nil
+}
+
+func (w *AtomicWriter) generateCSVHeader() []string {
+	return []string{
+		"symbol", "decision", "score", "rank", "momentum", "technical",
+		"volume", "quality", "social", "entry_gate", "spread_bps",
+		"depth_usd", "vadr", "heat_score", "regime", "exchange",
+		"top_reason", "cache_hit_rate",
+	}
+}
+
+func (w *AtomicWriter) structToStringSlice(s interface{}) []string {
+	val := reflect.ValueOf(s)
+	if val.Kind() == reflect.Ptr {
+		val = val.Elem()
+	}
+
+	var result []string
+	for i := 0; i < val.NumField(); i++ {
+		field := val.Field(i)
+		result = append(result, w.valueToString(field))
+	}
+	return result
+}
+
+func (w *AtomicWriter) valueToString(v reflect.Value) string {
+	switch v.Kind() {
+	case reflect.String:
+		return v.String()
+	case reflect.Int, reflect.Int8, reflect.Int16, reflect.Int32, reflect.Int64:
+		return strconv.FormatInt(v.Int(), 10)
+	case reflect.Float32, reflect.Float64:
+		return strconv.FormatFloat(v.Float(), 'f', 6, 64)
+	case reflect.Bool:
+		return strconv.FormatBool(v.Bool())
+	default:
+		return fmt.Sprintf("%v", v.Interface())
+	}
+}
+
+func (w *AtomicWriter) ensureDir() error {
+	return os.MkdirAll(w.BaseDir, 0755)
+}
+
 func WriteJSON(name string, v interface{}) error {
 	if err := ensureDir(); err != nil {
 		return fmt.Errorf("ensure dir: %w", err)
diff --git a/tests/unit/regime/detector_test.go b/tests/unit/regime/detector_test.go
index 8382c04..26987cb 100644
--- a/tests/unit/regime/detector_test.go
+++ b/tests/unit/regime/detector_test.go
@@ -1,317 +1,317 @@
-package regime
-
-import (
-	"context"
-	"testing"
-	"time"
-
-	"cryptorun/internal/regime"
-)
-
-// MockDetectorInputs provides test data for regime detection
-type MockDetectorInputs struct {
-	realizedVol   float64
-	breadth       float64
-	breadthThrust float64
-	timestamp     time.Time
-}
-
-func (m *MockDetectorInputs) GetRealizedVolatility7d(ctx context.Context) (float64, error) {
-	return m.realizedVol, nil
-}
-
-func (m *MockDetectorInputs) GetBreadthAbove20MA(ctx context.Context) (float64, error) {
-	return m.breadth, nil
-}
-
-func (m *MockDetectorInputs) GetBreadthThrustADXProxy(ctx context.Context) (float64, error) {
-	return m.breadthThrust, nil
-}
-
-func (m *MockDetectorInputs) GetTimestamp(ctx context.Context) (time.Time, error) {
-	return m.timestamp, nil
-}
-
-func TestDetector_VotingLogic(t *testing.T) {
-	baseTime := time.Date(2024, 1, 1, 12, 0, 0, 0, time.UTC)
-	
-	testCases := []struct {
-		name          string
-		realizedVol   float64 // Threshold: 0.25
-		breadth       float64 // Threshold: 0.60
-		breadthThrust float64 // Threshold: 0.70
-		expectedRegime regime.Regime
-	}{
-		{
-			name:          "trending_bull_clear",
-			realizedVol:   0.15, // Low vol
-			breadth:       0.75, // High breadth -> trending
-			breadthThrust: 0.80, // High thrust -> trending
-			expectedRegime: regime.TrendingBull, // 2/3 trending votes
-		},
-		{
-			name:          "high_vol_clear",
-			realizedVol:   0.35, // High vol
-			breadth:       0.40, // Low breadth -> choppy
-			breadthThrust: 0.50, // Low thrust -> choppy
-			expectedRegime: regime.HighVol, // High vol vote wins
-		},
-		{
-			name:          "choppy_default",
-			realizedVol:   0.20, // Low vol
-			breadth:       0.50, // Low breadth -> choppy
-			breadthThrust: 0.60, // Low thrust -> choppy
-			expectedRegime: regime.Choppy, // 2/3 choppy votes
-		},
-		{
-			name:          "boundary_conditions_vol",
-			realizedVol:   0.25, // Exactly at threshold
-			breadth:       0.60, // Exactly at threshold
-			breadthThrust: 0.70, // Exactly at threshold
-			expectedRegime: regime.TrendingBull, // At threshold = trending
-		},
-		{
-			name:          "mixed_signals",
-			realizedVol:   0.30, // High vol
-			breadth:       0.70, // High breadth -> trending
-			breadthThrust: 0.40, // Low thrust -> choppy
-			expectedRegime: regime.HighVol, // High vol overrides
-		},
-	}
-	
-	for _, tc := range testCases {
-		t.Run(tc.name, func(t *testing.T) {
-			inputs := &MockDetectorInputs{
-				realizedVol:   tc.realizedVol,
-				breadth:       tc.breadth,
-				breadthThrust: tc.breadthThrust,
-				timestamp:     baseTime,
-			}
-			
-			detector := regime.NewDetector(inputs)
-			ctx := context.Background()
-			
-			result, err := detector.DetectRegime(ctx)
-			if err != nil {
-				t.Fatalf("DetectRegime failed: %v", err)
-			}
-			
-			if result.Regime != tc.expectedRegime {
-				t.Errorf("Expected regime %s, got %s", tc.expectedRegime, result.Regime)
-			}
-			
-			// Validate signals are captured
-			if result.Signals["realized_vol_7d"] != tc.realizedVol {
-				t.Errorf("Realized vol signal mismatch: expected %.3f, got %.3f", 
-					tc.realizedVol, result.Signals["realized_vol_7d"])
-			}
-			
-			// Validate confidence is reasonable
-			if result.Confidence < 0.33 || result.Confidence > 1.0 {
-				t.Errorf("Confidence out of range: %.3f", result.Confidence)
-			}
-			
-			// Validate voting breakdown exists
-			if len(result.VotingBreakdown) != 3 {
-				t.Errorf("Expected 3 votes, got %d", len(result.VotingBreakdown))
-			}
-		})
-	}
-}
-
-func TestDetector_StabilityDetection(t *testing.T) {
-	baseTime := time.Date(2024, 1, 1, 12, 0, 0, 0, time.UTC)
-	
-	inputs := &MockDetectorInputs{
-		realizedVol:   0.15,
-		breadth:       0.75,
-		breadthThrust: 0.80,
-		timestamp:     baseTime,
-	}
-	
-	detector := regime.NewDetector(inputs)
-	ctx := context.Background()
-	
-	// First detection - should be stable (no history)
-	result1, err := detector.DetectRegime(ctx)
-	if err != nil {
-		t.Fatalf("First detection failed: %v", err)
-	}
-	
-	if !result1.IsStable {
-		t.Error("First detection should be stable (no history)")
-	}
-	
-	if result1.ChangesSinceStart != 0 {
-		t.Errorf("Expected 0 changes, got %d", result1.ChangesSinceStart)
-	}
-	
-	// Change regime and detect again
-	inputs.realizedVol = 0.35 // Switch to high vol
-	inputs.breadth = 0.40
-	inputs.timestamp = baseTime.Add(4 * time.Hour)
-	
-	result2, err := detector.DetectRegime(ctx)
-	if err != nil {
-		t.Fatalf("Second detection failed: %v", err)
-	}
-	
-	if result2.IsStable {
-		t.Error("Second detection should not be stable (recent change)")
-	}
-	
-	if result2.ChangesSinceStart != 1 {
-		t.Errorf("Expected 1 change, got %d", result2.ChangesSinceStart)
-	}
-	
-	// Verify regime change was recorded
-	history := detector.GetDetectionHistory()
-	if len(history) != 1 {
-		t.Fatalf("Expected 1 regime change, got %d", len(history))
-	}
-	
-	change := history[0]
-	if change.FromRegime != regime.TrendingBull || change.ToRegime != regime.HighVol {
-		t.Errorf("Regime change mismatch: %s -> %s", change.FromRegime, change.ToRegime)
-	}
-}
-
-func TestDetector_UpdateInterval(t *testing.T) {
-	baseTime := time.Date(2024, 1, 1, 12, 0, 0, 0, time.UTC)
-	
-	inputs := &MockDetectorInputs{
-		realizedVol:   0.15,
-		breadth:       0.75,
-		breadthThrust: 0.80,
-		timestamp:     baseTime,
-	}
-	
-	detector := regime.NewDetector(inputs)
-	ctx := context.Background()
-	
-	// First detection
-	_, err := detector.DetectRegime(ctx)
-	if err != nil {
-		t.Fatalf("First detection failed: %v", err)
-	}
-	
-	// Immediate second detection should return cached result
-	shouldUpdate, err := detector.ShouldUpdate(ctx)
-	if err != nil {
-		t.Fatalf("ShouldUpdate failed: %v", err)
-	}
-	
-	if shouldUpdate {
-		t.Error("Should not update immediately after first detection")
-	}
-	
-	// After 4 hours, should update
-	inputs.timestamp = baseTime.Add(4 * time.Hour)
-	
-	shouldUpdate, err = detector.ShouldUpdate(ctx)
-	if err != nil {
-		t.Fatalf("ShouldUpdate failed: %v", err)
-	}
-	
-	if !shouldUpdate {
-		t.Error("Should update after 4 hours")
-	}
-}
-
-func TestDetector_BoundaryConditions(t *testing.T) {
-	baseTime := time.Date(2024, 1, 1, 12, 0, 0, 0, time.UTC)
-	
-	testCases := []struct {
-		name        string
-		realizedVol float64
-		expected    string // Expected vote for realized vol
-	}{
-		{"exactly_at_threshold", 0.25, "high_vol"},  // At threshold should trigger
-		{"just_below", 0.249, "low_vol"},
-		{"just_above", 0.251, "high_vol"},
-		{"zero", 0.0, "low_vol"},
-		{"very_high", 1.0, "high_vol"},
-	}
-	
-	for _, tc := range testCases {
-		t.Run(tc.name, func(t *testing.T) {
-			inputs := &MockDetectorInputs{
-				realizedVol:   tc.realizedVol,
-				breadth:       0.50, // Below threshold
-				breadthThrust: 0.60, // Below threshold
-				timestamp:     baseTime,
-			}
-			
-			detector := regime.NewDetector(inputs)
-			ctx := context.Background()
-			
-			result, err := detector.DetectRegime(ctx)
-			if err != nil {
-				t.Fatalf("DetectRegime failed: %v", err)
-			}
-			
-			realizedVolVote := result.VotingBreakdown["realized_vol"]
-			if realizedVolVote != tc.expected {
-				t.Errorf("Expected realized vol vote %s, got %s", tc.expected, realizedVolVote)
-			}
-		})
-	}
-}
-
-func TestDetector_ErrorHandling(t *testing.T) {
-	// Test with nil inputs (should cause errors)
-	detector := regime.NewDetector(nil)
-	ctx := context.Background()
-	
-	_, err := detector.DetectRegime(ctx)
-	if err == nil {
-		t.Error("Expected error with nil inputs")
-	}
-}
-
-func TestWeightManager_Validation(t *testing.T) {
-	wm := regime.NewWeightManager()
-	
-	// Test all presets validate
-	for regimeType := range wm.GetAllPresets() {
-		err := wm.ValidateWeights(regimeType)
-		if err != nil {
-			t.Errorf("Weight validation failed for regime %s: %v", regimeType, err)
-		}
-	}
-	
-	// Test getting weights for each regime
-	trendingPreset, err := wm.GetWeightsForRegime(regime.TrendingBull)
-	if err != nil {
-		t.Fatalf("Failed to get trending bull weights: %v", err)
-	}
-	
-	// Trending should have weekly carry
-	if trendingPreset.Weights["weekly_7d_carry"] == 0.0 {
-		t.Error("Trending bull should have non-zero weekly carry")
-	}
-	
-	choppyPreset, err := wm.GetWeightsForRegime(regime.Choppy)
-	if err != nil {
-		t.Fatalf("Failed to get choppy weights: %v", err)
-	}
-	
-	// Choppy should have no weekly carry
-	if choppyPreset.Weights["weekly_7d_carry"] != 0.0 {
-		t.Error("Choppy should have zero weekly carry")
-	}
-	
-	highVolPreset, err := wm.GetWeightsForRegime(regime.HighVol)
-	if err != nil {
-		t.Fatalf("Failed to get high vol weights: %v", err)
-	}
-	
-	// High vol should have tightened gates
-	if !highVolPreset.MovementGate.TightenedThresholds {
-		t.Error("High vol should have tightened thresholds")
-	}
-	
-	if highVolPreset.MovementGate.MinMovementPercent <= 5.0 {
-		t.Error("High vol should have higher movement threshold than standard")
-	}
-}
\ No newline at end of file
+package regime
+
+import (
+	"context"
+	"testing"
+	"time"
+
+	"cryptorun/internal/regime"
+)
+
+// MockDetectorInputs provides test data for regime detection
+type MockDetectorInputs struct {
+	realizedVol   float64
+	breadth       float64
+	breadthThrust float64
+	timestamp     time.Time
+}
+
+func (m *MockDetectorInputs) GetRealizedVolatility7d(ctx context.Context) (float64, error) {
+	return m.realizedVol, nil
+}
+
+func (m *MockDetectorInputs) GetBreadthAbove20MA(ctx context.Context) (float64, error) {
+	return m.breadth, nil
+}
+
+func (m *MockDetectorInputs) GetBreadthThrustADXProxy(ctx context.Context) (float64, error) {
+	return m.breadthThrust, nil
+}
+
+func (m *MockDetectorInputs) GetTimestamp(ctx context.Context) (time.Time, error) {
+	return m.timestamp, nil
+}
+
+func TestDetector_VotingLogic(t *testing.T) {
+	baseTime := time.Date(2024, 1, 1, 12, 0, 0, 0, time.UTC)
+
+	testCases := []struct {
+		name           string
+		realizedVol    float64 // Threshold: 0.25
+		breadth        float64 // Threshold: 0.60
+		breadthThrust  float64 // Threshold: 0.70
+		expectedRegime regime.Regime
+	}{
+		{
+			name:           "trending_bull_clear",
+			realizedVol:    0.15,                // Low vol
+			breadth:        0.75,                // High breadth -> trending
+			breadthThrust:  0.80,                // High thrust -> trending
+			expectedRegime: regime.TrendingBull, // 2/3 trending votes
+		},
+		{
+			name:           "high_vol_clear",
+			realizedVol:    0.35,           // High vol
+			breadth:        0.40,           // Low breadth -> choppy
+			breadthThrust:  0.50,           // Low thrust -> choppy
+			expectedRegime: regime.HighVol, // High vol vote wins
+		},
+		{
+			name:           "choppy_default",
+			realizedVol:    0.20,          // Low vol
+			breadth:        0.50,          // Low breadth -> choppy
+			breadthThrust:  0.60,          // Low thrust -> choppy
+			expectedRegime: regime.Choppy, // 2/3 choppy votes
+		},
+		{
+			name:           "boundary_conditions_vol",
+			realizedVol:    0.25,                // Exactly at threshold
+			breadth:        0.60,                // Exactly at threshold
+			breadthThrust:  0.70,                // Exactly at threshold
+			expectedRegime: regime.TrendingBull, // At threshold = trending
+		},
+		{
+			name:           "mixed_signals",
+			realizedVol:    0.30,           // High vol
+			breadth:        0.70,           // High breadth -> trending
+			breadthThrust:  0.40,           // Low thrust -> choppy
+			expectedRegime: regime.HighVol, // High vol overrides
+		},
+	}
+
+	for _, tc := range testCases {
+		t.Run(tc.name, func(t *testing.T) {
+			inputs := &MockDetectorInputs{
+				realizedVol:   tc.realizedVol,
+				breadth:       tc.breadth,
+				breadthThrust: tc.breadthThrust,
+				timestamp:     baseTime,
+			}
+
+			detector := regime.NewDetector(inputs)
+			ctx := context.Background()
+
+			result, err := detector.DetectRegime(ctx)
+			if err != nil {
+				t.Fatalf("DetectRegime failed: %v", err)
+			}
+
+			if result.Regime != tc.expectedRegime {
+				t.Errorf("Expected regime %s, got %s", tc.expectedRegime, result.Regime)
+			}
+
+			// Validate signals are captured
+			if result.Signals["realized_vol_7d"] != tc.realizedVol {
+				t.Errorf("Realized vol signal mismatch: expected %.3f, got %.3f",
+					tc.realizedVol, result.Signals["realized_vol_7d"])
+			}
+
+			// Validate confidence is reasonable
+			if result.Confidence < 0.33 || result.Confidence > 1.0 {
+				t.Errorf("Confidence out of range: %.3f", result.Confidence)
+			}
+
+			// Validate voting breakdown exists
+			if len(result.VotingBreakdown) != 3 {
+				t.Errorf("Expected 3 votes, got %d", len(result.VotingBreakdown))
+			}
+		})
+	}
+}
+
+func TestDetector_StabilityDetection(t *testing.T) {
+	baseTime := time.Date(2024, 1, 1, 12, 0, 0, 0, time.UTC)
+
+	inputs := &MockDetectorInputs{
+		realizedVol:   0.15,
+		breadth:       0.75,
+		breadthThrust: 0.80,
+		timestamp:     baseTime,
+	}
+
+	detector := regime.NewDetector(inputs)
+	ctx := context.Background()
+
+	// First detection - should be stable (no history)
+	result1, err := detector.DetectRegime(ctx)
+	if err != nil {
+		t.Fatalf("First detection failed: %v", err)
+	}
+
+	if !result1.IsStable {
+		t.Error("First detection should be stable (no history)")
+	}
+
+	if result1.ChangesSinceStart != 0 {
+		t.Errorf("Expected 0 changes, got %d", result1.ChangesSinceStart)
+	}
+
+	// Change regime and detect again
+	inputs.realizedVol = 0.35 // Switch to high vol
+	inputs.breadth = 0.40
+	inputs.timestamp = baseTime.Add(4 * time.Hour)
+
+	result2, err := detector.DetectRegime(ctx)
+	if err != nil {
+		t.Fatalf("Second detection failed: %v", err)
+	}
+
+	if result2.IsStable {
+		t.Error("Second detection should not be stable (recent change)")
+	}
+
+	if result2.ChangesSinceStart != 1 {
+		t.Errorf("Expected 1 change, got %d", result2.ChangesSinceStart)
+	}
+
+	// Verify regime change was recorded
+	history := detector.GetDetectionHistory()
+	if len(history) != 1 {
+		t.Fatalf("Expected 1 regime change, got %d", len(history))
+	}
+
+	change := history[0]
+	if change.FromRegime != regime.TrendingBull || change.ToRegime != regime.HighVol {
+		t.Errorf("Regime change mismatch: %s -> %s", change.FromRegime, change.ToRegime)
+	}
+}
+
+func TestDetector_UpdateInterval(t *testing.T) {
+	baseTime := time.Date(2024, 1, 1, 12, 0, 0, 0, time.UTC)
+
+	inputs := &MockDetectorInputs{
+		realizedVol:   0.15,
+		breadth:       0.75,
+		breadthThrust: 0.80,
+		timestamp:     baseTime,
+	}
+
+	detector := regime.NewDetector(inputs)
+	ctx := context.Background()
+
+	// First detection
+	_, err := detector.DetectRegime(ctx)
+	if err != nil {
+		t.Fatalf("First detection failed: %v", err)
+	}
+
+	// Immediate second detection should return cached result
+	shouldUpdate, err := detector.ShouldUpdate(ctx)
+	if err != nil {
+		t.Fatalf("ShouldUpdate failed: %v", err)
+	}
+
+	if shouldUpdate {
+		t.Error("Should not update immediately after first detection")
+	}
+
+	// After 4 hours, should update
+	inputs.timestamp = baseTime.Add(4 * time.Hour)
+
+	shouldUpdate, err = detector.ShouldUpdate(ctx)
+	if err != nil {
+		t.Fatalf("ShouldUpdate failed: %v", err)
+	}
+
+	if !shouldUpdate {
+		t.Error("Should update after 4 hours")
+	}
+}
+
+func TestDetector_BoundaryConditions(t *testing.T) {
+	baseTime := time.Date(2024, 1, 1, 12, 0, 0, 0, time.UTC)
+
+	testCases := []struct {
+		name        string
+		realizedVol float64
+		expected    string // Expected vote for realized vol
+	}{
+		{"exactly_at_threshold", 0.25, "high_vol"}, // At threshold should trigger
+		{"just_below", 0.249, "low_vol"},
+		{"just_above", 0.251, "high_vol"},
+		{"zero", 0.0, "low_vol"},
+		{"very_high", 1.0, "high_vol"},
+	}
+
+	for _, tc := range testCases {
+		t.Run(tc.name, func(t *testing.T) {
+			inputs := &MockDetectorInputs{
+				realizedVol:   tc.realizedVol,
+				breadth:       0.50, // Below threshold
+				breadthThrust: 0.60, // Below threshold
+				timestamp:     baseTime,
+			}
+
+			detector := regime.NewDetector(inputs)
+			ctx := context.Background()
+
+			result, err := detector.DetectRegime(ctx)
+			if err != nil {
+				t.Fatalf("DetectRegime failed: %v", err)
+			}
+
+			realizedVolVote := result.VotingBreakdown["realized_vol"]
+			if realizedVolVote != tc.expected {
+				t.Errorf("Expected realized vol vote %s, got %s", tc.expected, realizedVolVote)
+			}
+		})
+	}
+}
+
+func TestDetector_ErrorHandling(t *testing.T) {
+	// Test with nil inputs (should cause errors)
+	detector := regime.NewDetector(nil)
+	ctx := context.Background()
+
+	_, err := detector.DetectRegime(ctx)
+	if err == nil {
+		t.Error("Expected error with nil inputs")
+	}
+}
+
+func TestWeightManager_Validation(t *testing.T) {
+	wm := regime.NewWeightManager()
+
+	// Test all presets validate
+	for regimeType := range wm.GetAllPresets() {
+		err := wm.ValidateWeights(regimeType)
+		if err != nil {
+			t.Errorf("Weight validation failed for regime %s: %v", regimeType, err)
+		}
+	}
+
+	// Test getting weights for each regime
+	trendingPreset, err := wm.GetWeightsForRegime(regime.TrendingBull)
+	if err != nil {
+		t.Fatalf("Failed to get trending bull weights: %v", err)
+	}
+
+	// Trending should have weekly carry
+	if trendingPreset.Weights["weekly_7d_carry"] == 0.0 {
+		t.Error("Trending bull should have non-zero weekly carry")
+	}
+
+	choppyPreset, err := wm.GetWeightsForRegime(regime.Choppy)
+	if err != nil {
+		t.Fatalf("Failed to get choppy weights: %v", err)
+	}
+
+	// Choppy should have no weekly carry
+	if choppyPreset.Weights["weekly_7d_carry"] != 0.0 {
+		t.Error("Choppy should have zero weekly carry")
+	}
+
+	highVolPreset, err := wm.GetWeightsForRegime(regime.HighVol)
+	if err != nil {
+		t.Fatalf("Failed to get high vol weights: %v", err)
+	}
+
+	// High vol should have tightened gates
+	if !highVolPreset.MovementGate.TightenedThresholds {
+		t.Error("High vol should have tightened thresholds")
+	}
+
+	if highVolPreset.MovementGate.MinMovementPercent <= 5.0 {
+		t.Error("High vol should have higher movement threshold than standard")
+	}
+}
